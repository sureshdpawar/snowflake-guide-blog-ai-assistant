{"docstore/metadata": {"95ad9605-f0d4-4bc3-8aaf-9212c12b29e5": {"doc_hash": "8cc73704464407255b43c3f1ef18f70684f061081f3bfcb78d55fb5197f5ee91"}, "f2ca1935-cb5a-4945-82f5-f27e8b1a6f96": {"doc_hash": "64e5db6923e31da2c07ab6dd9137c2de463cd2687e5926699693a2bf2429fa1f"}, "4f5c7665-f156-4b41-ad35-8df8f9ad013e": {"doc_hash": "f1c20581c7ea6cb8d1ee33d6834e8e0b7520f2744f1e7d2db17f10411f1a7ceb"}, "0d7d8577-801f-4242-b4fe-8efb3010aebd": {"doc_hash": "3336e22a7833a442166a36a1e3a1c8371ca887a3253dc78aec4dd69c9625b9cd"}, "c5fa9ef1-ac5c-41b9-8ab3-a33983c68cbe": {"doc_hash": "06edd4c7fbdbe2c10b66789092e27ca0e6ae0af4aa36a04353f33566c27b5aca"}, "7aba2b2b-a34a-4d73-b81e-7a815a067512": {"doc_hash": "0208e718c53f87a75d398bd6226fb7c654a495165ffa120d9a793f78189b6d29"}, "a56aa6b5-5dfa-43bc-84cf-4dbaadc5138a": {"doc_hash": "e71c28c151df7bcb8216de8f242520ec35becd6032a59189c9134c27d0290278"}, "810734c5-8988-406b-84b8-58d212b43716": {"doc_hash": "7b2bef9f5373b95c4edc17f0bc0ad61b036694b3b5bf7a60134ecbecf620b4ab"}, "b7868653-afc3-467c-b796-4a6382eb5407": {"doc_hash": "46ace7ad33faada49ce6b047492eca9297d9a1c0e89e286a8a9e0f5fd2217afc"}, "803d1f18-0bb5-4e29-8f82-7dfb005f7b4b": {"doc_hash": "de335db7e9619f03d26b8e0ad06acf02e82dc73e81e2d4488bdb0b2a1c6057f9"}, "3fac0a30-ffb1-4e20-aab2-1181fc1ac6e2": {"doc_hash": "f538bf3f92f0ab32f6eaf88cf9e65301a9ff174df5406c880bd8fef0200f2639"}, "7baec97b-de2b-443a-a08e-c9a78636ff2c": {"doc_hash": "7b43c5893a0da6127a2bed3f7f499d1ce63d828497ee80046e4e67bef9698730"}, "20054bdd-e5cd-4958-a755-662c0a52a0ae": {"doc_hash": "d02066886c9430fe30f717e4d31a14955e3676304840e92f7e4b145925dbbc16"}, "7557ca6f-4040-4bab-b0ff-59655ed13169": {"doc_hash": "4b59538dee8cc1b7a1014f39fa500713d0891e0316042196081b5ae735fef845"}, "62b3ca07-dcd7-4326-9f1a-f09a1b21b0af": {"doc_hash": "be294339844f66ec00047ab124bf2d499103933c6bf32191c6e1f6a89f54e364"}, "bdd73f33-aba0-48df-a07a-903239b03a0c": {"doc_hash": "18682111fc054fdb715ec61155a967e1e8f7168e1c65204d2efa967cc4d1d12f"}, "cf3bce4b-776f-46a0-bc31-9e9eb747a0c0": {"doc_hash": "615caea614cf75f4095baf5d66a179045025cbca333edc08d908650edcd2b97f"}, "052f529a-4826-4498-af43-c34c1f7f51df": {"doc_hash": "bcd8d6c5726445f5af19910057231c446b4c5584b47e0d17ce526bd2c912e6cf"}, "106d0f0a-9104-4283-a354-95c62e57c866": {"doc_hash": "b3e4f35e8ece9a61011cebdef78552518911572348c355c522ba2bf01132ea92"}, "376f4579-e99f-4f91-ab15-163e9fb527f8": {"doc_hash": "435c814d1b671deb79674ce0eb01ed59fb4926b1472c4169780da5bd3f4a2c8a"}, "1e5b25b5-d5cd-4df8-8d39-641ed9f4be5e": {"doc_hash": "baa68a704e608ecb3a446d13e9e2e8bd7047805c12d8e84645118d18f2a5fe1c"}, "186a0c4f-4cb9-4256-a5f4-53c96ddb73f5": {"doc_hash": "84498b075b32adda87dd5f2e0882eb2d239fc4ba294a0c70c99ce3b1af2e4fb7"}, "dcf65fc6-f88e-48b4-87f2-1900d41de02b": {"doc_hash": "0514618d17cfaa644f0ccf24704e842d13208e144583966f9145afe71d84f20f"}, "eba9096b-64db-4b8b-94ab-5f02e1ba42b9": {"doc_hash": "46a3e4f327c978249353ec1c4613121bbafb57745b65eded4478cd31e151e142"}, "434464da-3718-4906-b4a9-4f0dacf6e2af": {"doc_hash": "287a4ebcfc1bb7ddb8d9c6bfc5ad9fbaadf5c529861a2e16b11049f92a547c8e"}, "e9bbc953-907a-46ec-a9c3-31c23d7ea908": {"doc_hash": "4539a9962b5d1d0b7052bb437b741d53efa8cabf294eebceed835b30e43dd676"}, "4910572b-9e9c-46f7-b230-12f17e471e52": {"doc_hash": "3dbca17a3fdfcd566f5f1918078f871f93c1dcd134ede1e74427251b167608ef"}, "fc326342-a839-4cc3-90f1-38c1a0e27f01": {"doc_hash": "c797fad572fd67d45a68b6a4cf658569421bfc9da5bef07234a601acbae56eca"}, "029099c6-84d0-406d-bef8-cfbbfd73c0b0": {"doc_hash": "d8c6edcffe7251f9b31ceb0ef4e7c9ed7b47158588cd4dad7fab3eb23954dc6e"}, "cee08ed0-4513-4e2d-9d2d-a97de599c4ff": {"doc_hash": "b44e3b487fc416d774c2d43906f61d607f9b303a017e797dc85d84349cc7038f"}, "d44e99ff-027a-440a-a126-6d8b141a29a3": {"doc_hash": "f82527c2ce0638a002e10b0fcf9864ea78a2856b2264a402e91c0ea8e97f4e31"}, "85cebce5-42fb-4a8c-b87e-7e952b0f483b": {"doc_hash": "df73a0b60a2defd772b54606e8157eb3ffd23059acd293c7f1925d9826a81a0c"}, "b5efa891-4a65-4d19-ad79-3adfb61d5a50": {"doc_hash": "2ad70d46340b4a44fcc2e71fab3175e26eb6ddc848a0ca4678598331f40bb494"}, "fb3db9aa-6583-4276-a46a-10659bd4fc10": {"doc_hash": "aa10d4097509abe2cb6bbfc138f48e7e187e5676e1c35709cae7f43c65e2ecd9"}, "3e5ff74d-d112-4fb7-8eec-fae6954cd457": {"doc_hash": "7980b1078cba29324376ee0640703d5d8274f8fb5bc74bd7e20caab0770c3145"}, "8e99f547-c15e-4eda-9493-c0c5f39beebb": {"doc_hash": "c3ab38e32eb02332566eb777e0d42f4b5159ad270d783da47596ef568e848aa5"}, "da9b7e1c-d5de-4cd4-9dcb-e3e8a66220be": {"doc_hash": "0f94e0d667e097a66e10bc63633da0cacc2b2d53080ad66cfdf066454018ae20"}, "33218217-3d78-4c4a-824f-f82492907e73": {"doc_hash": "c30ccbe3a77396414799117b6f81ed9d86ef599251efda503aca5ba0e88e679c"}, "20b56c6a-7e30-434e-afef-c9506ecab9ec": {"doc_hash": "6f3ea2d32f88c33c92925e25a04a33b11d0fe72d8ca37be103ad8bd3c8e2aab2"}, "545eeb11-1a1b-4638-bbae-0ea57477cb54": {"doc_hash": "23eb1b793f1ab6c34f19c18e08ac38856e3fd11e627eed45c63fab42b551d4b0"}, "e7b2c337-0599-40d9-b210-cec1d9a7a25b": {"doc_hash": "6879ad69068e184ca0f32ef2236e5dd5b9bb3c8a0b202009ac216a9b16910662"}, "6e4acd37-ba41-48ad-921f-9b65dc9f47b2": {"doc_hash": "2ef377284889536c6770ed1fed2f6cb908fca12d096bc8db21fba9aae468d691"}, "604cb90f-a657-40c8-8353-c58df0a35b61": {"doc_hash": "da8eaeedf249c44ecef360f07f8669f1726bfc86354fa54a3cdf120b8d730985"}, "62b59f10-0a7b-4149-8fdc-05b0d5fd1ef3": {"doc_hash": "74bad7e1aca50edcef366405d5ee35a25063e14f16d400514524281effb78a6c"}, "6cd45afe-f968-436a-8e81-2dd2b58194f5": {"doc_hash": "d80aacc43387ed738aedec83f9b8221411a1ba8cecc3906307761ddf79f84fde"}, "f20c0e72-b4f0-4ada-bb96-6cdd95963ee5": {"doc_hash": "4050a30415afb5c4d82d1b41fd6e353d092580748283551a5c6d5c4ab34de391"}, "7deb220d-4a1f-4237-80b5-33f5c49d0e2c": {"doc_hash": "05dd0df413069db5afc5253f578b2c918247cc34f1c634a1eed56aa94c3ce2e9"}, "78440e52-aaff-470c-8781-fd2b5b3fc92d": {"doc_hash": "0fc8b393ce89b3f42392335482b9dd9fcf80b6d7700fbc55658b0ac59b1293f7"}, "dbb76a11-9160-489e-a013-51bee5761cc8": {"doc_hash": "aadd75c00461a1517cb68277ed25dc8351e3d9e0511a787dd0cf2c9717968d6f"}, "a0c28fc5-05d8-41d3-9bd2-fcc5cfa65b4a": {"doc_hash": "2e93623561f2978e282caeca6fadb71d09ff0aaa8b433993fd53236ed64c5e89"}, "d9dd41ac-8aa2-4467-97a9-88b9db0a84d8": {"doc_hash": "f1c6dfaf985be3cf6322cd4742cfbdda673c46b49483cd8b0103c476cd53876f"}, "c74bef42-e58f-4600-b5a8-d2fbccf4fb0f": {"doc_hash": "e8534cd64ba56556c7e1c4741c38cee272db289d22787a798fad303e4b367230"}, "093f2e9d-6aab-4dd1-b135-86b5d19a2cc4": {"doc_hash": "e3f8edcedddffeae11c5a79830a1037494f41911f1dce68a7c9eaad111786463"}, "57de6d4e-95d5-4f91-a123-e1a36afea9c1": {"doc_hash": "3e3cab5cb51bea7e7fe58e0caf4013cd0f7ac5ce128fb970b3e5d41ba9473811"}, "9e5df34b-2a6c-41d3-8c54-564d339ae9a4": {"doc_hash": "ae4ab154fc3c9cfdfbf50edef0f16e1d0182005e32d2d60d0fe1376015152261"}, "f115bb9b-2b87-4288-953c-f20e45ff9fc1": {"doc_hash": "e2b56e0d9759e4588c2506fddd2b94b5d5073169e72dbec9ba112744190879cb"}, "347315ec-253d-4b49-b83c-b32e27a95544": {"doc_hash": "16df6f04ffd50a3c7a7b56c09dde54de1b2a58b61c81319365a8eaf92308aade"}, "bf9bfd70-d6f0-4f68-900b-50276268e19a": {"doc_hash": "7b652a3e364d71b65855adaf32dae068820953c1245e080103717ebea6db4e27"}, "f563e7ed-2918-4695-81fd-08cf02679a66": {"doc_hash": "1bf67a86b46c604ffff10564ecd714bf883ccf396e0c6168008e491fa60dc871"}, "e95ef82a-dd67-42ab-b785-0b8eb04d22fb": {"doc_hash": "5eb3aa1bc9c7fa1959a058f5b1556956d040f31042c94e5211650068e7e8147f"}, "f1dcfb1e-6fd0-435c-b103-87c877d79d18": {"doc_hash": "4cd03a233c176d7172df1f83df5e0aed29506f6889678856d157ca430ee7a1a0"}, "30fcf9a8-04b9-4774-a290-840e5169a5e0": {"doc_hash": "d375790a500952701ada733b7369c3495fa9fb192b4ed841fb3c280add34aa29"}, "fd6d4c43-2e78-4a77-a30c-e1101d7f267c": {"doc_hash": "1db9b4d03128faf27cda357c633546a7305a92e09b98206c00e1fa02ba2f99c9"}, "5658da6d-6827-4cfd-8f88-86b2dc87a1e2": {"doc_hash": "3a3812a31961f707731b3ff7cd8ff905c93142a40fa80e93c8b2dd1bd38749d6"}, "183844fd-a2cb-42a9-836b-13a5cd5bb9b9": {"doc_hash": "ba4da67ec1322b2b6deae128e402274c2c112e1b971ef845068b454098f198d8"}, "9e71769f-c97b-465c-ace2-833cc64f6b4f": {"doc_hash": "e4d2171c66bf6015c0151c2d5da94ee879be9c310ffbb89611edf4cfe67e9779"}, "f73f02a0-ba7d-4d26-9361-4c039781b924": {"doc_hash": "123fdfe8b71221c0b127625cbf5519dc0eef8aba4cf63aaa551f12369a6ad99b"}, "a2ee8b75-fd02-4176-8760-848b15e9d6a9": {"doc_hash": "03a6c419d910383b05bcd7c6125adb8e21da5bbb3c6a671f75668da5aa58c721"}, "e85fb5d8-87c2-42c7-8559-0daf40607a34": {"doc_hash": "3d25ac200555e0aeb708ecaf237d2119e7c263a0f904ca38b3a604b008261688"}, "10e6c215-3511-4944-8ff1-dfbc9ee37dab": {"doc_hash": "44ba2a7bb6a15615572415237716324b9fa9873a7b6e28789904e19610109bf2"}, "dd2a6354-857b-4f29-b213-010226709082": {"doc_hash": "86f87d0f51bfa2f84f1b2625e72f8a16941277f9d843ace2fbbfe047e72d09bd"}, "54dc5a26-f401-492a-9571-2af41969fe79": {"doc_hash": "9836318acd6d95b0b0c9a77e4d0a1b6bec544601daa4f1e73c947552b7d132f6"}, "917be18f-8d2a-412b-8f05-c5a599d3343e": {"doc_hash": "a27fb97171d3ee36bd75ff0cf1f1d7ce85a6f16213b800bcb784418b2d26a697"}, "7fe0a61b-3435-4f36-a819-4c6bdb894523": {"doc_hash": "3649ee019ebca95c4051c727a41df26bbe959c816b2180890709565ff13334e3"}, "9f063f5c-be2a-49fc-a7fa-8ece9cf447b1": {"doc_hash": "2af6cf4ca90fb87820a3996d8c02a6a4e4179b6b93733f69a4e5354c5082228d"}, "85c83823-db05-44a9-80b4-f7d5c55e1d73": {"doc_hash": "97953ae55800b096aafbd0616d54133d45520f032ff40be34923f22f3c270740"}, "0c407ece-0af7-420a-8daf-b29a6c3b07a2": {"doc_hash": "fc69e5a8fb9c3ba32f568b43dd82ef3549ac09d39ed320d00df6efd9649c9095"}, "6e18eba2-7b31-4f5e-8194-f3c49b04577a": {"doc_hash": "c1febe611189b53a4b3c8c9bb8db736f183d707cb758207bac1934bf8124ee53"}, "c89a58e2-49ab-44bb-b5ba-de444fea94ac": {"doc_hash": "9ef1b885a094003a211651c7fcfcef20b1d5908f29c1c095ecbed43ae1b830b9"}, "0f6ef32e-8282-4782-b688-a5584d519c41": {"doc_hash": "2b4e2093da4b2b8db55a63c45031f59c8677225a5c95c8c3cacf05d66344f540"}, "019fed00-da53-4202-8a61-228b8cc8f1d5": {"doc_hash": "044862e8bb39cceabe34e39d57f2d8da8a2c41a4e452d537bce084e4cde8904c"}, "c11c2700-a60a-4ea1-b81b-78ff5298bcc5": {"doc_hash": "9d4182606a4cf2d84313d5c776f08a94729d01a959f861d97736f53d9573f309"}, "7275ffbc-4ceb-4b8d-a295-5a80cc46b027": {"doc_hash": "7cb6210fe4a99438f7dbb617154f8c879d7e0ff4c2d6ca44b89ffd9a66db4e6c"}, "3e4e3074-122c-4aa6-887f-bb3306d80bb3": {"doc_hash": "0a33e3cfb33e4dee5099100c62093404f4c9b06a40fde5b6721eca49356dbadd"}, "9fa01623-0348-40cd-8b5c-87ca69e5678b": {"doc_hash": "ba39e16afc904f3d06f23f50f148f55fb9629b01d9f137c4f5e34a57b455d7fb"}, "8a05e471-4072-42f4-8360-96a8211333ba": {"doc_hash": "26fc6c3ea650290e8ac85d633d6ceee145da71aac340dfa97168914ae1d5b2de"}, "db102b6f-ab71-4ee9-a851-6aa5d8355447": {"doc_hash": "76532c8fd9a9a06741de32650b29ba9b4415ab9033feb9a06cd4f11ecce35398"}, "16f67917-3ecf-4982-8412-fe0c652d1c2a": {"doc_hash": "8e45e1354e30df2882b67bce934ad03ba6d56017c82541e1ba49de7edb9741b3"}, "8fc458a1-63ee-4526-bc09-8227c21bd13d": {"doc_hash": "6b6c8e4c1bac955d9d840016523c06b5358302b2773ae11da47e9451198e79f2"}, "71ae0e58-1d2b-40a2-bddb-1b5cabcd82cb": {"doc_hash": "421785549a7f4216c55615e0029af5e43194f8680d2d221fec29f08438be4562"}, "64092970-53da-4b4f-9ec7-a65a0ca58551": {"doc_hash": "01d7a7cc560a000461c2556b7f7ed06c14496fa0c4766157e671961ff3d89f7e"}, "43a1c15d-f815-4260-bc04-3bf73e295bf5": {"doc_hash": "b9694561b2b33efbeb7b00879a4be12a36f7e1ed95bfa7816c3982fd904130f8"}, "6abead3b-1d43-4181-89c5-688e14e555e2": {"doc_hash": "cbfb5459633ff388be4d09b533513ecb4a3d613ab3c75d2f6aa0798217789564"}, "aaa9bb82-bba0-40ad-8bc6-3571e4698bc0": {"doc_hash": "a076a69990ae4696b6c33c58a61650cb5628930a2e207d7bef537e4c029a8e7b"}, "a71869a8-8ef7-49e2-b64f-d365d3df7ca1": {"doc_hash": "c7bf945ae1a5b07d832beb317ee4a7852dc928feb8b9ef7a4bded823a5f09a7e"}, "51b5bdb9-9108-4a0f-a89a-467bc99288b7": {"doc_hash": "a459431026f73256b45fc4122b714cac8d6057e2e2db97f4750e3a75f6ae9de9"}, "5557f947-a70c-409c-8946-926445b10890": {"doc_hash": "fa6b560c50177c0196de340edf5342bf24d7377dee33080b093538d7c48a007a"}, "980b3526-c63a-4596-bc4d-66c986b49b32": {"doc_hash": "37300a4344097c022cbf8a286a9473a14e1f5b76c0509bd4de249176ed65e78a"}, "a64f3653-738f-4768-902d-fccf3e1be5a3": {"doc_hash": "de8d4c2a3e9f9b123a63fd317b189a9eb054a6878660e0d97bbdaf0dcecf58a7"}, "b9fbf0d3-6f2c-4fc5-bfe4-45c1cc377beb": {"doc_hash": "f9ba6128b6249adaeca662b6818f419df4546387024e11be606c4d317fa66331"}, "76763497-e58f-4819-8640-da876e9da6b4": {"doc_hash": "64949fbaa0ca616231b11d8a9c4044f0e28e540fc827b78e7e7250e96961ef97"}, "c35d9e44-90b5-4766-9d50-a2c38a050015": {"doc_hash": "f2fb25e897962df85c330bb5ff227e4168122fe3aabd94b42f085022c9f73b43"}, "09eb4de1-b223-451d-83fe-6555c0de6742": {"doc_hash": "f8f1699c968c5f2c94a43f5fd0924bee4096f28b8366359ed83de6f51b43251d"}, "163a2cc0-c33d-4e34-8c74-2d69e3247bea": {"doc_hash": "19e657d2a769a3cdf1db8d2eda745f8c6fce30c99a77cd8a28f803b9eb202ba5"}, "b7f19304-dd58-4e75-a745-96f9105e9fe2": {"doc_hash": "2656c6574131cc2b841f6230fc8ece3e2abe0ba0a084057476aa8d99e2b7da70"}, "7b5c9401-281a-40b6-b72a-77d5206685ba": {"doc_hash": "1aaa599f611f3952f5f2fbad314b0b693b6716ca524ff3cf6f7b0d8ac231dddf"}, "7cb08143-5e41-44d5-a8da-1e626eb271f1": {"doc_hash": "74aec4c387959cbbb9ca6fbfbf8707ab66b737d24ec649835e29db6d49882bc9"}, "fed2e603-ed06-4762-89b6-3deb2c5935b5": {"doc_hash": "9a21e211aa6971aa858ad949628daaeb9cea32566fc6351467cf9226a03b7857"}, "6d7912ef-c68f-4bce-86c2-f04793fe1dc0": {"doc_hash": "7b47e9c26bb137f22e15230d3d68ee3cc863595f0e7859be89c09dd677d7af1e"}, "78f8c795-ea9e-4eae-8e07-5025fefbcb90": {"doc_hash": "59f1ab12372275eb67bfc7c4710580afe54744b7532feb0d2e2ae8fe9819db0f"}, "75d5e96c-a8cf-4bfa-b131-4b850ed8ae43": {"doc_hash": "4dbc9ebacac8e84ed4bfdda3caab376e1bdc8d0261c53929ee9db7c5fae69a06"}, "c817712f-d768-4d9f-a7fa-5deb72ddac90": {"doc_hash": "d0fcdc220416ff8d024c1bc1375b6d13c5e42960068dd69f121466b5963ac45f"}, "3eff2c07-e375-4bda-8031-c81ef3314f89": {"doc_hash": "cf6f1a9b7effbc5a8e4b1f5b192de70a8fc048f833c485565cf60f25b335b04f"}, "5a8eaabf-e0df-4ee8-85af-aead151d7ebc": {"doc_hash": "9adcc19f5e155554334ffc9b1420678685bfb85688a99c53b671690bfe8337a2"}, "64a779fd-74a5-4fe1-8536-04937d022f0f": {"doc_hash": "db4d6f2fb369ad56d0de2c562fd3007ebcfc2b788e131a4dd6062915f5afd959"}, "7732490d-fd9a-4f47-91ca-8c28687a3ff6": {"doc_hash": "ff17511f9c4a11962d36d522d870aa0be79ac46cda213a454a85b920b90ef5e2"}, "3110282f-d8ee-4b21-8212-1e96891acd23": {"doc_hash": "7df64cc276684e509927e0190cd028e581b5cd318fae3f76ace17e5a5207eed3"}, "cfab389d-3054-4336-8e55-26d73f4cea03": {"doc_hash": "c7d7e06f6c658750931c78af45d5cdfe6820456b4901fbfcd946fb30faf7acfc"}, "b92cffaa-499b-418b-8857-39ccd5a2202c": {"doc_hash": "2a758ba6fc90d53f2b06a837d959bc0cea3d80421dbac852022d2f9196b9a89c"}, "c8a10ff8-5321-47d6-90dc-8e262e259895": {"doc_hash": "c6d24bb6777d326ef1c673b0fb604a211a7969d6844dbe56fb0ce771dcd16c03"}, "a6bf2401-22f1-4ba2-b289-c183ea1a9203": {"doc_hash": "d9990b4e7912073c31c33de09fd86f4c506a5727b00b6a26c3abbf406d1bd0a5"}, "88a94c02-45ac-4f28-9aca-67851e7c1bc4": {"doc_hash": "5e5499cae169c1fc8ae35b5c3ec5857da68cb5c12344e1b524686f88eb8e4753"}, "90056080-e722-4354-afc0-ccce76552a66": {"doc_hash": "48e10b6fbc6eb59ae7befd74d12952f3f2ee821640164850c669e3e4046863e6"}, "10cddbf2-38d8-4e46-b8cf-94fa7561d4e1": {"doc_hash": "8fcc46d3eb408769bf03f079f9578911f571a04878b099c9a55747f32959d947"}, "82d51892-61ce-437b-a2dd-067ec10b6155": {"doc_hash": "722d0c40490bffb75f3c211eeeea6da5755595024ef4253d97f511b469ae755c"}, "3a6a55f1-6d9c-4dee-b23e-ddbf9db019ca": {"doc_hash": "b389160e7882cef3d0b8fe3d9bac9d428a1b4c97701b3a9ba4890359835f37f4"}, "00af1a4e-94cf-40de-830d-d93570989fa4": {"doc_hash": "831455ef71091f233921580d199f715ad39b6fb9e03c60adb2318cd8b580e249"}, "273d027a-3b3e-41c9-b6d6-ea8efc3a679c": {"doc_hash": "60190e076b252ad2bbc5b9ccc401279caa2ab763f80d8f2949ab6d26daa51a48"}, "bc258d10-f3f1-4693-b227-973ff974f16d": {"doc_hash": "f9f8cd03dd114e58ca09b1471f78173edb572be1b4a56c02394a5ff00a559b43"}, "3239be1b-1c0e-4a4a-8361-663be0ed3c73": {"doc_hash": "2e23a4f152fac7613968a58318162cf729ac04c3f72269bb7364ba62e8b4b754"}, "546426ca-21a0-47b4-9661-e8f2495d2769": {"doc_hash": "725fc7f0dabe6f9c78a85207de4c04eb1ff6321143f09e220876573d5b756d07"}, "b7741213-9ceb-4385-bf59-62411fa9d8d2": {"doc_hash": "fcd22d1702790d82845812a004434a6eeae541dd5cd216202df76f0baf553404"}, "54bd5851-7151-434b-8f49-d024ecfe1222": {"doc_hash": "0946cc39d982b56ca4fad750935e7fd16c0a07ea86b0d9c700dc7da7a93d7124"}, "c5c336bd-8240-44c0-b522-4a6a923c49a4": {"doc_hash": "2f137698180702fbf55fc7291ddb1b3a004d04d01a32a6fe99b93a0b2edc8c00"}, "830e2c34-86b8-4a53-b3ac-e69854db7132": {"doc_hash": "f21b03e64ee0d2eb5a4ac2983e2cefb3c958ed789dcf52c87a4e6f6795a9f705"}, "953faea1-1bb9-494c-bcd9-642c51d78fc0": {"doc_hash": "00098aa08224bc5cbf75d25ad047862f1e9ed7a6b728e3e421282cafe0fe21bf"}, "719e8fec-2706-4e2a-9bf0-3d0e3d0c224a": {"doc_hash": "92192c7e21b5897b19ab3525ff19d0224343b1e36cbddd6ad3aa76406632befa"}, "7c0692e1-a293-42ab-a093-dcd19dc6e376": {"doc_hash": "24b05f3ca9dd4da69395d1070f44e205a88f3acc6ec9d8fbf61e31039f296834"}, "e0cc3c41-cab9-44a8-85fa-1f25a674b638": {"doc_hash": "4e891d77ed6e75e77f791aa583236725851ad5b81d409b3486a9626ff863cdaf", "ref_doc_id": "95ad9605-f0d4-4bc3-8aaf-9212c12b29e5"}, "672501e6-4f5e-48ad-9678-43e71b643117": {"doc_hash": "29958ae481efcc84e56026861d1a73563246c06bbf7e23b141a39ca585025f8c", "ref_doc_id": "f2ca1935-cb5a-4945-82f5-f27e8b1a6f96"}, "886c95dd-fc49-4176-b86f-a255f6d3d812": {"doc_hash": "d7bae9d4dd348c17108bef571851f6de4b608dc46d50d5acd4c9c6c934905b88", "ref_doc_id": "4f5c7665-f156-4b41-ad35-8df8f9ad013e"}, "7361ef57-4b45-4d2b-83bf-827b953a57a7": {"doc_hash": "08844375b2dab648a82eb40c62b2de6a3fd414971b0f2c2e2be32665b6c8b0df", "ref_doc_id": "4f5c7665-f156-4b41-ad35-8df8f9ad013e"}, "7dae2f14-5878-4730-96ff-e152c953c045": {"doc_hash": "64ab28a665acb067d5a2a8c0dbc8848e72e731f029d1b7f9c760bde3659ad6d6", "ref_doc_id": "0d7d8577-801f-4242-b4fe-8efb3010aebd"}, "cdc3816a-8895-46ed-978f-6ab3eab87cbe": {"doc_hash": "95ca265e3f37b39b4e26fc775d6f9ecc111feac1dbc8077152b30db85cd1ef32", "ref_doc_id": "c5fa9ef1-ac5c-41b9-8ab3-a33983c68cbe"}, "6c52d986-5eae-4ee4-ac40-61dda63ec8c0": {"doc_hash": "9e8ebf95ab80472ae2a1bc8443926227a8f1ac5ee0606987db6a923f7bcf8c87", "ref_doc_id": "7aba2b2b-a34a-4d73-b81e-7a815a067512"}, "cc040959-623c-4040-956b-782c6dd734a4": {"doc_hash": "1c9cde3f5a1d985e047661a8432f1be334f48539227aea956cd63d926bfe7091", "ref_doc_id": "a56aa6b5-5dfa-43bc-84cf-4dbaadc5138a"}, "95d837a6-2e21-467d-ad6a-b19dd90e5eaf": {"doc_hash": "a663d4a67318c9251a0fdf43ae8665acc5077956b12dbc373fc62c5dc53f801e", "ref_doc_id": "810734c5-8988-406b-84b8-58d212b43716"}, "e82cf00f-fd12-4db3-a391-bc9f361676f7": {"doc_hash": "a867a7f26a779c80b549895171faf6ff0a9742946a75ef03a45dbc1d6d8ed25e", "ref_doc_id": "b7868653-afc3-467c-b796-4a6382eb5407"}, "a8367fbe-ea13-4c0a-a26f-ed5d9093c1aa": {"doc_hash": "ca3d33334949a609d6d441a675de2f4b1c868f735f465d2389dfdb3def844f28", "ref_doc_id": "803d1f18-0bb5-4e29-8f82-7dfb005f7b4b"}, "14ff226c-ce77-47bf-adf8-2d4bc8714d91": {"doc_hash": "ad9ce0bbda79ea8b80c70a2c86a7ac172f9ee795a97bc5040c3795fa6fb90d6b", "ref_doc_id": "3fac0a30-ffb1-4e20-aab2-1181fc1ac6e2"}, "483dce61-d77f-4626-b5d0-65cad7dd3eac": {"doc_hash": "e50f57f6fd3b20c375324816a847f68847636a01fb58fddb54e815cbcb74afa6", "ref_doc_id": "7baec97b-de2b-443a-a08e-c9a78636ff2c"}, "6c781c20-22e9-4f6c-a8bb-54c760b70d00": {"doc_hash": "99daf4cf0b1d008f2377f7289f75460aefdab4ccd5665de2150aec5c4e83dc1d", "ref_doc_id": "20054bdd-e5cd-4958-a755-662c0a52a0ae"}, "d610c7ff-9365-497d-ad24-d8fa79697341": {"doc_hash": "e9a7e1b8e820a72d608167a84b76efb22b69eebfae573104932c06b28ebc46a1", "ref_doc_id": "7557ca6f-4040-4bab-b0ff-59655ed13169"}, "72df2313-fdd4-44c8-b7d7-0631a662bd5d": {"doc_hash": "9a9b29e46d5e4460255873da168833d9be157f4298af44cf98c299a7cd5c43b4", "ref_doc_id": "62b3ca07-dcd7-4326-9f1a-f09a1b21b0af"}, "ccad89f5-e782-4a9a-b64c-4425f024c759": {"doc_hash": "f276f8215948ef59cb2b4d0830c1fba26dc3d15746f4a749a28f07a215865060", "ref_doc_id": "bdd73f33-aba0-48df-a07a-903239b03a0c"}, "3d622c97-5302-4bbb-9bfd-182b51c66ef8": {"doc_hash": "9b93819123cac1de47cafa5ed290dbaaf8be03beea00ee46c31a3484fc66f92e", "ref_doc_id": "cf3bce4b-776f-46a0-bc31-9e9eb747a0c0"}, "14450234-6df5-461c-a1d4-aabd69f184c2": {"doc_hash": "6e9677543f9d4c8eac253375952e9efa63161d45e77f8326738e03e40b84faf3", "ref_doc_id": "052f529a-4826-4498-af43-c34c1f7f51df"}, "72062af9-3fa2-4e19-9042-58b69d05c586": {"doc_hash": "81818bc106e1a10422b79a07fc454fa5a18eb7091ed138a7757bd1d4266d7c4a", "ref_doc_id": "106d0f0a-9104-4283-a354-95c62e57c866"}, "04efc28d-9f1f-4d85-8319-2c0a63072f69": {"doc_hash": "e38c64a083116e9cb1c5ee90f6fc5588247f9893273f8beebc730971c3a166ac", "ref_doc_id": "376f4579-e99f-4f91-ab15-163e9fb527f8"}, "806bb1e9-799a-4f9f-9c52-d63b275c6d94": {"doc_hash": "6ffffb1d2bffd7dcbdf5f83b2b5f46b02c763cc33380e440f7c16a0f4be9f398", "ref_doc_id": "1e5b25b5-d5cd-4df8-8d39-641ed9f4be5e"}, "d45bbc1f-1d6e-4c89-842c-7f813e91a44d": {"doc_hash": "3e6e464d1996256e6048bfb69180ca14776ce7e456ce815a246745274bfe3969", "ref_doc_id": "186a0c4f-4cb9-4256-a5f4-53c96ddb73f5"}, "38eef608-e212-49f5-b019-c3827e95fcc8": {"doc_hash": "62e027c542d81702b7b6e846f68a2214d201ef20147720a7b43146d5c0f7f3aa", "ref_doc_id": "dcf65fc6-f88e-48b4-87f2-1900d41de02b"}, "f716fa92-79fa-4e58-9e5d-554f43d5c217": {"doc_hash": "09c4ab173e56bc560caa6ca0b26543da1ddf02edcb873b9eaf4686de3b03fb46", "ref_doc_id": "eba9096b-64db-4b8b-94ab-5f02e1ba42b9"}, "26bda402-a0b0-4b52-9c33-cb4c37723d1f": {"doc_hash": "16d764fac3d4076056d25c20954bd9ac3052e47c8cc8d1f161b75be3cfc2d897", "ref_doc_id": "434464da-3718-4906-b4a9-4f0dacf6e2af"}, "6c78f99d-86c0-43f5-bd0b-4679c6062b4b": {"doc_hash": "68df93ccca7a583d5c799b2e539d4ce2d81b20e698c7cf26c84bab7c45b1bb78", "ref_doc_id": "e9bbc953-907a-46ec-a9c3-31c23d7ea908"}, "965b3ea3-b7ef-45e5-a898-2a1919a5f259": {"doc_hash": "9f313e9e86df4bb1bfc366840e6115449de1d666aab3ea37295b97124b2721b6", "ref_doc_id": "4910572b-9e9c-46f7-b230-12f17e471e52"}, "95ef1cc5-6086-4ed4-a48a-6dd39e3a1c26": {"doc_hash": "6d337bf88ed23d7559ea78447aac18aed35c9209ed86a20a93e418523d568b83", "ref_doc_id": "fc326342-a839-4cc3-90f1-38c1a0e27f01"}, "05b23e8e-78df-4abf-a5d8-82c2537ba0a0": {"doc_hash": "e09fba04594e6d4e65ffa9208e77e4b4cbdc4a7e18c5088f5dc6dde0463a56a3", "ref_doc_id": "029099c6-84d0-406d-bef8-cfbbfd73c0b0"}, "b97bdc72-dc8f-498d-af3e-5297e4ce1aa9": {"doc_hash": "18da681a4ce7bb7a2d4dc540ab32e0965943347e2c9248db98ac71a2f0543295", "ref_doc_id": "cee08ed0-4513-4e2d-9d2d-a97de599c4ff"}, "639e14a0-3ccd-4a7b-bd12-7a53bc437331": {"doc_hash": "bb9d3c67d2c3a5e0dcb5dd3f0bf565d7fadc2fdb106171bd3115401f9d96fe44", "ref_doc_id": "d44e99ff-027a-440a-a126-6d8b141a29a3"}, "d50f388c-4891-4cbd-80f9-77f515e1bc6d": {"doc_hash": "e9d0484f5801b643a65c22041ad9f749ed7c83ebbd682ceea757f54a71380f1b", "ref_doc_id": "85cebce5-42fb-4a8c-b87e-7e952b0f483b"}, "eadff117-e0df-49dd-8027-2517eaea58bd": {"doc_hash": "b875544516bd1f0843d069c21f32c3b519211eb58d7a2e54898e6cbd50248eae", "ref_doc_id": "b5efa891-4a65-4d19-ad79-3adfb61d5a50"}, "ccba5cf1-3d9b-4d8b-b203-a5ec70676e44": {"doc_hash": "0ccf88638b52256dedd281be9e070122cc3ab0b6c438ea43cd8d3723a802f737", "ref_doc_id": "fb3db9aa-6583-4276-a46a-10659bd4fc10"}, "3be92cbc-3c55-40c6-8f74-d5449eae7262": {"doc_hash": "911395fa49d9c9a43c3bb62912e733be9bcdd3afec2d1d1b1caa43dc269ebba4", "ref_doc_id": "3e5ff74d-d112-4fb7-8eec-fae6954cd457"}, "3e5a72b7-6352-44bb-be25-3272f9ba2341": {"doc_hash": "c4a6da7e7c141534e2acd4359e740b36ed8ecccb437a859abb7e4602d648aacb", "ref_doc_id": "8e99f547-c15e-4eda-9493-c0c5f39beebb"}, "5b76e307-f671-4a5d-9643-516c3f7c93b0": {"doc_hash": "dee89429c7450813ace24902b9649b05ee693f2cb8ac8afe67bdc5fe8c794027", "ref_doc_id": "da9b7e1c-d5de-4cd4-9dcb-e3e8a66220be"}, "76c0efbe-1b9c-4960-9174-35d2ec36abeb": {"doc_hash": "f4b987b28af80f9809bef30e28fb579522213b5485f0fec56e10426db6919290", "ref_doc_id": "33218217-3d78-4c4a-824f-f82492907e73"}, "dafc84dd-d6a1-40ad-8d96-43d38d1c27cf": {"doc_hash": "2297e910f78008ca96790fee35c9643fcf0d14a76321f95fc99f3edb39025253", "ref_doc_id": "20b56c6a-7e30-434e-afef-c9506ecab9ec"}, "04e0cef6-36a4-4b76-84b0-f9a296bd310c": {"doc_hash": "4773a40c89447c28134dcc021f3196c9aead43b287ee0c5c30c8fd93919e33cc", "ref_doc_id": "545eeb11-1a1b-4638-bbae-0ea57477cb54"}, "da2f741d-5e0a-45ea-bfe5-09c851bd80fb": {"doc_hash": "a460e2180ff05eac6e2eedfa93ba250fe73f6cd98ef645f27ad66eb3eea930d0", "ref_doc_id": "e7b2c337-0599-40d9-b210-cec1d9a7a25b"}, "07574e85-5069-4f67-be09-6cc940e3623a": {"doc_hash": "6fd702e33e02d4d9bda47a90fecd5042c0d71db619ac5394e89394cc04312286", "ref_doc_id": "6e4acd37-ba41-48ad-921f-9b65dc9f47b2"}, "3feed60a-bf50-449c-8f93-514c0066c30e": {"doc_hash": "215e56593230c55c2e1b58ca0e174d66258d83de00ec68080a841d780ee6ab3c", "ref_doc_id": "604cb90f-a657-40c8-8353-c58df0a35b61"}, "ef3c8757-f0b2-4a18-b7ea-a5c24daeaeb4": {"doc_hash": "c69112d708a8b50761e9157b9d85fbaa899dd930642dddb1c1b6122ee120f974", "ref_doc_id": "62b59f10-0a7b-4149-8fdc-05b0d5fd1ef3"}, "41907683-eced-4a7b-a812-64f10df7f0d3": {"doc_hash": "05690cfc38671821ac1559beb4b91e0655b4b9170e6e8f042909180727018020", "ref_doc_id": "6cd45afe-f968-436a-8e81-2dd2b58194f5"}, "ac422644-bd04-4d7d-9f64-3b8e071a4fc7": {"doc_hash": "f0620d7732dc8bfa788660fa1b7ce4a779c21d7b8d8808117a685c603972199f", "ref_doc_id": "f20c0e72-b4f0-4ada-bb96-6cdd95963ee5"}, "d91831d0-712f-48d8-be84-032035486903": {"doc_hash": "885ec8b91707bf7a5bf93baf7b875039e3b39edf97d8f8721acbff88f53edf8a", "ref_doc_id": "7deb220d-4a1f-4237-80b5-33f5c49d0e2c"}, "d1da3857-695e-48a7-a157-2525c080587d": {"doc_hash": "91280c02a41de96688489aee64abab749c1ffd6e613d064ef5148dd3573138fd", "ref_doc_id": "78440e52-aaff-470c-8781-fd2b5b3fc92d"}, "c915dd27-2f1d-4a68-af5b-a311cb8d411d": {"doc_hash": "12227d90f3f262ec1690646bc35b30b0cd0af89c8858caeb0d41bb97b7de38e3", "ref_doc_id": "dbb76a11-9160-489e-a013-51bee5761cc8"}, "f1da23db-1cdf-4c07-9463-d456e05cc406": {"doc_hash": "be07b187e73af590cd78de945d746c4936c5abef6621c00d1d67e7a9219435e1", "ref_doc_id": "a0c28fc5-05d8-41d3-9bd2-fcc5cfa65b4a"}, "e2c740c4-5275-49c1-9ca6-3961c9bb1245": {"doc_hash": "5d89d65760feaf89deb8a6dac2b6e924a935649a46dedfc945d34bc48a28cac6", "ref_doc_id": "d9dd41ac-8aa2-4467-97a9-88b9db0a84d8"}, "cf83cc77-2620-4ce1-8f92-d71f9b153818": {"doc_hash": "8486b85898ddd1415d31b642f02d6038aa37612dd84aedf3ea4a5e700b1fd78e", "ref_doc_id": "c74bef42-e58f-4600-b5a8-d2fbccf4fb0f"}, "c9169c54-efc4-490a-b76d-8c183d465ca3": {"doc_hash": "fb9b726780df419c5beb793f6c08eb83f76ffdffafa1b3be8a35dafecd637cc8", "ref_doc_id": "093f2e9d-6aab-4dd1-b135-86b5d19a2cc4"}, "bebc0cc4-07ad-43c9-81f9-38e4cc77c141": {"doc_hash": "8acc500f0f1b8828e3adf6f6163ebfb66a032dd21f3bb6c8ee7419b012cf6c6a", "ref_doc_id": "57de6d4e-95d5-4f91-a123-e1a36afea9c1"}, "b2f6183c-aade-475d-84ca-6e4ed9062030": {"doc_hash": "d8c381d0a489d4e00ff4f19724d46cd6bafc55f8d24382751b95e92c08630b29", "ref_doc_id": "9e5df34b-2a6c-41d3-8c54-564d339ae9a4"}, "9c196c65-be91-486c-8da0-8823d1084831": {"doc_hash": "317a2ebfad9a164a74a83acf62bea06a2c196f6cf0c4ccc4d0756c5ef1d6d904", "ref_doc_id": "f115bb9b-2b87-4288-953c-f20e45ff9fc1"}, "b0238ce9-0b10-4082-a853-8f63b1883232": {"doc_hash": "c11bbf02ea924790e14e5108f9dc7aad9dc2f2161cf7097814ee316ebea08902", "ref_doc_id": "347315ec-253d-4b49-b83c-b32e27a95544"}, "418bc452-5f15-43c3-bf53-a23d59901b16": {"doc_hash": "bcc21ef42b0e5e905c1a8cc65be857f2d9b28f09c0ee74b9bd190e8b7a0a5216", "ref_doc_id": "bf9bfd70-d6f0-4f68-900b-50276268e19a"}, "da535acd-26c7-42ed-8e68-3e0e24c645f9": {"doc_hash": "a8e98e072ef620348dd1e22da84879e03dcea763e94d20cb7870493dcda6453e", "ref_doc_id": "f563e7ed-2918-4695-81fd-08cf02679a66"}, "84686a4b-f84e-4fcd-84b8-af52047746a7": {"doc_hash": "973f5eaae6cfebb24757d5c86750a926d2ab1fdfc5108695201cfe8261846390", "ref_doc_id": "e95ef82a-dd67-42ab-b785-0b8eb04d22fb"}, "664c805b-40fc-41d5-92e9-a94a3755e319": {"doc_hash": "a3e3e2678f26c5725f4823e16a457c8d6bf1b11f2c8faf40551444f373eeeee4", "ref_doc_id": "f1dcfb1e-6fd0-435c-b103-87c877d79d18"}, "2bfc3a6c-ddcf-4b53-b1ea-7c500b1fa03e": {"doc_hash": "0823f1e041ab7e9e944c6290cd2e5377bc37defd1575c05c9d1355cb36a2b6d6", "ref_doc_id": "30fcf9a8-04b9-4774-a290-840e5169a5e0"}, "f68991ce-455e-4d5d-9f9f-7863d96ca8a7": {"doc_hash": "add1bc3ca771a8956a72658e88793ca61d4d31c318a3b903a40d9843df6cecfe", "ref_doc_id": "fd6d4c43-2e78-4a77-a30c-e1101d7f267c"}, "932b775e-8193-4648-b194-7a77e3f90baa": {"doc_hash": "e36aecdf73a36197ae73366e936804fe3fa9148452fa7c1f5e46ad12bd0bc13f", "ref_doc_id": "5658da6d-6827-4cfd-8f88-86b2dc87a1e2"}, "e210a659-6bd1-4edf-ac06-7c9045bf2575": {"doc_hash": "757004411020cb5802ee145df1b7d217005ae774b9ea831b8b3e503eb0ae4a3e", "ref_doc_id": "183844fd-a2cb-42a9-836b-13a5cd5bb9b9"}, "59e0119e-dba2-42e5-86b2-9abef45b4ff8": {"doc_hash": "84da9a1944c796e35c3721f59bea17639e7cd22187ff41e831a3e49eb7f2494b", "ref_doc_id": "9e71769f-c97b-465c-ace2-833cc64f6b4f"}, "7f0b889f-8a6b-4302-bea8-29b61576b350": {"doc_hash": "3e8470f9cee5491dcfc9fac88cae9d3e42cee04dd8b2a399fca7e49e6a8d1f45", "ref_doc_id": "f73f02a0-ba7d-4d26-9361-4c039781b924"}, "e10b80ab-d049-4e06-8391-33eb010089d9": {"doc_hash": "a95e1c181b962a217452139b9ce1ca9abb2412eae7edcb1162ea105e2acf67f5", "ref_doc_id": "a2ee8b75-fd02-4176-8760-848b15e9d6a9"}, "cd6703cd-c79b-4e2c-b870-1da40ebd0fb5": {"doc_hash": "18a7be79a2370945b8329b722a5868bfdfeed9c10d3d33a5e9992407db20f3d0", "ref_doc_id": "e85fb5d8-87c2-42c7-8559-0daf40607a34"}, "63192a43-5a8e-460e-8039-adcddca6b484": {"doc_hash": "345a458f6b260ff75ecf94998c385378a34eeefb6261baee0493ca97e21c8a64", "ref_doc_id": "10e6c215-3511-4944-8ff1-dfbc9ee37dab"}, "847c1f20-1db1-4e93-968a-c158ef1ed826": {"doc_hash": "947fefe141417cd9fe829a869bcd9380392cbad68afd2823828a6afbd464be33", "ref_doc_id": "dd2a6354-857b-4f29-b213-010226709082"}, "6c1e93f8-ccb6-42bf-b74d-f6fcccc62497": {"doc_hash": "de15fad8e1a0074cb3aa2b40e378dd59e83ecd9e2952d5d67c138d6814d8135f", "ref_doc_id": "54dc5a26-f401-492a-9571-2af41969fe79"}, "0c210266-1fc7-486f-96c6-9a2f2f7766d6": {"doc_hash": "a677c7397133f47e3dfc8913af6bb096987fabca2eab7766037eaa31c59ec98d", "ref_doc_id": "917be18f-8d2a-412b-8f05-c5a599d3343e"}, "0ad3e607-5b1a-48de-b12f-40160ca7db63": {"doc_hash": "210d8e6410c90a2f6bd1ffb643a3c190755bb3f3d5769baffb8eebb106b2fe51", "ref_doc_id": "7fe0a61b-3435-4f36-a819-4c6bdb894523"}, "7114d7d5-1d90-4a0f-8fed-75e6fba75344": {"doc_hash": "247bcd3a7cae48bfbf9f039b851b0d6321df048a562b2dfaeb0fb560c365239f", "ref_doc_id": "9f063f5c-be2a-49fc-a7fa-8ece9cf447b1"}, "12724248-74e7-454c-b0c3-42a4a83269c8": {"doc_hash": "c0559a68a4f0f5c3073e11a850c19bf80f1de2cc9c412edcee2d20d7045a1e78", "ref_doc_id": "85c83823-db05-44a9-80b4-f7d5c55e1d73"}, "571ecbf1-142b-4195-bd3e-e8761924cf5a": {"doc_hash": "83fbc59a81b8644174ca91593bc30352c568b69f76e0a538b7241eec218805e0", "ref_doc_id": "0c407ece-0af7-420a-8daf-b29a6c3b07a2"}, "3732f2dd-fad0-4aa8-920b-7187e54ae973": {"doc_hash": "7d361bc00554a59cb972b653e8c09c752d257e5bcfd8ade04a40a1d351da287b", "ref_doc_id": "6e18eba2-7b31-4f5e-8194-f3c49b04577a"}, "68fd2dfe-e540-4626-8e34-4627fd6883aa": {"doc_hash": "4fb90e8641aeb33787cb7e9dfdbb16cb0b4f042fb7691748eebcc517c7843a3d", "ref_doc_id": "c89a58e2-49ab-44bb-b5ba-de444fea94ac"}, "4af9bd85-b29e-4eb3-823e-eb099d22d9ef": {"doc_hash": "59b94be7606d91ee5e75a099420829a35c689d4e8c748786f6912ba3cc781dbd", "ref_doc_id": "0f6ef32e-8282-4782-b688-a5584d519c41"}, "7abbc120-615b-4839-baa3-cfa9d9d1abae": {"doc_hash": "dad99f56f28031632aa5074deb250536fc190d4810e6b2e6e1a34eb28870111e", "ref_doc_id": "019fed00-da53-4202-8a61-228b8cc8f1d5"}, "f7f22a94-b0ac-44d0-b0ad-c851eba9adb6": {"doc_hash": "2a0efdd5fd157f229d4d106f66538325b219a50b31d366e56a53a63430eec8c1", "ref_doc_id": "c11c2700-a60a-4ea1-b81b-78ff5298bcc5"}, "ac00025c-986e-4388-aa4d-bbe113b04aad": {"doc_hash": "eff4d39ef8c342be1ee5ecef27f131817d9a16cf2ae0f2f5c499b1b1d2b9776a", "ref_doc_id": "7275ffbc-4ceb-4b8d-a295-5a80cc46b027"}, "a203f1dc-d00a-4151-b09d-a070bbfc246c": {"doc_hash": "c7324d00a9c07de393d55f401bce7f196c9ec6f1298d0fe04c221c8c32cbd591", "ref_doc_id": "3e4e3074-122c-4aa6-887f-bb3306d80bb3"}, "c8413286-8464-445e-a43e-451f8e2b4df5": {"doc_hash": "0ae6270884bd0245113c1b5f67c44434eaa21638ea8e448dc92a193308a93fba", "ref_doc_id": "9fa01623-0348-40cd-8b5c-87ca69e5678b"}, "c935c321-644a-48da-8435-af62f5f3b83c": {"doc_hash": "2074459d28b690ee37d3444d72077600fb0470a2a74c2f672a51da663476095e", "ref_doc_id": "8a05e471-4072-42f4-8360-96a8211333ba"}, "82386efe-a813-4e7c-bb5a-975fa8f69c66": {"doc_hash": "32d15a7a4e237eb1d0e5fde36136d1adfb83357ea46e021a35ab0923b0f1998f", "ref_doc_id": "db102b6f-ab71-4ee9-a851-6aa5d8355447"}, "bbf1d756-a7b4-4c33-a41a-de7df05619b4": {"doc_hash": "17004868f8998f645c418ba227b577d3b19409878eed239aa19f264f5acceb6c", "ref_doc_id": "16f67917-3ecf-4982-8412-fe0c652d1c2a"}, "d846d164-8f4e-4759-bcbb-2a42a0e202a6": {"doc_hash": "567d29d95b399598a083c0f5cf5715d9851f81b060e756ffd62d1f8e3075973a", "ref_doc_id": "8fc458a1-63ee-4526-bc09-8227c21bd13d"}, "84d45653-9d38-430a-a57d-99c96dbe8a2a": {"doc_hash": "6f0b9fd0bf9dc521c62255d7f45a783702df98b321a78b1d5664ae187d2aceb1", "ref_doc_id": "71ae0e58-1d2b-40a2-bddb-1b5cabcd82cb"}, "69cb76e5-83bc-47ec-87ce-e937aef3d408": {"doc_hash": "5e631177392f1d0764f8d22aafdbd17ee55bb9b4922869b7acb6d0a26b26ef76", "ref_doc_id": "64092970-53da-4b4f-9ec7-a65a0ca58551"}, "862fa2fc-fbf8-4a67-b2c2-7cc63c6cd22b": {"doc_hash": "b2497aa270206092b0e0dd87f5de0b1e65d65c9a537c22779c9010c5fe71e23b", "ref_doc_id": "43a1c15d-f815-4260-bc04-3bf73e295bf5"}, "ca1d1c0e-7faa-431a-94a0-f92d55064823": {"doc_hash": "de2cccfdaf88d7bdabea4de2ceac29f0447d04a8c716c57648627bc8d2cdffc3", "ref_doc_id": "43a1c15d-f815-4260-bc04-3bf73e295bf5"}, "7f07a55b-a751-4ba8-8097-f9a9e9e65fec": {"doc_hash": "1cf050d505c0eae6d117da453590c1611bc8f56bfa9a163f911bf49f5b5bf6b0", "ref_doc_id": "43a1c15d-f815-4260-bc04-3bf73e295bf5"}, "b927f684-ca7f-4fff-8b5d-211041461a41": {"doc_hash": "24b97240eeb68223be09a68161af7bd4f772fa278fe68e527f465b6d5bf18bdd", "ref_doc_id": "6abead3b-1d43-4181-89c5-688e14e555e2"}, "8ef84d32-5a52-4b18-bafe-573e6078caba": {"doc_hash": "998b066d4d3d841702a67aa086fd9a6d4ee2c1bee9943aee209551837724270f", "ref_doc_id": "aaa9bb82-bba0-40ad-8bc6-3571e4698bc0"}, "554f3eb7-194f-416f-8f1c-fb43c0cc8b0b": {"doc_hash": "690771af8c1ffcd845a6ad6ad3a179a9171b364f12b6ffdf95beb40681670718", "ref_doc_id": "aaa9bb82-bba0-40ad-8bc6-3571e4698bc0"}, "b9330e42-bd95-4513-a9f0-2b1283dc0a79": {"doc_hash": "f86e22c4f986a33d4e38015f2fa1dd57d9050c7fa074c66a8820a94b362e14cc", "ref_doc_id": "a71869a8-8ef7-49e2-b64f-d365d3df7ca1"}, "d2559101-6a2f-4664-8845-098727c5cbdd": {"doc_hash": "164b55ef5037d17d9633caf76085f90c0297957d0afba6fa9848d6ed9249753c", "ref_doc_id": "a71869a8-8ef7-49e2-b64f-d365d3df7ca1"}, "f30b7af8-c113-42de-ab8b-f1447c523663": {"doc_hash": "788870588dfef550cacd58312923107e8a225a3a876d8a4c2187d4fbc30c07b3", "ref_doc_id": "a71869a8-8ef7-49e2-b64f-d365d3df7ca1"}, "4dac82da-e676-4141-97ed-9b576e358bf9": {"doc_hash": "b73d5b7f2eeb525910b783239145e43929c4bc105a0faa42cf84e47a3ac51551", "ref_doc_id": "51b5bdb9-9108-4a0f-a89a-467bc99288b7"}, "66ea6cb5-aff7-4fab-a6bc-cc462947f5c6": {"doc_hash": "bde164474b2429c041dd0c3f586449f2b4eef92a6574e96607652979009aa0ee", "ref_doc_id": "5557f947-a70c-409c-8946-926445b10890"}, "93600d8f-e327-4d83-9039-135861ba9035": {"doc_hash": "0b89541eedccb02d447fdfbedcaff7665f4e9930a93372ca1a8f144c34bcf849", "ref_doc_id": "980b3526-c63a-4596-bc4d-66c986b49b32"}, "a051959a-dd48-4b5f-92dc-7d02e091099a": {"doc_hash": "179ee0a35ad7794463dce001df2c28a422f6e0e28ec76f054ef052561f2e7029", "ref_doc_id": "a64f3653-738f-4768-902d-fccf3e1be5a3"}, "9562397c-d024-4942-ad37-cb919365016e": {"doc_hash": "674c07cad2a167b5d0601c421f2716c51740403582c8537dcfc07ee8399b67ce", "ref_doc_id": "b9fbf0d3-6f2c-4fc5-bfe4-45c1cc377beb"}, "4f80f3ca-b081-4102-be03-92d5d6e8af32": {"doc_hash": "3b095aa222d7ba49fa5ed64b1fa9ca0ea294884c04703f44015fc9a561a1046a", "ref_doc_id": "76763497-e58f-4819-8640-da876e9da6b4"}, "8c62a42e-f52c-4cb3-a01e-eeb47d1f5020": {"doc_hash": "c78881e4551ccda4093d2e8d9a7adc3b1e6057443e609617601b246e9a8878b1", "ref_doc_id": "c35d9e44-90b5-4766-9d50-a2c38a050015"}, "3cc6d81e-15f0-43fd-9812-1d43d6a44c37": {"doc_hash": "05f8226111d608911a77831dd75810f7b72e112eb4827ba1f41ec7bf00124cc3", "ref_doc_id": "09eb4de1-b223-451d-83fe-6555c0de6742"}, "0da4f774-4ba2-4797-9d77-330d5f98eb7a": {"doc_hash": "b3c90c8c5b75bc9064c6f38aa54ab369928558fbc98cf806eff34713bc4a5f1e", "ref_doc_id": "163a2cc0-c33d-4e34-8c74-2d69e3247bea"}, "7fb51725-faa5-4e3b-9721-d41b25a6c315": {"doc_hash": "286c18c0a9056c5583597805ab609c5c2b55aaa3ca9fbd67e11c1c674d7078b7", "ref_doc_id": "b7f19304-dd58-4e75-a745-96f9105e9fe2"}, "f29aea69-bf90-4b99-af81-1b8606123fd2": {"doc_hash": "cad001887ca581fd5a0b342377f3572941f2a419bceb924af91ecc2b82701b03", "ref_doc_id": "7b5c9401-281a-40b6-b72a-77d5206685ba"}, "97603c75-aa6f-451b-a7e8-5e3305477932": {"doc_hash": "a7cce8159b767177bf760cb74d968053f85285867b5301a855e3ab85843cea50", "ref_doc_id": "7cb08143-5e41-44d5-a8da-1e626eb271f1"}, "f47e422a-e24b-422e-9de2-53c16dbfba2b": {"doc_hash": "9b020049c73ee3f131dda644625584fd941c3024c8c8330482d5e4f8892d6c9f", "ref_doc_id": "fed2e603-ed06-4762-89b6-3deb2c5935b5"}, "fee303f5-616e-4bb2-85b4-6920906f46ad": {"doc_hash": "5019ddec9dedfabf7dcb99a79b4a4eacabec145078764940400aa1ff69f5742a", "ref_doc_id": "6d7912ef-c68f-4bce-86c2-f04793fe1dc0"}, "1d478305-3651-4df2-be1d-121b38db2ab9": {"doc_hash": "cd66d76284bd99a9fcc3969fdc0b10e071528e49ac6392c5f7379c3a2004a8e5", "ref_doc_id": "78f8c795-ea9e-4eae-8e07-5025fefbcb90"}, "af7c6f2c-4fa2-43c7-a73e-525026cdb4df": {"doc_hash": "6b0e56a4c018dd2f64d8f3556ef204cced2a0e3bf5ab06e20030501b654c41c0", "ref_doc_id": "78f8c795-ea9e-4eae-8e07-5025fefbcb90"}, "a69ffa28-4676-4bc1-a148-f00adc53c1e0": {"doc_hash": "5da0f7cb07bdc02f4680ac676db205285d884537b069791bbe549a8e208fa297", "ref_doc_id": "78f8c795-ea9e-4eae-8e07-5025fefbcb90"}, "2422a6aa-a9b7-4066-a51a-ecdf1829d9c3": {"doc_hash": "77b7a8ad118b4dbbf9f112e3179312e08b52baa9dadd8bb04b14584cdbe5adce", "ref_doc_id": "75d5e96c-a8cf-4bfa-b131-4b850ed8ae43"}, "2a22afc1-351e-40a5-b820-d0d702c5cbb8": {"doc_hash": "aeced7ff31f7db2dcd05717b52cf35e1e38609b59eb55dba255b359ad079fc0d", "ref_doc_id": "c817712f-d768-4d9f-a7fa-5deb72ddac90"}, "c85882ad-9aa3-4a3c-a743-b95c65aa1f3d": {"doc_hash": "74c5cc5640b60c3507e9f50c8052357efe5bc420f6268994cd15ca466f4839f9", "ref_doc_id": "3eff2c07-e375-4bda-8031-c81ef3314f89"}, "9b922dca-78a7-49cc-81e5-6702aebc9b02": {"doc_hash": "a63d56229a733dd935789e547f878481154f02b96dc87b4782df03c17dd1700a", "ref_doc_id": "5a8eaabf-e0df-4ee8-85af-aead151d7ebc"}, "125feea5-8124-4058-8b1b-e4f43296f1d6": {"doc_hash": "b52bc7b64c8f6d2247e0029127448111f44cd2bc356af9c845926c61f33cea1c", "ref_doc_id": "64a779fd-74a5-4fe1-8536-04937d022f0f"}, "c33a5a18-dc83-402e-94a1-b106c02e6570": {"doc_hash": "f20be59950a026143d6354c460c0f4504ca12bad216963f3f4759562ab195dee", "ref_doc_id": "7732490d-fd9a-4f47-91ca-8c28687a3ff6"}, "f78f606e-2762-4d36-879a-d732473a858d": {"doc_hash": "0a3339a64204e83726c0e4ef4d7c8bada6d31e6346d6713d235e473d7802188b", "ref_doc_id": "3110282f-d8ee-4b21-8212-1e96891acd23"}, "86975a3e-7d1b-4457-bc94-9596cbfe77fc": {"doc_hash": "5d5074acc09a1c3fc82dbae067a4f7e9b3e3150045a135037c0ac5e4fdf36bdd", "ref_doc_id": "cfab389d-3054-4336-8e55-26d73f4cea03"}, "c5252813-0706-4e98-b55f-88c21063a4fd": {"doc_hash": "8fc2dd327180fa0b03e85a7a55f5cadfb7c256001b02b8ca30477d8f73cfd9ad", "ref_doc_id": "b92cffaa-499b-418b-8857-39ccd5a2202c"}, "3c999d6c-441a-4664-9efa-2c6fa51890df": {"doc_hash": "e615edfc39a4dea79292fe5b7aac405e96a949c38877dfa973f69986d49419a9", "ref_doc_id": "c8a10ff8-5321-47d6-90dc-8e262e259895"}, "33f79220-28b9-42a4-93a1-66fe1ea9c8e7": {"doc_hash": "31bed298a27860cdbacbfad65a8aa1895e54390800f6a446ce51755bd3a56646", "ref_doc_id": "a6bf2401-22f1-4ba2-b289-c183ea1a9203"}, "e834e703-b659-4e4b-8b86-a725252722d1": {"doc_hash": "898553751ac34c86b9b6cc8dfeb618339f6b012331c99559e3aa1ee3f318430c", "ref_doc_id": "88a94c02-45ac-4f28-9aca-67851e7c1bc4"}, "ce35d7d2-ce7f-4e50-8ac6-a981c68715c5": {"doc_hash": "2781f13a6e9e5aaaf364daf3a731c76844071f68a2c1bfc0770a25b67d000969", "ref_doc_id": "90056080-e722-4354-afc0-ccce76552a66"}, "271baa91-1e12-4763-bc0a-c94bf11c8df7": {"doc_hash": "36a689eef3e64620e342513bfbd142dec3304e9799031babd666b7b7122c4844", "ref_doc_id": "10cddbf2-38d8-4e46-b8cf-94fa7561d4e1"}, "8c79111e-5991-4052-b7ef-924f5419bbf0": {"doc_hash": "8d91c2c210263df810447aeab8bad68e248e50b11255e6f1d4d7c2491ce7b555", "ref_doc_id": "82d51892-61ce-437b-a2dd-067ec10b6155"}, "a6b4d19e-b174-447f-85dc-8a8662fbd397": {"doc_hash": "1708815dbdee4db39fb1362dd7aa5155fcbc791593c3ec166e45ecebe4ff154b", "ref_doc_id": "3a6a55f1-6d9c-4dee-b23e-ddbf9db019ca"}, "bb7c2f4d-2930-4f36-9791-7bd99cf21736": {"doc_hash": "d0b4c30d603ac3485f5c9a976d0a21d8b907fbd85322d947dab0825f0b590ad8", "ref_doc_id": "00af1a4e-94cf-40de-830d-d93570989fa4"}, "8075e816-806e-44aa-84a6-53e17241c595": {"doc_hash": "3d0511323fc585984836ed5c438eac31896e586ad940aa50edcf0913eacbc9da", "ref_doc_id": "273d027a-3b3e-41c9-b6d6-ea8efc3a679c"}, "3e94555c-c2f3-4b25-80e5-1270ea419acd": {"doc_hash": "7c47fd32dbb7a9f15e27605f827677a84aab36684aba91c6177c0f8f38785644", "ref_doc_id": "bc258d10-f3f1-4693-b227-973ff974f16d"}, "2a00af8b-eee8-4443-90ad-2d51f4a83d57": {"doc_hash": "d31eb14faa5745c540c706c44a7af24da3570095cf9105cd1e85f5f10bb6c4f0", "ref_doc_id": "3239be1b-1c0e-4a4a-8361-663be0ed3c73"}, "2804681c-94ad-47c3-96b5-1b687eb390ae": {"doc_hash": "cc15a12dcbc01a2353f47324e4b779b19ae7ec1f0498cd7cb101b1425d524e43", "ref_doc_id": "546426ca-21a0-47b4-9661-e8f2495d2769"}, "3fd8d337-8f8a-45d5-ab78-6c640c9c9a2d": {"doc_hash": "d384f5732dd85ac1188d2a0c4a2c72e13e32ba9d4bad9a5d3182d6baf191cf8e", "ref_doc_id": "b7741213-9ceb-4385-bf59-62411fa9d8d2"}, "be2f5ad6-4c2a-4b91-8c2d-bab368226a1f": {"doc_hash": "e2a9e659b23acb08f94693b2fabb04dfa0217c44e295a32a0893693bd9865775", "ref_doc_id": "54bd5851-7151-434b-8f49-d024ecfe1222"}, "4095955f-215a-4210-9fb6-46e1342a3657": {"doc_hash": "549667d58d3ba6d9772028a2a6c1fcf1ad855d1f632853d47e207b6f38fe0ebe", "ref_doc_id": "c5c336bd-8240-44c0-b522-4a6a923c49a4"}, "9bb76320-5208-4ddd-8b1d-f7ddeb589402": {"doc_hash": "9e079ed791accbaa6cc78e31541f65dd2e64380171cd36f5643d19bf15192c94", "ref_doc_id": "830e2c34-86b8-4a53-b3ac-e69854db7132"}, "ede56a6c-8e2a-4ca0-aa91-ae59024343f0": {"doc_hash": "dacfcf1daaa3c60dfd43f93dda66dc612a47e1cf9133448124745a7ca3b1d981", "ref_doc_id": "953faea1-1bb9-494c-bcd9-642c51d78fc0"}, "43f8215f-456c-4ff8-a8a5-4f71ca3fd577": {"doc_hash": "90173e8f6a5daa2abd9c366482b1d0769bb4e8d981d6350a7fa71308a5f90618", "ref_doc_id": "719e8fec-2706-4e2a-9bf0-3d0e3d0c224a"}, "62f4c0bf-3779-4b68-b049-69016c1a821d": {"doc_hash": "c61990d8500ccf8ebe140ce9989386a282afb315e946ae3d129e71dda411fe0f", "ref_doc_id": "7c0692e1-a293-42ab-a093-dcd19dc6e376"}, "9873c114-bd9e-4f9d-b8bb-23695668a938": {"doc_hash": "cb8ee395359b7ca8025c369f17676516fbfe5a68ebdd143c30086db8775a015e"}, "962563b4-e4d1-4481-9129-d8ca7ecea48f": {"doc_hash": "3c1566e23eb287eed39131f83dda4b673e537fe9a8b99512aa7b7f6dc306a3ec"}, "5c8b2e19-1275-4764-b65d-81fe58c59e72": {"doc_hash": "81ae59649886421f5b4c04ecd93ebeefb6fa69e8544dd29346c9fea888ed819c"}, "77986709-cd77-49b6-93cd-ba1fdb5283c1": {"doc_hash": "8f54ad4c75da722bf188e0d1b8a198f0d22bc6bd1f055c39895376ad70163456"}, "ffeab90b-eea4-4615-8495-63a508faca0e": {"doc_hash": "3f804db530c7451d368a567c32ac0e99340fd99a15f71d935871ec12562a09d8"}, "bdb3373c-2c0b-4521-85fe-3c34889fd46e": {"doc_hash": "9bd0f626b518a55fcbfc194e0bf800be7def4b1b3f41fc125507e10876a0c83d"}, "3ad541c3-084f-44dd-8f5a-55480a8140ad": {"doc_hash": "5ed76bb438e7a9babe7634ca47cfebe5d05a85e600e2ce936c64a7eb048e1fbe"}, "f5370f65-bb14-4729-bb9a-54b7e8462aaf": {"doc_hash": "af443a015dfe8bddf8258b60421995ea917c0147f589a11db137bb52cb316e92"}, "ed9f5596-1e66-40db-86ef-b3e9fe17ce29": {"doc_hash": "f11e15a2c4adf7d9be8b350b1cabe49ead2a027b155b62197ae6fc47461f15fa"}, "427bf846-34e9-4338-98a5-513a6d279c05": {"doc_hash": "2b0a9e95285b9caad06379fcad0b786a228b581fc5f9e7e086ff2aace3ee5fd6"}, "6fdf46bc-37d4-49a5-82c0-8496b204a11e": {"doc_hash": "cca910e7fd88b147aecb643d8b01a1c625dd396e091dac3200f08a77c82cc0e5"}, "685defd6-96f1-4897-9546-f81063fff1f2": {"doc_hash": "1354fc7ad0cb7acec093ffa8d02eaf3ba349a2d9caa48b686a09cda40be8d4a4"}, "dcbb48d4-d09d-4dfd-a8f5-8923929cb554": {"doc_hash": "e71bc8a60582a6f208185922c301307e4e1a5e0a99778277c21c81879a69795f"}, "409baa8c-2dea-4762-a444-814214cacee9": {"doc_hash": "934b1187d48dd974ec0d4db0048ba387a1c1e210505758270242ab98d9d89e95"}, "f9836401-2894-468e-8124-9283ba78fc69": {"doc_hash": "475edc4baeb0b3c416a814105687e21185fa0c3459714881022775bf3f6b7cdb"}, "f8badfca-c366-4a3c-9068-eea224fb0510": {"doc_hash": "a9184ca7017858397b6172de564e682bab7ae52667a7b5c7f44c2244cdb2986b"}, "f18a2285-c3bf-452d-93fe-8f56cc7b2c9e": {"doc_hash": "5931ba7ab77b128f8b35702993ce61c5ec2d9208034efad257bcb7086c3477c7"}}, "docstore/data": {"e0cc3c41-cab9-44a8-85fa-1f25a674b638": {"__data__": {"id_": "e0cc3c41-cab9-44a8-85fa-1f25a674b638", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95ad9605-f0d4-4bc3-8aaf-9212c12b29e5", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "8cc73704464407255b43c3f1ef18f70684f061081f3bfcb78d55fb5197f5ee91", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "672501e6-4f5e-48ad-9678-43e71b643117", "node_type": "1", "metadata": {}, "hash": "e2934d9f877e3f783af3ba105454ffd49fd1854da2ab998ac4c7f4867387cd7d", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Use\n\n  * A trial Snowflake Account with `ACCOUNTADMIN` access\n  * A Matillion account, provisioned through snowflake's partner connect", "start_char_idx": 2, "end_char_idx": 148, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "672501e6-4f5e-48ad-9678-43e71b643117": {"__data__": {"id_": "672501e6-4f5e-48ad-9678-43e71b643117", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f2ca1935-cb5a-4945-82f5-f27e8b1a6f96", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "64e5db6923e31da2c07ab6dd9137c2de463cd2687e5926699693a2bf2429fa1f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0cc3c41-cab9-44a8-85fa-1f25a674b638", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "4e891d77ed6e75e77f791aa583236725851ad5b81d409b3486a9626ff863cdaf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "886c95dd-fc49-4176-b86f-a255f6d3d812", "node_type": "1", "metadata": {}, "hash": "8a59ec9e9fd067d85bfe9f1b8e7c0aeed13913912c8de574cc174c8697e4e205", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Learn\n\n  * How to source 3rd party data from Snowflake data marketplace\n  * How to use Matillion's GUI to build end-to-end transformation pipeline\n  * How to use Matillion to extract real time data from public APIs\n  * How to leverage Matillion scale up/down Snowflake's virtual warehouses", "start_char_idx": 2, "end_char_idx": 303, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "886c95dd-fc49-4176-b86f-a255f6d3d812": {"__data__": {"id_": "886c95dd-fc49-4176-b86f-a255f6d3d812", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f5c7665-f156-4b41-ad35-8df8f9ad013e", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "f1c20581c7ea6cb8d1ee33d6834e8e0b7520f2744f1e7d2db17f10411f1a7ceb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "672501e6-4f5e-48ad-9678-43e71b643117", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "29958ae481efcc84e56026861d1a73563246c06bbf7e23b141a39ca585025f8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7361ef57-4b45-4d2b-83bf-827b953a57a7", "node_type": "1", "metadata": {}, "hash": "bde6743597b139e61d6d25f173f818b5bf89e1c45b71da699a66928b09e5d13b", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Build\n\n  * An end to end data transformation pipeline for Financial Services data leveraging Matillion and Snowflake, leveraging different data sources - joining, transforming, orchestrating them all through user friendly, and easily managed GUI services\n\nYou are a stock portfolio manager of a team of 10 traders !!! Each of your\ntraders trade stocks in 10 separate industries. You have with you available 10\nyears of historical data of trades that your team performed, sitting in an S3\nbucket - you know what stocks they traded (BUY or SELL), and at what price.\n\nYou would like to aggregate their Profit & Loss, and even get a real time\naggregated view of total realized and unrealized gains/loss of each of your\ntraders. To accomplish this, we will follow the following steps:\n\n  1. Acquire stocks historical data, freely provided by Zepl, from snowflake data marketplace. This will create a new database in your snowflake account.\n  2. Launch a Matillion ETL instance through snowflake partner connect.\n  3. Use Matillion to :\n\n  * Ingest your traders' historical data sitting in a S3 bucket, into a Snowflake table.\n  * Develop a transformation pipeline to create each trader's PnL as of today, by joining with stock data from Zepl\n  * Leverage Yahoo Finance API to get real time stock data\n\nThe 10,000 foot view of what we will build today:\n\n!2_Lab_Overview\n\nSneak Peek of the orchestration job that will accomplish all this, nested with\n2 transformation jobs within it:\n\n!2_sneak_peek\n\nLogin to your snowflake account. For a detailed UI walkthrough, please refer\n[here](https://docs.snowflake.com/en/user-guide/ui-snowsight-gs.html#getting-\nstarted-with-snowsight).\n\nAs the `ACCOUNTADMIN` role, navigate to Marketplace, and search for \"zepl\".\nClick on the tile.\n\n!3_zepl\n\nNext:\n\n  1. Click on \"Get Data\" on the right.\n  2. A pop-up screen opens: prefix the database name with \"ZEPL_\" so the name becomes `ZEPL_US_STOCKS_DAILY`\n  3. Click on \"Get Data\" in the center.\n\n!3_get_zepl\n\nSo what is happening here? Zepl has granted access to this data from their\nSnowflake account to yours. You're creating a new database in your account for\nthis data to live - but the best part is that no data is going to move between\naccounts! When you query, you'll really be querying the data that lives in the\nZepl account. If they change the data, you'll automatically see those changes.\nNo need to define schemas, move data, or create a data pipeline either!\n\nClick on Query Data to access the newly created database.\n\n!3_query_datal\n\nA new worksheet tab opens up, pre-populated with sample queries. The newly\ncreated database has 3 tables. Feel free to click on them and browse what\ntheir schema looks like, and preview the data they have.\n\n!3_zepl_worksheet\n\nCongrats ! You now have decades worth of stock data acquired in minutes !\n\nOne more thing: we need to locate and note down our snowflake account\ninformation for subsequent steps. To locate snowflake account information,\nnavigate to **Admin \u2192 Accounts** , and click on the link icon next to the\nAccount name to copy the account name URL to your clipboard (The **text that\nprefixes .snowflakecomputing.com** is the account information needed to\nconnect Matillion to Snowflake). Paste it in your worksheet, we will need it\nin section 5.\n\nIn the screenshot below, the account text we are look for is: `bjjihzu-\nji91805`\n\n!3_account_id\n\n  1. Navigate to Admin \u2013> Partner Connect, then click on the **\"Matillion ETL\"** tile\n\n!4_pc_metl\n\n  2. All fields are pre-populated, **give additional \u2018Optional Grant' to** **`ZEPL_US_STOCKS_DAILY`** **database** (created in previous section), then **click Connect**\n\n!4_og\n\n!4_metl_connect\n\n  3. Once the partner account has been created, **Click Activate**\n\n!4_activate\n\nYou will be redirected to the Matillion ETL web console. Your username and\npassword will be auto-generated and sent to the same email you provided to\nlaunch your Snowflake trial account.", "start_char_idx": 2, "end_char_idx": 3959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7361ef57-4b45-4d2b-83bf-827b953a57a7": {"__data__": {"id_": "7361ef57-4b45-4d2b-83bf-827b953a57a7", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f5c7665-f156-4b41-ad35-8df8f9ad013e", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "f1c20581c7ea6cb8d1ee33d6834e8e0b7520f2744f1e7d2db17f10411f1a7ceb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "886c95dd-fc49-4176-b86f-a255f6d3d812", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "d7bae9d4dd348c17108bef571851f6de4b608dc46d50d5acd4c9c6c934905b88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7dae2f14-5878-4730-96ff-e152c953c045", "node_type": "1", "metadata": {}, "hash": "56da0adb83938f5d7542bcc74ed515d094af3ce1e88dd325f70522eee4fd2e45", "class_name": "RelatedNodeInfo"}}, "text": "Paste it in your worksheet, we will need it\nin section 5.\n\nIn the screenshot below, the account text we are look for is: `bjjihzu-\nji91805`\n\n!3_account_id\n\n  1. Navigate to Admin \u2013> Partner Connect, then click on the **\"Matillion ETL\"** tile\n\n!4_pc_metl\n\n  2. All fields are pre-populated, **give additional \u2018Optional Grant' to** **`ZEPL_US_STOCKS_DAILY`** **database** (created in previous section), then **click Connect**\n\n!4_og\n\n!4_metl_connect\n\n  3. Once the partner account has been created, **Click Activate**\n\n!4_activate\n\nYou will be redirected to the Matillion ETL web console. Your username and\npassword will be auto-generated and sent to the same email you provided to\nlaunch your Snowflake trial account.\n\n  1. Once logged in to Matillion, you will be prompted to join a project. Click **Create Project** to get started.\n\n!5_metl_cp\n\n  2. Within the **Project Group** dropdown select **\"Partner Connect\"** , add a new name for the project (for the purpose of this lab we will name it **\"TraderPnL\"** ). You can leave Project Description blank, and the check-box's with the default settings. Click **Next**\n\n!5_project_name\n\n  3. In the **AWS Connection** set the **\"Environment Name\"** (for the purpose of this lab we will name it **\"Lab\"** ). Click Next\n\n!5_lab\n\n  4. Enter your Snowflake Connection details here. The Account field is the same text you saved from Snowflake UI in section 3. Also enter your Snowflake account \"Username\" and \"Password\". Click Next.\n\n!5_metl_sf_connect\n\n  5. Now we will set the Snowflake Defaults. Select the following default values:  \nDefault Role: `ACCOUNTADMIN`  \nDefault Warehouse: `PC_MATILLION_WH`  \nDefault Database: `PC_MATILLION_DB`  \nDefault Schema: `PUBLIC`\n\nClick **Test** , to test and verify the connection. Once you receive\n**success** response, you are properly connected to Snowflake. Click\n**Finish** , and now the real fun begins!\n\n!5_sf_defaults\n\nWe will now create our first orchestration job. The job will consist of first\nloading the trading history from AWS S3 to a single Snowflake table. To\nefficiently work with the data, we will modify the warehouse to the\nappropriate size using the Alter Warehouse component. We will then create two\nseparate transformation jobs to perform complex calculations and joins and\ncreate new tables back in Snowflake. Finally, we will scale down our warehouse\nwhen job completes. By the end of it, the orchestration job should look like\nthis:\n\n!2_sneak_peek\n\nLets get started!!\n\nWithin the Project Explorer on the left hand side, right-click and select\n**Add Orchestration Job**.\n\n!6_add_orch\n\nName your job **\"VHOL_orchestration\"** and click \"OK\". You will be prompted to\nswitch to the new job, click **\"Yes\"**. You should now see a blank workspace\n(new tab)\n\n!6_orch_name\n\nThe following steps will walk through adding different components to the\nworkspace to build our data pipeline. The first step is to load trading data\nfrom S3 using the **S3 Load Generator** component.", "start_char_idx": 3243, "end_char_idx": 6221, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7dae2f14-5878-4730-96ff-e152c953c045": {"__data__": {"id_": "7dae2f14-5878-4730-96ff-e152c953c045", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0d7d8577-801f-4242-b4fe-8efb3010aebd", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "3336e22a7833a442166a36a1e3a1c8371ca887a3253dc78aec4dd69c9625b9cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7361ef57-4b45-4d2b-83bf-827b953a57a7", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "08844375b2dab648a82eb40c62b2de6a3fd414971b0f2c2e2be32665b6c8b0df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cdc3816a-8895-46ed-978f-6ab3eab87cbe", "node_type": "1", "metadata": {}, "hash": "5c31ecc9c4e759bc44fdc51d490269b2318eaec1be4dbe71ef5cbf2b48b65779", "class_name": "RelatedNodeInfo"}}, "text": "S3 Load Generator\n\n  1. From the Components section on the left hand side, expand the Wizards folder. Find the S3 Load Generator component.\n\n!6_s3_lg\n\n  2. Drag and drop the S3 Load Generator component onto the workspace as the first step after the Start component.\n  3. A S3 Load Generator menu will automatically pop up. Click the ... button to explore S3 bucket\n  4. Copy and paste the S3 bucket into the wizard: `s3://mtln-techworkshops/VHOL_Trades/`\n\nClick **Go** to explore the contents of the bucket, you should see several CSV\nfiles - these are trade history data of 10 traders, trading in 10 different\nindustries. Highlight the file name `ARYA_SWINFRA.csv` and click **Ok**.\n\n!6_s3_files\n\n  5. You can now sample the dataset by clicking **Get Sample** , it will return a 50 row sample of the dataset. Click **Next**.\n  6. Matillion will guess the schema on the dataset, you can make any modifications to the configuration. For the purpose of this lab, we will keep the configuration settings as **Default**. Click **Next**\n\n!6_file_schema\n\n  7. Click **Create & Run**, this will render two components on the VHOL_orchestration canvas (Create Table and S3 Load).\n\n!6_create_run\n\n_Note if you click test you may receive a permission error on the S3 bucket.\nYou can ignore this for the lab, and move on to the next step. Don't worry\nabout any errors at this point, we will resolve them in the upcoming steps._\n\n  8. Link the **Start** component to the **Create Table** component.\n\n!6_start_ct\n\n  9. Click on the **Create Table** component, in the Properties Tab you will see several parameters. Note the **Create/Replace** parameter by default is set to Create. Click the ... button and change it to `Replace` from the dropdown menu.\n\n!6_replace\n\n  10. Now we will modify the size of each column. In the properties tab, click on the ... button for the **Columns** parameter. Update the Size for each Column name as shown in figure below, then click Ok.\n\n!6_column_size\n\n  11. Change the component name and table name to `TRADES_HISTORY`, by clicking on the ... button in the Properties tab\n\n!6_comp_name_TH\n\n  12. Right click on the TRADES_HISTORY component and select **Run Component**. This will create a new table in your Snowflake account !\n\n!6_run_comp\n\n  13. Next, Select the S3 Load component, and change the Name in the Properties tab to **LOAD TRADES_HISTORY**.\n  14. Change the **S3 Object Prefix** by clicking on the ... button to select the VHOL_Trades directory, and then click OK.\n\n!6_s3_prefix\n\n  15. Change the **Pattern** parameter bu clicking on the ... button, and change to `.*`. Click OK.\n  16. Change the **Target Table** parameter by clicking on ... button, and select TRADES_HISTORY from the drop down, click OK.\n  17. The LOAD TRADE_HISTORY component Properties should now reflect as shown below, all other fields should be left as default.\n\n!6_load_history\n\n  18. Right click on the LOAD TRADE_HISTORY component and run it by clicking **Run Component**.\n  19. Check back in your Snowflake console to confirm the TRADES_HISTORY table was created, and data loaded - 1.7 million rows, and compressed to < 10 MB !\n\n!6_sf_th", "start_char_idx": 2, "end_char_idx": 3153, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cdc3816a-8895-46ed-978f-6ab3eab87cbe": {"__data__": {"id_": "cdc3816a-8895-46ed-978f-6ab3eab87cbe", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c5fa9ef1-ac5c-41b9-8ab3-a33983c68cbe", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "06edd4c7fbdbe2c10b66789092e27ca0e6ae0af4aa36a04353f33566c27b5aca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7dae2f14-5878-4730-96ff-e152c953c045", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "64ab28a665acb067d5a2a8c0dbc8848e72e731f029d1b7f9c760bde3659ad6d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c52d986-5eae-4ee4-ac40-61dda63ec8c0", "node_type": "1", "metadata": {}, "hash": "4413b9bea9ffb3a9155366d5966b960afd0d1f3bbff0b227af648b0b764d633c", "class_name": "RelatedNodeInfo"}}, "text": "Alter Warehouse\n\nThe next step of the orchestration is to scale up Snowflake's Virtual\nWarehouse to accommodate resource heavy transformation jobs.\n\n  1. Find the Alter Warehouse component from the Components pane.\n  2. Drag and drop the component as the last step, connected to the LOAD TRADES_HISTORY component. Click on the component to edit its Properties.\n  3. Rename of the component to `Size Up Warehouse to M`.\n  4. Change the **Command Type** to Set.\n  5. A new field will appear, edit Properties to add a new line with Property set to **WAREHOUSE_SIZE** and Value set to `MEDIUM`.\n\n!6_WH_M\n\n  6. Your orchestration job should now look like this:\n\n!6_end\n\nThe trading history data from S3 gives a listing of ten traders with both BUY\nand SELL actions. In this transformation job, the transactions will be\naggregated to find out the number of shares bought/sold and for how much. With\nthose figures, the net # of shares and value will be calculated, and a table\nwill be created, enriched with each traders' average price for each stock. The\nbelow figure shows the end product of the transformation pipeline we will\ncreate in this section:\n\n!7_1_job_view", "start_char_idx": 2, "end_char_idx": 1163, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c52d986-5eae-4ee4-ac40-61dda63ec8c0": {"__data__": {"id_": "6c52d986-5eae-4ee4-ac40-61dda63ec8c0", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7aba2b2b-a34a-4d73-b81e-7a815a067512", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "0208e718c53f87a75d398bd6226fb7c654a495165ffa120d9a793f78189b6d29", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cdc3816a-8895-46ed-978f-6ab3eab87cbe", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "95ca265e3f37b39b4e26fc775d6f9ecc111feac1dbc8077152b30db85cd1ef32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc040959-623c-4040-956b-782c6dd734a4", "node_type": "1", "metadata": {}, "hash": "59f7961b3e13c37d1e939eb2610d156d929558c0d68a4e555ea44f993e6c3de7", "class_name": "RelatedNodeInfo"}}, "text": "Let's get started!\n\n  1. Within the Project Explorer, right-click and select **Add Transformation Job**.\n\n!7_2_add_tran\n\n  2. Set the title to `VHOL_CURRENT_POSITION`, and click Ok.\n  3. Next prompt will ask you to switch to the new job, click NO.\n  4. From the explorer, drop the newly created job as the next step after the Alter Warehouse component within the previously created orchestration job (VHOL_orchestration) and complete the connection, as shown below:\n\n!7_3_orch_view\n\n  5. Double click the new transformation job VHOL_CURRENT_POSITION. A new tab gets opened with a blank canvas. We will now build a transformation pipeline.", "start_char_idx": 2, "end_char_idx": 640, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc040959-623c-4040-956b-782c6dd734a4": {"__data__": {"id_": "cc040959-623c-4040-956b-782c6dd734a4", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a56aa6b5-5dfa-43bc-84cf-4dbaadc5138a", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "e71c28c151df7bcb8216de8f242520ec35becd6032a59189c9134c27d0290278", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c52d986-5eae-4ee4-ac40-61dda63ec8c0", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "9e8ebf95ab80472ae2a1bc8443926227a8f1ac5ee0606987db6a923f7bcf8c87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95d837a6-2e21-467d-ad6a-b19dd90e5eaf", "node_type": "1", "metadata": {}, "hash": "79c29aa5202c60bba0b6c80a258bc918cebdc0a435ade270890cdaeaeac2ab19", "class_name": "RelatedNodeInfo"}}, "text": "Table Input \\- Read\nTRADES_HISTORY\n\nFind/search the **Table Input** component in the component palette under Data\n> Read folder and drop it on the blank canvas, then set it up with the\nappropriate properties below:\n\nName: `TRADES_HISTORY`  \nDatabase: [Environment Default]  \nSchema: [Environment Default]  \nTarget Table: `TRADES_HISTORY`  \nColumn Names: Select all columns by clicking the ... button\n\n!7_4_Table_Input", "start_char_idx": 2, "end_char_idx": 419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95d837a6-2e21-467d-ad6a-b19dd90e5eaf": {"__data__": {"id_": "95d837a6-2e21-467d-ad6a-b19dd90e5eaf", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "810734c5-8988-406b-84b8-58d212b43716", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "7b2bef9f5373b95c4edc17f0bc0ad61b036694b3b5bf7a60134ecbecf620b4ab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc040959-623c-4040-956b-782c6dd734a4", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "1c9cde3f5a1d985e047661a8432f1be334f48539227aea956cd63d926bfe7091", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e82cf00f-fd12-4db3-a391-bc9f361676f7", "node_type": "1", "metadata": {}, "hash": "e0a469f1d73aa554efcf448e589dfd598671fe00f0db7e2b50bfdf1ba5c49a89", "class_name": "RelatedNodeInfo"}}, "text": "Filter \\- Filter Buy\nactions\n\nNow, let's add a second step to filter the data based on the type action.\n\n  1. Find/Search the Filter component in the component list under Data > Transform folder and drop it on the canvas, connect it to the TRADES_HISTORY component.\n  2. Click on the component and update the **Name** property to `ACTION = BUY`\n  3. Then use the Filter Conditions property wizard, add a line with the following settings:\n\nInput Column: `ACTION` Qualifier: `Is` Comparator: `Equal to` Value: `BUY`\n\nYour Transformation Job should now look like this:\n\n!7_5_Filter", "start_char_idx": 2, "end_char_idx": 580, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e82cf00f-fd12-4db3-a391-bc9f361676f7": {"__data__": {"id_": "e82cf00f-fd12-4db3-a391-bc9f361676f7", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7868653-afc3-467c-b796-4a6382eb5407", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "46ace7ad33faada49ce6b047492eca9297d9a1c0e89e286a8a9e0f5fd2217afc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95d837a6-2e21-467d-ad6a-b19dd90e5eaf", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "a663d4a67318c9251a0fdf43ae8665acc5077956b12dbc373fc62c5dc53f801e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8367fbe-ea13-4c0a-a26f-ed5d9093c1aa", "node_type": "1", "metadata": {}, "hash": "acac010c548b887ca7810a8586465d504f00df7bae62865f168de260c4cd4fba", "class_name": "RelatedNodeInfo"}}, "text": "Calculator\n\nNow we will add a calculator to calculate the amount of investment in each buy\ntransaction:\n\n  1. Find/Search the CALCULATOR component under **Data > Transform** folder and link it to the ACTION = BUY component created in the previous step.\n  2. Click on the component and name it `TOTAL_PAID`\n  3. Edit the Calculations property and use the expression builder to create the calculation:\n\n  * Add a new field with \"+\" button, name it TOTAL_PAID\n  * Build the expression: `-(\"NUM_SHARES\" * \"PRICE\")`\n\n!7_6_Calc\n\nClicl OK. Your transformation job should now look like this:\n\n!7_7_calc_view", "start_char_idx": 2, "end_char_idx": 601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8367fbe-ea13-4c0a-a26f-ed5d9093c1aa": {"__data__": {"id_": "a8367fbe-ea13-4c0a-a26f-ed5d9093c1aa", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "803d1f18-0bb5-4e29-8f82-7dfb005f7b4b", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "de335db7e9619f03d26b8e0ad06acf02e82dc73e81e2d4488bdb0b2a1c6057f9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e82cf00f-fd12-4db3-a391-bc9f361676f7", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "a867a7f26a779c80b549895171faf6ff0a9742946a75ef03a45dbc1d6d8ed25e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14ff226c-ce77-47bf-adf8-2d4bc8714d91", "node_type": "1", "metadata": {}, "hash": "84fd2a955c143744ba14a33ea4f67d3585501bb210b2f2a1d7b69204c803fe89", "class_name": "RelatedNodeInfo"}}, "text": "Aggregate\n\nNext we will sum up the investments made in each stock by aggregating.\n\n  1. Let's add an Aggregate component from the palette under **Data > Transform** folder and link it to the previous Calculator component.\n  2. Click on the component to edit the Properties:  \nName: `BUY_AGG`  \nGroupings: `TRADER, SYMBOL`\n\n  3. Open the Aggregations field wizard and add 2 lines then configure them like this:  \n **Source Column: Aggregation Type  \n** TOTAL_PAID: `Sum`  \nNUM_SHARES: `Sum`\n\n!7_8_agg\n\nClicl OK. Your transformation job should now look like this:\n\n!7_9_agg_view", "start_char_idx": 2, "end_char_idx": 578, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14ff226c-ce77-47bf-adf8-2d4bc8714d91": {"__data__": {"id_": "14ff226c-ce77-47bf-adf8-2d4bc8714d91", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3fac0a30-ffb1-4e20-aab2-1181fc1ac6e2", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "f538bf3f92f0ab32f6eaf88cf9e65301a9ff174df5406c880bd8fef0200f2639", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8367fbe-ea13-4c0a-a26f-ed5d9093c1aa", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "ca3d33334949a609d6d441a675de2f4b1c868f735f465d2389dfdb3def844f28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "483dce61-d77f-4626-b5d0-65cad7dd3eac", "node_type": "1", "metadata": {}, "hash": "7410d1fa3153b06788330bf9be80b391637580b80be7aae6bf2b916616ee0c38", "class_name": "RelatedNodeInfo"}}, "text": "We will now copy & paste the Filter, Calculator, and Aggregate components\nto create a similar pipeline, but for SELL filter.\n\n  1. Right-click on each of the components and select copy.\n  2. Paste the component by right clicking on a blank area in the canvas, and selecting paste. Connect the new components as shown below. Your Transformation Job show now looks like this:\n\n!7_10_cp\n\n  3. Update the properties of the new components with the information below:  \n3.1 Filter:\n\n  * Name: `ACTION = SELL`\n  * Filter Conditions: \n    * Input Column: `ACTION`\n    * Qualifier: `Is`\n    * Comparator: `Equal to`\n    * Value: `BUY`\n\n!7_11_filter\n\n3.2 Calculator:\n\n  * Name: `TOTAL_GAIN`\n  * Add a new field with \"+\" button, name it TOTAL_GAIN\n  * Build the expression: `(\"NUM_SHARES\" * \"PRICE\")`\n\n!7_12_calc\n\n3.3 Aggregate:\n\n  * Name: `SELL_AGG`\n  * Groupings: `TRADER, SYMBOL`\n  * Aggregations: `TOTAL_GAIN, Sum, NUM_SHARES, Sum`\n\n!7_13_agg\n\nWe are now going to join the 2 flows together.", "start_char_idx": 2, "end_char_idx": 985, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "483dce61-d77f-4626-b5d0-65cad7dd3eac": {"__data__": {"id_": "483dce61-d77f-4626-b5d0-65cad7dd3eac", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7baec97b-de2b-443a-a08e-c9a78636ff2c", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "7b43c5893a0da6127a2bed3f7f499d1ce63d828497ee80046e4e67bef9698730", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14ff226c-ce77-47bf-adf8-2d4bc8714d91", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "ad9ce0bbda79ea8b80c70a2c86a7ac172f9ee795a97bc5040c3795fa6fb90d6b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c781c20-22e9-4f6c-a8bb-54c760b70d00", "node_type": "1", "metadata": {}, "hash": "c630bfb2ea84aac5251f53925cc1898dfc6305b106b0aa0ecc22c4688dd31a49", "class_name": "RelatedNodeInfo"}}, "text": "Join \\- Join the BUY\nand SELL aggregations into a single dataset\n\n  1. Find/Search the Join component under Data > Join folder and drag and drop it as the last step of the job. Connect the Join component to both the BUY_AGG and SELL_AGG.\n  2. Click on the Join component to edit the Properties:\n\n  * Name: `Join BUY and SELL Transactions`\n  * Main Table: `BUY_AGG`\n  * Main Table Alias: `buy`\n  * Joins: `SELL_AGG, sell, Inner`\n  * Join Expressions \u2013> buy_Inner_sell: `\"buy\".\"TRADER\" = \"sell\".\"TRADER\" and \"buy\".\"SYMBOL\" = \"sell\".\"SYMBOL\"`\n  * Output Columns: \n    * buy.TRADER: `TRADER`\n    * buy.SYMBOL: `SYMBOL`\n    * buy.sum_TOTAL_PAID: `sum_INVESTMENT`\n    * buy.sum_NUM_SHARES: `sum_SHARESBOUGHT`\n    * sell.sum_TOTAL_GAIN: `sum_RETURN`\n    * sell.sum_NUM_SHARES: `sum_SHARESSOLD`\n\n!7_14_join\n\nYour Transformation Job should now look like this.\n\n!7_15_join_view", "start_char_idx": 2, "end_char_idx": 869, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c781c20-22e9-4f6c-a8bb-54c760b70d00": {"__data__": {"id_": "6c781c20-22e9-4f6c-a8bb-54c760b70d00", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "20054bdd-e5cd-4958-a755-662c0a52a0ae", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "d02066886c9430fe30f717e4d31a14955e3676304840e92f7e4b145925dbbc16", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "483dce61-d77f-4626-b5d0-65cad7dd3eac", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "e50f57f6fd3b20c375324816a847f68847636a01fb58fddb54e815cbcb74afa6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d610c7ff-9365-497d-ad24-d8fa79697341", "node_type": "1", "metadata": {}, "hash": "063467bb6ce8ba9c3b0d925fa57a3439f6eac7fc26b388e9dd996cbd91093e36", "class_name": "RelatedNodeInfo"}}, "text": "Calculate the amount of investment in each buy transaction\n\nAdd a new Calculator component to the canvas and set up with the below values\n( _use the same steps than in previous Calculator components to set up the\nexpressions_ ).\n\n  * Name: `NET_SHARES NET_VALUE`\n  * Include Input Columns: Yes\n  * Expressions: \n    * NET_SHARES: `\"sum_SHARESBOUGHT\" - \"sum_SHARESSOLD\"`\n    * NET_VALUE: `\"sum_INVESTMENT\" + \"sum_RETURN\"`\n\n!7_16_calc", "start_char_idx": 2, "end_char_idx": 434, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d610c7ff-9365-497d-ad24-d8fa79697341": {"__data__": {"id_": "d610c7ff-9365-497d-ad24-d8fa79697341", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7557ca6f-4040-4bab-b0ff-59655ed13169", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "4b59538dee8cc1b7a1014f39fa500713d0891e0316042196081b5ae735fef845", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c781c20-22e9-4f6c-a8bb-54c760b70d00", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "99daf4cf0b1d008f2377f7289f75460aefdab4ccd5665de2150aec5c4e83dc1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72df2313-fdd4-44c8-b7d7-0631a662bd5d", "node_type": "1", "metadata": {}, "hash": "474417a5d8b1eb115658a610cdb28767714f2f1e45fc12923448581912d17d40", "class_name": "RelatedNodeInfo"}}, "text": "Calculate the average price of stocks traded\n\nAdd another Calculator component to the job and configure it as follows.\n\n  * Name: `AVG_PRICE`\n  * Include Input Columns: Yes\n  * Expressions: \n    * AVG_PRICE: `-(\"NET_VALUE\" / \"NET_SHARES\")`\n\n!7_17_avg", "start_char_idx": 2, "end_char_idx": 252, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72df2313-fdd4-44c8-b7d7-0631a662bd5d": {"__data__": {"id_": "72df2313-fdd4-44c8-b7d7-0631a662bd5d", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62b3ca07-dcd7-4326-9f1a-f09a1b21b0af", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "be294339844f66ec00047ab124bf2d499103933c6bf32191c6e1f6a89f54e364", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d610c7ff-9365-497d-ad24-d8fa79697341", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "e9a7e1b8e820a72d608167a84b76efb22b69eebfae573104932c06b28ebc46a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccad89f5-e782-4a9a-b64c-4425f024c759", "node_type": "1", "metadata": {}, "hash": "3374dea9c22c74a285c0fb24ce260012f3d31129d7771502f413ce7505fbe351", "class_name": "RelatedNodeInfo"}}, "text": "Rewrite Table\n\nAdd a last component to the job to write the result of the transformation to\nthe CURRENT_POSITION table.\n\nFind/Search the Rewrite Table component and drag and drop it as the last\ncomponent in the flow. Connect to the AVG_PRICE calculator, and edit the\nproperties as below:\n\n  * Target Table: `CURRENT_POSITION`\n  * Warehouse: [Environment Default]\n  * Database: [Environment Default]\n  * Schema: [Environment Default]\n  * Target Table: `CURRENT_POSITION`\n\n!7_18_table\n\nThe job flow should look like this now:\n\n!7_19_flow", "start_char_idx": 2, "end_char_idx": 537, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ccad89f5-e782-4a9a-b64c-4425f024c759": {"__data__": {"id_": "ccad89f5-e782-4a9a-b64c-4425f024c759", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bdd73f33-aba0-48df-a07a-903239b03a0c", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "18682111fc054fdb715ec61155a967e1e8f7168e1c65204d2efa967cc4d1d12f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72df2313-fdd4-44c8-b7d7-0631a662bd5d", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "9a9b29e46d5e4460255873da168833d9be157f4298af44cf98c299a7cd5c43b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d622c97-5302-4bbb-9bfd-182b51c66ef8", "node_type": "1", "metadata": {}, "hash": "ae86ba84ebbcf8a89a25cd5d46a8555aec1884b759877a3bb43f23dd00ee6677", "class_name": "RelatedNodeInfo"}}, "text": "Execute this job\n\nRight click anywhere on the job and select **Run Job**. To preview the result\nof the job:\n\n  * Click on the last component (Write to CURRENT_POSITION)\n  * Open the Sample tab\n  * Hit the **Data** button\n  * Data will be sampled and previewed in the pane below\n\n!7_20_sample\n\nYou can now go back and validate the CURRENT_POSITION table is generated in\nSnowflake:\n\n!7_21_sf\n\nCongratulations, you're done with building and running the first\ntransformation job!\n\nThe previous Transformation job provided a snapshot of every trader, based on\nthe BUY and SELL transactions which took place. This job will take it a step\nfurther by calculating the profit or loss each trader is experiencing by\nstock, as well as the cumulative profit or loss, based on their entire\nportfolio. The below figure shows the end product of the transformation\npipeline we will create in this section.\n\n!8_flow\n\nLet's get started!\n\n  1. Within the Projects Explorer, right click and select **Add Transformation Job** , title it **VHOL_PNL_xform** and drop it as the last step step after the **VHOL_CURRENT_POSITION** transformation component in the VHOL_orchestration job.\n  2. Double click on the newly created **VHOL_PNL_xform** component to wwitch back to the new workspace to start building the job.", "start_char_idx": 2, "end_char_idx": 1292, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d622c97-5302-4bbb-9bfd-182b51c66ef8": {"__data__": {"id_": "3d622c97-5302-4bbb-9bfd-182b51c66ef8", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cf3bce4b-776f-46a0-bc31-9e9eb747a0c0", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "615caea614cf75f4095baf5d66a179045025cbca333edc08d908650edcd2b97f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccad89f5-e782-4a9a-b64c-4425f024c759", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "f276f8215948ef59cb2b4d0830c1fba26dc3d15746f4a749a28f07a215865060", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14450234-6df5-461c-a1d4-aabd69f184c2", "node_type": "1", "metadata": {}, "hash": "11f3e004447d2229c42d1def27e5079b218c169cff57380ff2b623a2732fa61c", "class_name": "RelatedNodeInfo"}}, "text": "Table Input \\- Read\n**STOCK_HISTORY**\n\n  1. Find the Table Input component and drop it into the canvas. Click on the component to configure as per the table below.\n\nNote that we are switching database to point to **ZEPL_US_STOCKS_DAILY** to\nget the **STOCK_HISTORY** table.\n\nName: `STOCK_HISTORY`  \nDatabase: `ZEPL_US_STOCKS_DAILY`  \nTarget Table: `STOCK_HISTORY`  \nColumn Names: `Select all columns`\n\n!8_stock_history", "start_char_idx": 2, "end_char_idx": 420, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14450234-6df5-461c-a1d4-aabd69f184c2": {"__data__": {"id_": "14450234-6df5-461c-a1d4-aabd69f184c2", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "052f529a-4826-4498-af43-c34c1f7f51df", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "bcd8d6c5726445f5af19910057231c446b4c5584b47e0d17ce526bd2c912e6cf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d622c97-5302-4bbb-9bfd-182b51c66ef8", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "9b93819123cac1de47cafa5ed290dbaaf8be03beea00ee46c31a3484fc66f92e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72062af9-3fa2-4e19-9042-58b69d05c586", "node_type": "1", "metadata": {}, "hash": "f18d9e6ffba4060db098c3a92b19dae7e618a9038576034545a11e3e81cdbbd0", "class_name": "RelatedNodeInfo"}}, "text": "Filter \\- Only include\nthe most recent close date for the stock\n\nWe will filter to only include the most recent clost date for the stock.\n\n  1. Right-click on the canvas and select Manage Job Variables.\n\n!8_filter\n\n  2. Fill out the Manage Job Variables as follows, using the !8_add to add a new variable as follows:\n\nName: `yest_date`  \nType: `DateTime`  \nBehavior: `Shared`  \nVisibility: `Public`  \nValue: `1900-01-01`\n\n!8_stock_jv\n\n  3. Click **Ok**\n  4. Drag and drop a filter component as the next step after the STOCK_HISTORY table input. Fill out the filter properties as follows:\n\nName: `FILTER ON YEST_DATE`\n\n  5. Update the Filter Conditions and Combine Conditions as follows:\n\n**FILTER CONDITIONS:**\n\nInput Column: `DATE`  \nQualifier: `Is`  \nComparartor: `Equal to`  \nValue: `${yest_date.now().add(\"days\", -1).format(\"yyyy-MM-dd\")} `\n\n**Note** If you are doing this lab offline (not on the webinar day),\nsubtracting -1 days may or may not work. You basically have to subtract enough\ndays so that the resultant date is a date when the stock market was open. So\nif you're doing this lab on Monday, subtract -3 days so that the date becomes\nFriday (assuming the stock market was open on Friday)\n\nCombine Conditions: `AND`\n\n**Note** that we entered sets the variable yest_date to yesterday's date, in a\nyyyy-mm-dd format.\n\n!8_yest_date\n\n  6. **Sample** the data by switching to the Sample tab and clicking !8_data to validate the filter is working correctly, with the DATE field reflecting yesterday's date.\n\n!8_sample.png\n\n  7. Locate the Table Input component and drag and drop it to the above-right of the FILTER ON YEST_DATE Filter component. Click on the component and edit the Properties as follows:", "start_char_idx": 2, "end_char_idx": 1714, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72062af9-3fa2-4e19-9042-58b69d05c586": {"__data__": {"id_": "72062af9-3fa2-4e19-9042-58b69d05c586", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "106d0f0a-9104-4283-a354-95c62e57c866", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "b3e4f35e8ece9a61011cebdef78552518911572348c355c522ba2bf01132ea92", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14450234-6df5-461c-a1d4-aabd69f184c2", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "6e9677543f9d4c8eac253375952e9efa63161d45e77f8326738e03e40b84faf3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04efc28d-9f1f-4d85-8319-2c0a63072f69", "node_type": "1", "metadata": {}, "hash": "85793173102daa30984ce9914360a552380402075643b8fe57e620ca030f8441", "class_name": "RelatedNodeInfo"}}, "text": "Table Input \\- Read\nCURRENT_POSITION\n\nName: `CURRENT_POSITION`  \nTarget Table: `CURRENT_POSITION`  \nColumn Names: `Select all columns`\n\n!8_current_pos\n\n  1. Locate the Join component, drag and drop into the workspace and connect the previous Filter and Table Input components. The flow should look like this:\n\n!8_flow_join", "start_char_idx": 2, "end_char_idx": 324, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04efc28d-9f1f-4d85-8319-2c0a63072f69": {"__data__": {"id_": "04efc28d-9f1f-4d85-8319-2c0a63072f69", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "376f4579-e99f-4f91-ab15-163e9fb527f8", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "435c814d1b671deb79674ce0eb01ed59fb4926b1472c4169780da5bd3f4a2c8a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72062af9-3fa2-4e19-9042-58b69d05c586", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "81818bc106e1a10422b79a07fc454fa5a18eb7091ed138a7757bd1d4266d7c4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "806bb1e9-799a-4f9f-9c52-d63b275c6d94", "node_type": "1", "metadata": {}, "hash": "0502e1b754af25a740f8ae89f50408800b1c89fc914484f3017b3a77d6884522", "class_name": "RelatedNodeInfo"}}, "text": "Join \\- Join\nyestserday's stock close with the CURRENT_POSITION dataset\n\n  1. Click on the Join component to edit its properties as follows:\n\nName: `Join CURRENT_POSITION and STOCK_HISTORY`  \nMain Table: `CURRENT_POSITION`  \nMain Table Alias: `current_position`  \nJoins: `FILTER ON YEST_DATE` , `stock_history` , `Left`\n\n  2. Edit the Join Expressions property to add the following:\n\n**Join Expressions:  \n** current_position_Left_stock_history: `\"current_position\".\"SYMBOL\" =\n\"stock_history\".\"SYMBOL\"`\n\n!8_join_expressions\n\n  3. Update the Output Columns property to add the following:\n\n**Output Columns:  \n** current_position.TRADER: `TRADER`  \ncurrent_position.SYMBOL: `SYMBOL`  \ncurrent_position.sum_INVESTMENT: `sum_INVESTMENT`  \ncurrent_position.sum_SHARESBOUGHT: `sum_SHARESBOUGHT`  \ncurrent_position.sum_RETURN: `sum_RETURN`  \ncurrent_position.sum_SHARESSOLD: `NET_SHARES`  \ncurrent_position.NET_SHARES: `NET_SHARES`  \ncurrent_position.NET_VALUE: `NET_VALUE`  \ncurrent_position.AVG_PRICE: `AVG_PRICE`  \nstock_history.CLOSE: `CLOSE`\n\n!8_output_col\n\n  4. Click **OK**\n\n!8_join_prop\n\nThe job flow now looks like this:\n\n!8_join_flow\n\nLet's now calculate the realized and unrealized gains/losses for each trader.", "start_char_idx": 2, "end_char_idx": 1217, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "806bb1e9-799a-4f9f-9c52-d63b275c6d94": {"__data__": {"id_": "806bb1e9-799a-4f9f-9c52-d63b275c6d94", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e5b25b5-d5cd-4df8-8d39-641ed9f4be5e", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "baa68a704e608ecb3a446d13e9e2e8bd7047805c12d8e84645118d18f2a5fe1c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04efc28d-9f1f-4d85-8319-2c0a63072f69", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "e38c64a083116e9cb1c5ee90f6fc5588247f9893273f8beebc730971c3a166ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d45bbc1f-1d6e-4c89-842c-7f813e91a44d", "node_type": "1", "metadata": {}, "hash": "64810804f7e00a9175c5b12a37f73a53faafe30913bea39207910328b8c3b50b", "class_name": "RelatedNodeInfo"}}, "text": "Calculator \\- Calculate\nRealized & Unrealized Gains\n\n  1. Locate the calculator component. Drag and drop it to the end of the flow and connect it to the Join component.\n\n!8_calc\n\n  2. Click on the Calculator component and edit the Properties as follows:\n\nName: `GAINS`  \nInclude Input Columns: `Yes`\n\n  3. Edit the Calculations property, and add the following expressions.\n\n**Expressions:  \n** UNREAL_GAINS: `(\"NET_SHARES\" * \"CLOSE\") - (\"NET_SHARES\" * \"AVG_PRICE\")`  \nREAL_GAINS: `CASE WHEN \"NET_SHARES\" = 0 THEN \"NET_VALUE\" ELSE \"sum_INVESTMENT\"\n- (\"sum_SHARESSOLD\" * \"AVG_PRICE\") END`\n\n!8_real\n\n!8_unreal\n\n!8_gains_flow\n\nThe flow should now look like this:\n\n!8_gains_flow2\n\n  4. Now, let's write the results to a new table called TRADER_PNL_TODAY using the Rewrite Table component.", "start_char_idx": 2, "end_char_idx": 785, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d45bbc1f-1d6e-4c89-842c-7f813e91a44d": {"__data__": {"id_": "d45bbc1f-1d6e-4c89-842c-7f813e91a44d", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "186a0c4f-4cb9-4256-a5f4-53c96ddb73f5", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "84498b075b32adda87dd5f2e0882eb2d239fc4ba294a0c70c99ce3b1af2e4fb7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "806bb1e9-799a-4f9f-9c52-d63b275c6d94", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "6ffffb1d2bffd7dcbdf5f83b2b5f46b02c763cc33380e440f7c16a0f4be9f398", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38eef608-e212-49f5-b019-c3827e95fcc8", "node_type": "1", "metadata": {}, "hash": "81a9d87048f0c08b898950c0c06b760eb82c1e54efe6b839778f6cfc9f887a7f", "class_name": "RelatedNodeInfo"}}, "text": "Rewrite Table\n\n  1. Locate the Rewrite Tablecomponent and drag and drop into the workspace. Link it to the GAINS calculator, and click to edit the Properties as follows:\n\nName: `TRADER_PNL_TODAY`  \nTarget Table: `TRADER_PNL_TODAY`\n\n!8_pnl_today\n\nThe flow should now look like this:\n\n!8_pnl_flow\n\n  2. Now, locate a Aggregate component to it connect to the Calculator GAINS component, creating a parallel flow.\n\n!8_para_flow", "start_char_idx": 2, "end_char_idx": 425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38eef608-e212-49f5-b019-c3827e95fcc8": {"__data__": {"id_": "38eef608-e212-49f5-b019-c3827e95fcc8", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcf65fc6-f88e-48b4-87f2-1900d41de02b", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "0514618d17cfaa644f0ccf24704e842d13208e144583966f9145afe71d84f20f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d45bbc1f-1d6e-4c89-842c-7f813e91a44d", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "3e6e464d1996256e6048bfb69180ca14776ce7e456ce815a246745274bfe3969", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f716fa92-79fa-4e58-9e5d-554f43d5c217", "node_type": "1", "metadata": {}, "hash": "30effa41c8da6d63c28e192db373f258844ceb92095582e4206610dcb68be570", "class_name": "RelatedNodeInfo"}}, "text": "Aggregate \\- Sum up the\ngains, both realized and unrealized by each trader\n\n  1. Click on the Aggregate component and edit the Properties as follows:\n\nName: `SUM GAINS PER TRADER`  \nGroupings: `TRADER`  \nAggregations: `UNREAL_GAINS, Sum` , `REAL_GAINS, Sum`\n\n!8_agg_prop\n\nThe flow should now look like this:\n\n!8_agg_flow\n\nFinally, we are going to create a view to store this last aggregation result.", "start_char_idx": 2, "end_char_idx": 401, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f716fa92-79fa-4e58-9e5d-554f43d5c217": {"__data__": {"id_": "f716fa92-79fa-4e58-9e5d-554f43d5c217", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eba9096b-64db-4b8b-94ab-5f02e1ba42b9", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "46a3e4f327c978249353ec1c4613121bbafb57745b65eded4478cd31e151e142", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38eef608-e212-49f5-b019-c3827e95fcc8", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "62e027c542d81702b7b6e846f68a2214d201ef20147720a7b43146d5c0f7f3aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26bda402-a0b0-4b52-9c33-cb4c37723d1f", "node_type": "1", "metadata": {}, "hash": "05e81f464b1da0b277f18dcbd19fbfa208ba5797697d4dda06815c557b239b14", "class_name": "RelatedNodeInfo"}}, "text": "Create View \\- Write\nthe trader and gains fields to a new view in Snowflake\n\n  1. Locate the Rewrite Table component and drag and drop it to connect to the SUM GAINS PER TRADER Aggregate component.\n  2. Click on the component and edit the Properties as follows:\n\nName: `TRADER_PNL_TOTAL_VIEW`  \nTarget Table: `TRADER_PNL_TOTAL_VIEW`\n\n!8_rewrite_today\n\nThe final flow of the job, should look like this:\n\n!8_today_flow\n\nYou can check the datasets either with the Matillion sample function or go to\nSnowflake UI. There should be two tables created TRADER_PNL_TODAY and\nTRADER_PNL_TOTAL_VIEW.\n\n!8_snowflake_today", "start_char_idx": 2, "end_char_idx": 610, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26bda402-a0b0-4b52-9c33-cb4c37723d1f": {"__data__": {"id_": "26bda402-a0b0-4b52-9c33-cb4c37723d1f", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "434464da-3718-4906-b4a9-4f0dacf6e2af", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "287a4ebcfc1bb7ddb8d9c6bfc5ad9fbaadf5c529861a2e16b11049f92a547c8e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f716fa92-79fa-4e58-9e5d-554f43d5c217", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "09c4ab173e56bc560caa6ca0b26543da1ddf02edcb873b9eaf4686de3b03fb46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c78f99d-86c0-43f5-bd0b-4679c6062b4b", "node_type": "1", "metadata": {}, "hash": "081e986d0cec0d24e663c819602b1b73f29ecb287fdef365a0d608a3ea5acd74", "class_name": "RelatedNodeInfo"}}, "text": "Completing the Orchestration Job:\n\nReturn back to the VHOL_orchestration job, and drag and drop an **Alter\nWarehouse** component as the final step, linked to the VHOL_PNL_xform\nTransformation component.\n\n_Pro tip: you can also COPY and PASTE the other Alter Warehouse component to\njust edit it._\n\nEdit the component to reflect as follows:\n\nName:\n\n|\n\n`Size Down Warehouse to XS`  \n  \n---|---  \n  \nCommandType:\n\n|\n\n`Set`  \n  \nProperties:\n\n|\n\n`WAREHOUSE_SIZE XSMALL`  \n  \nThis will scale down your Virtual Warehouse after the orchestration job is\ncompleted.\n\n!8_alter_final\n\nYour final pipeline result should now look like this:\n\n!8_final_flow\n\nRight click anywhere on the workspace click **Run Job** to run the job and\nenjoy seeing the data being loaded, transformed, while scaling up and down\nSnowflake warehouse dynamically!\n\nThe portfolio manager wants up-to-date stock information to know exactly where\ntheir realized and unrealized gains stand. Utilizing Matillion's Universal\nConnectivity feature they can pull real-time market prices and make the\ncalculation.\n\n  1. Begin by right-clicking in the Project Explorer and select Add Orchestration job to create a new Orchestration job. Name it Yahoo_Orch.\n  2. Righ click on the canvas, and click Manage Grid Variables.\n\n!9_1\n\n  3. Create a Grid Variable called `gv_tickers`, with a single column (`gvc_tickers`) populated with: AAPL and SBUX.\n\n!9_2\n\n  4. Click **Next** to add the columns AAPL and SBUX.\n\n!9_3\n\n  5. Click on **Project** dropdown and select **Manage Environment Variables**\n\n!9-4\n\n  6. Create a Environment Variable called `ev_tickerlist` using the following properties:\n\nName:\n\n|\n\n`ev_tickerlist`  \n  \n---|---  \n  \nType:\n\n|\n\n`text`  \n  \nBehavior:\n\n|\n\n`Copied`  \n  \nValue:\n\n|\n\n`AAPL%2CGOOG`  \n  \n!9-5\n\n  6. Drag and drop the **Query Result to Grid** component as the first step in the flow. Fill out the component as follows:\n\nName:\n\n|\n\n`Tickers to Grid`  \n  \n---|---  \n  \nBasic/Advanced\n\n|\n\n`Advanced`  \n  \nSQL Query\n\n|\n\n`SELECT DISTINCT(\"SYMBOL\") FROM \"TRADES_HISTORY\" WHERE \"TRADER\" = 'CERSEI'\nLIMIT 10`  \n  \nGrid Variable\n\n|\n\n`gv_tickers`  \n  \nGrid Variable Mapping\n\n|\n\n`gvc_tickers: SYMBOL`  \n  \n!9-6\n\n!9-7", "start_char_idx": 2, "end_char_idx": 2181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c78f99d-86c0-43f5-bd0b-4679c6062b4b": {"__data__": {"id_": "6c78f99d-86c0-43f5-bd0b-4679c6062b4b", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9bbc953-907a-46ec-a9c3-31c23d7ea908", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "4539a9962b5d1d0b7052bb437b741d53efa8cabf294eebceed835b30e43dd676", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26bda402-a0b0-4b52-9c33-cb4c37723d1f", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "16d764fac3d4076056d25c20954bd9ac3052e47c8cc8d1f161b75be3cfc2d897", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "965b3ea3-b7ef-45e5-a898-2a1919a5f259", "node_type": "1", "metadata": {}, "hash": "191d87ca58af8b6351be5f1e51fb073d3e7024018b667948c022ff3015b52dd0", "class_name": "RelatedNodeInfo"}}, "text": "Python Script\n\nWe will incorporate a Python script to \"unpack\" the Grid Variable set in the\nnext step. With the stock symbols saved to a variable called loc_TICKERS, a\nloop will be performed to reformat a query parameter needed for a call to the\nYahoo! Finance quote endpoint.\n\n  1. Locate the **Python Script** component and drop as the last step in the flow:\n\n!9-8\n\n  2. Update the Python Script component with the following:\n\n**Script** :\n\n    \n    \n    print (context.getGridVariable('gv_tickers'))\n    loc_TICKERS = context.getGridVariable('gv_tickers')\n    \n    api_param = ''\n    \n    for layer1 in loc_TICKERS:\n      for each in layer1:\n        api_param = api_param + each + '%2C'\n        #print(each) validate unpackaging of array\n        \n    api_param = api_param[:-3]\n    print(api_param)\n    \n    context.updateVariable('ev_tickerlist', api_param)\n    \n    print(ev_tickerlist)\n    \n    \n\n**Interpeter:**`Python 3`", "start_char_idx": 2, "end_char_idx": 930, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "965b3ea3-b7ef-45e5-a898-2a1919a5f259": {"__data__": {"id_": "965b3ea3-b7ef-45e5-a898-2a1919a5f259", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4910572b-9e9c-46f7-b230-12f17e471e52", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "3dbca17a3fdfcd566f5f1918078f871f93c1dcd134ede1e74427251b167608ef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c78f99d-86c0-43f5-bd0b-4679c6062b4b", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "68df93ccca7a583d5c799b2e539d4ce2d81b20e698c7cf26c84bab7c45b1bb78", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95ef1cc5-6086-4ed4-a48a-6dd39e3a1c26", "node_type": "1", "metadata": {}, "hash": "29a28e36761b013b58c80c19043f52883b99f215436233b262d4df3615014f4c", "class_name": "RelatedNodeInfo"}}, "text": "API-Extract \\- Pull\nCurrent Stock Price Data\n\n  1. From the Projects drop down in the top left, select **Manage API Profiles** > **Manage Extract Profiles**.\n\n!9-9\n\n  2. Add a new **Extract Profile** using the information below:\n\n**Profile Name:**`YahooFinance`\n\n!9-10\n\n  3. Click **Ok** and select **New Endpoint** , and update with the following information:\n\n**Endpoint Name:**`QuotesByTicker`\n\n  4. Click **Next**.\n  5. Set the Endpoint Configuration GET to the following URI: `https://yfapi.net/v6/finance/quote`\n  6. Select the **Params** tab and update with the following:\n\n**Params:**\n\n| |  \n---|---|---  \n  \n`lang`\n\n|\n\n`en`\n\n|\n\n`Query`  \n  \n`region`\n\n|\n\n`US`\n\n|\n\n`Query`  \n  \n`symbols`\n\n|\n\n`AAPL,BTC-USD,EURUSD=X`\n\n|\n\n`Query`  \n  \n`X-API-KEY`\n\n|\n\n**`SEE NOTE BELOW`**\n\n|\n\n`Header`  \n  \n**NOTE:** Your X-API-KEY must be obtained from Yahoo Finance API (This can be\nretrieved by following the instructions\nHERE)\n\n  7. Click **Next** and **Finish** to complete creating the Endpoint.\n  8. Locate the **API Extract** component and place is after the **Python Script** component.\n\n!9-11\n\n  9. Update the component as follows:\n\n|  \n---|---  \n  \nProfile\n\n|\n\n`YahooFinance`  \n  \nData Source\n\n|\n\n`QuotesByTicker`  \n  \nQuery Params\n\n|\n\n`lang - en`  \n  \n|\n\n`region - US`  \n  \n|\n\n`symbols - ${ev_tickerlist}`  \n  \nHeader Params\n\n|\n\n**`YOUR API KEY`**  \n  \nLocation\n\n|\n\n`Select the default S3 bucket provided by Partner Connect`  \n  \nTable\n\n|\n\n`VHOL_YAHOORAW`  \n  \n  10. Now we will create a new Transformation Job - Yahoo Transform - which will sit as the next step after the Yahoo Orchestration job just worked on.", "start_char_idx": 2, "end_char_idx": 1614, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95ef1cc5-6086-4ed4-a48a-6dd39e3a1c26": {"__data__": {"id_": "95ef1cc5-6086-4ed4-a48a-6dd39e3a1c26", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc326342-a839-4cc3-90f1-38c1a0e27f01", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "c797fad572fd67d45a68b6a4cf658569421bfc9da5bef07234a601acbae56eca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "965b3ea3-b7ef-45e5-a898-2a1919a5f259", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "9f313e9e86df4bb1bfc366840e6115449de1d666aab3ea37295b97124b2721b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05b23e8e-78df-4abf-a5d8-82c2537ba0a0", "node_type": "1", "metadata": {}, "hash": "a2a16f3f0318cb03a9e89d9ee5581bae46d85ed2a5d4b3f4de6b206229c0cc4c", "class_name": "RelatedNodeInfo"}}, "text": "Transformation - Yahoo Transform\n\n  1. Within the Projects Explorer, right click and select **Add Transformation Job**. Name it Yahoo_transform.\n  2. Create a new Table Input component and update as follows:\n\n| |  \n---|---|---  \n  \nName\n\n|\n\n`VHOL_YAHOORAW`\n\n|  \n  \nTarget Table\n\n|\n\n`VHOL_YAHOORAW`\n\n|  \n  \nColumn Names\n\n|\n\n`Data Value`\n\n|  \n  \n!9-12", "start_char_idx": 2, "end_char_idx": 351, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05b23e8e-78df-4abf-a5d8-82c2537ba0a0": {"__data__": {"id_": "05b23e8e-78df-4abf-a5d8-82c2537ba0a0", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "029099c6-84d0-406d-bef8-cfbbfd73c0b0", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "d8c6edcffe7251f9b31ceb0ef4e7c9ed7b47158588cd4dad7fab3eb23954dc6e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95ef1cc5-6086-4ed4-a48a-6dd39e3a1c26", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "6d337bf88ed23d7559ea78447aac18aed35c9209ed86a20a93e418523d568b83", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b97bdc72-dc8f-498d-af3e-5297e4ce1aa9", "node_type": "1", "metadata": {}, "hash": "38c4c07aa2ab1f180a7b0fb57c4bbca74faa38eb9afac9283e796ab594de3c59", "class_name": "RelatedNodeInfo"}}, "text": "Extract Nested Data \\-\nWe will flatten the semi structured format & extract the values needed\n\n  1. Find the **Extract Nested Data** Component and drag and drop it after the Table Input.\n  2. Update the component as follows:\n\n| |  \n---|---|---  \n  \nName\n\n|\n\n`Extract Nested Data`\n\n|  \n  \nInclude Input Column\n\n|\n\n`No`\n\n|  \n  \nColumns: Select **Autofill** , to populate all the available columns and\nselect the following values:\n\n`displayName, regularMarketPrice, symbol`\n\n!9-13", "start_char_idx": 2, "end_char_idx": 479, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b97bdc72-dc8f-498d-af3e-5297e4ce1aa9": {"__data__": {"id_": "b97bdc72-dc8f-498d-af3e-5297e4ce1aa9", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cee08ed0-4513-4e2d-9d2d-a97de599c4ff", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "b44e3b487fc416d774c2d43906f61d607f9b303a017e797dc85d84349cc7038f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05b23e8e-78df-4abf-a5d8-82c2537ba0a0", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "e09fba04594e6d4e65ffa9208e77e4b4cbdc4a7e18c5088f5dc6dde0463a56a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "639e14a0-3ccd-4a7b-bd12-7a53bc437331", "node_type": "1", "metadata": {}, "hash": "0aa9105a3e301f7a286f6fac8b5b79e785407f21384d8e0037a8f5c9cc533910", "class_name": "RelatedNodeInfo"}}, "text": "We will now read TRADER_PNL_TODAY from our previous transformation job.\n\n  1. Locate the **Table Input** component and place is underneath the previous Table Input. And update the properties as follows:\n\n| |  \n---|---|---  \n  \nName:\n\n|\n\n`TRADER_PNL_TODAY`\n\n|  \n  \nTarget Table:\n\n|\n\n`TRADER_PNL_TODAY`\n\n|  \n  \nColumn Names:\n\n|\n\nSelect all columns\n\n|  \n  \n!9-14", "start_char_idx": 2, "end_char_idx": 361, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "639e14a0-3ccd-4a7b-bd12-7a53bc437331": {"__data__": {"id_": "639e14a0-3ccd-4a7b-bd12-7a53bc437331", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d44e99ff-027a-440a-a126-6d8b141a29a3", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "f82527c2ce0638a002e10b0fcf9864ea78a2856b2264a402e91c0ea8e97f4e31", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b97bdc72-dc8f-498d-af3e-5297e4ce1aa9", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "18da681a4ce7bb7a2d4dc540ab32e0965943347e2c9248db98ac71a2f0543295", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d50f388c-4891-4cbd-80f9-77f515e1bc6d", "node_type": "1", "metadata": {}, "hash": "bb0ff32291e2f3f6d6661ca3c7ed5fd9d759cfe3f237f12dbdbefb71d54ecad5", "class_name": "RelatedNodeInfo"}}, "text": "Now we will only filter for Cersei's trades by using the Filter component.\n\n  1. Locate the **Filter** component and connect it to the table input from the previous step. And update the properties as follows:\n\n| |  \n---|---|---  \n  \nName:\n\n|\n\n`Cersei's Trades`\n\n|  \n  \nInput Column:\n\n|\n\n`Trader`\n\n|  \n  \nQualifier:\n\n|\n\n`Is`\n\n|  \n  \nComparator:\n\n|\n\n`Value`\n\n|  \n  \nValue:\n\n|\n\n`CERSEI`\n\n|  \n  \n!9-15", "start_char_idx": 2, "end_char_idx": 399, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d50f388c-4891-4cbd-80f9-77f515e1bc6d": {"__data__": {"id_": "d50f388c-4891-4cbd-80f9-77f515e1bc6d", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85cebce5-42fb-4a8c-b87e-7e952b0f483b", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "df73a0b60a2defd772b54606e8157eb3ffd23059acd293c7f1925d9826a81a0c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "639e14a0-3ccd-4a7b-bd12-7a53bc437331", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "bb9d3c67d2c3a5e0dcb5dd3f0bf565d7fadc2fdb106171bd3115401f9d96fe44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eadff117-e0df-49dd-8027-2517eaea58bd", "node_type": "1", "metadata": {}, "hash": "b2074152b20463c6954c5c751de78205e142cc548e77bcdad1fe47689d14ff16", "class_name": "RelatedNodeInfo"}}, "text": "Now we will join Cersei's trades with the Yahoo API data using the Join\ncomponent.\n\n  1. Locate the **Join** component and connect to the Extract Nested Data and Cersei's Trades, and update the properties as follows:\n\n| |  \n---|---|---  \n  \nName:\n\n|\n\nJoin\n\n|  \n  \nMain Table:\n\n|\n\n`Cersei's Trades`\n\n|  \n  \nMain Table Alias:\n\n|\n\n`cersei`\n\n|  \n  \nJoins:\n\n|\n\n`Joins Table - Extract Nested Data`\n\n|  \n|\n\n`Join Alias - trades`\n\n|  \n|\n\n`Join Type - Inner`\n\n|  \n  \n**Join Expressions:**\n\n| |  \n---|---|---  \n  \ncersei_inner_trades\n\n|\n\n`\"cersei\".\"SYMBOL\" = \"trades\".\"symbol\"`\n\n|  \n  \n**Output Columns:**\n\n| |  \n---|---|---  \n  \n`cersei.TRADER`\n\n|\n\n`TRADER`\n\n|  \n  \n`cersei.SYMBOL`\n\n|\n\n`SYMBOL`\n\n|  \n  \n`trades.regularMarketPrice`\n\n|\n\n`MARKETPRICE`\n\n|  \n  \n`cersei.AVG_PRICE`\n\n|\n\n`AVG_PRICE`\n\n|  \n  \n`cersei.NET_SHAERES`\n\n|\n\n`# SHARES`\n\n|  \n  \n!9-16", "start_char_idx": 2, "end_char_idx": 842, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eadff117-e0df-49dd-8027-2517eaea58bd": {"__data__": {"id_": "eadff117-e0df-49dd-8027-2517eaea58bd", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b5efa891-4a65-4d19-ad79-3adfb61d5a50", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "2ad70d46340b4a44fcc2e71fab3175e26eb6ddc848a0ca4678598331f40bb494", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d50f388c-4891-4cbd-80f9-77f515e1bc6d", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "e9d0484f5801b643a65c22041ad9f749ed7c83ebbd682ceea757f54a71380f1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccba5cf1-3d9b-4d8b-b203-a5ec70676e44", "node_type": "1", "metadata": {}, "hash": "c577310947471edd76e6dec6548c5dfde833347e6563ffbd30eac21b2ea92452", "class_name": "RelatedNodeInfo"}}, "text": "Calculate the Win / Loss Logic\n\n  1. Drag and drop the Calculator component as the last step of the flow and update as follows:\n\n| |  \n---|---|---  \n  \nName:\n\n|\n\n`Calculator`\n\n|  \n  \nExpressions:\n\n|  \n---|---  \n  \n`MARKET_VALUE`\n\n|\n\n`\"# SHARES\" * \"MARKETPLACE\"`  \n  \n`PORTFOLIO_VALUE`\n\n|\n\n`\"AVG_PRICE\" * \"# SHARES\"`  \n  \n`UNREALIZED_GAINS`\n\n|\n\n`(\"AVG_PRICE\" * \"# SHARES\") - (\"# SHARES\" * \"MARKETPLACE\")`  \n  \n!9-17\n\nFinally, we will write Cersei's profits back to Snowflake using the\n**Rewrite** component, and update as follows:\n\n|  \n---|---  \n  \nName:\n\n|\n\n`CERSEI PROFITS`  \n  \nTarget Table:\n\n|\n\n`CERSEI PROFITS`  \n  \nYour final flow should now look like this:\n\n!9-18\n\nWhat this shows is the stock, quantity, and the real time average price of\neach stock. The resulting table is how much realized gains Cersei can expect\nbased on the quantity of shares she owns. You can check the data in Snowflake\nto see that it was written correctly:\n\n!9-19\n\nCongrats! You have successfully developed a well-orchestrated data engineering\npipeline!", "start_char_idx": 2, "end_char_idx": 1037, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ccba5cf1-3d9b-4d8b-b203-a5ec70676e44": {"__data__": {"id_": "ccba5cf1-3d9b-4d8b-b203-a5ec70676e44", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fb3db9aa-6583-4276-a46a-10659bd4fc10", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "aa10d4097509abe2cb6bbfc138f48e7e187e5676e1c35709cae7f43c65e2ecd9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eadff117-e0df-49dd-8027-2517eaea58bd", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "b875544516bd1f0843d069c21f32c3b519211eb58d7a2e54898e6cbd50248eae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3be92cbc-3c55-40c6-8f74-d5449eae7262", "node_type": "1", "metadata": {}, "hash": "8db0175ed61cbbdf008e4d2a9daa4d017f6dda411c16b61e6d6fae841d651b50", "class_name": "RelatedNodeInfo"}}, "text": "What we have covered\n\n  * Source 3rd party data from Snowflake data marketplace\n  * Use Matillion's GUI to build end-to-end transformation pipeline\n  * Leverage Matillion scale up/down Snowflake's virtual warehouses\n\nUsing Matillion ETL for Snowflake we were able to easily extract data from S3,\nperform complex joins, filter and aggregate through an intuitive, browser\nbased, easy to use UI. If we were to have used traditional ETL tools, it would\nhave required a lot code, resources, and time to complete.\n\nMatillion ETL makes data engineering easier by allowing you to build your data\npipelines more efficiently with a low-code/no-code platform built for the Data\nCloud. We can build complex data pipelines to scale up and down within\nSnowflake based on your workload profile.", "start_char_idx": 2, "end_char_idx": 781, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3be92cbc-3c55-40c6-8f74-d5449eae7262": {"__data__": {"id_": "3be92cbc-3c55-40c6-8f74-d5449eae7262", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e5ff74d-d112-4fb7-8eec-fae6954cd457", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "7980b1078cba29324376ee0640703d5d8274f8fb5bc74bd7e20caab0770c3145", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccba5cf1-3d9b-4d8b-b203-a5ec70676e44", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "0ccf88638b52256dedd281be9e070122cc3ab0b6c438ea43cd8d3723a802f737", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e5a72b7-6352-44bb-be25-3272f9ba2341", "node_type": "1", "metadata": {}, "hash": "adadbe96c037c4e69e3d69dc981fdb89787d07ab31184873d17b1be44ae47405", "class_name": "RelatedNodeInfo"}}, "text": "Related Resources\n\n  * Snowflake Docs\n  * Matillion Docs", "start_char_idx": 2, "end_char_idx": 58, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e5a72b7-6352-44bb-be25-3272f9ba2341": {"__data__": {"id_": "3e5a72b7-6352-44bb-be25-3272f9ba2341", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e99f547-c15e-4eda-9493-c0c5f39beebb", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "c3ab38e32eb02332566eb777e0d42f4b5159ad270d783da47596ef568e848aa5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3be92cbc-3c55-40c6-8f74-d5449eae7262", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "911395fa49d9c9a43c3bb62912e733be9bcdd3afec2d1d1b1caa43dc269ebba4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b76e307-f671-4a5d-9643-516c3f7c93b0", "node_type": "1", "metadata": {}, "hash": "489d0abb504135f75125bd05f7ee31a306e91ee27e76dee1eec3a1d0981471ae", "class_name": "RelatedNodeInfo"}}, "text": "Prerequisites\n\n  * Familiarity with Python\n  * Familiarity with the DataFrame API\n  * Familiarity with Snowflake\n  * Familiarity with Git repositories and GitHub", "start_char_idx": 2, "end_char_idx": 163, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b76e307-f671-4a5d-9643-516c3f7c93b0": {"__data__": {"id_": "5b76e307-f671-4a5d-9643-516c3f7c93b0", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da9b7e1c-d5de-4cd4-9dcb-e3e8a66220be", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "0f94e0d667e097a66e10bc63633da0cacc2b2d53080ad66cfdf066454018ae20", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e5a72b7-6352-44bb-be25-3272f9ba2341", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "c4a6da7e7c141534e2acd4359e740b36ed8ecccb437a859abb7e4602d648aacb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76c0efbe-1b9c-4960-9174-35d2ec36abeb", "node_type": "1", "metadata": {}, "hash": "ac35dac1f5cb2afcc4f042ba9f7bcdc70de0749de1f0f7c96a3a1e3987812066", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Learn\n\nYou will learn about the following Snowflake features during this Quickstart:\n\n  * Snowflake's Table Format\n  * Data ingestion with COPY\n  * Schema inference\n  * Data sharing/marketplace (instead of ETL)\n  * Streams for incremental processing (CDC)\n  * Streams on views\n  * Python UDFs (with third-party packages)\n  * Python Stored Procedures\n  * Snowpark DataFrame API\n  * Snowpark Python programmability\n  * Warehouse elasticity (dynamic scaling)\n  * Visual Studio Code Snowflake native extension (PuPr, Git integration)\n  * SnowCLI (PuPr)\n  * Tasks (with Stream triggers)\n  * Task Observability\n  * GitHub Actions (CI/CD) integration", "start_char_idx": 2, "end_char_idx": 657, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76c0efbe-1b9c-4960-9174-35d2ec36abeb": {"__data__": {"id_": "76c0efbe-1b9c-4960-9174-35d2ec36abeb", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "33218217-3d78-4c4a-824f-f82492907e73", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "c30ccbe3a77396414799117b6f81ed9d86ef599251efda503aca5ba0e88e679c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b76e307-f671-4a5d-9643-516c3f7c93b0", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "dee89429c7450813ace24902b9649b05ee693f2cb8ac8afe67bdc5fe8c794027", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dafc84dd-d6a1-40ad-8d96-43d38d1c27cf", "node_type": "1", "metadata": {}, "hash": "745945b164797c580d0c5dd55e822166b93713f5d3997a9039e28ebf7fbab342", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Need\n\nYou will need the following things before beginning:\n\n  * Snowflake account \n    * **A Snowflake Account**\n    * **A Snowflake user created with ACCOUNTADMIN permissions**. This user will be used to get things setup in Snowflake.\n    * **Anaconda Terms & Conditions accepted**. See Getting Started section in Third-Party Packages.\n  * GitHub account \n    * **A GitHub account**. If you don't already have a GitHub account you can create one for free. Visit the Join GitHub page to get started.", "start_char_idx": 2, "end_char_idx": 513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dafc84dd-d6a1-40ad-8d96-43d38d1c27cf": {"__data__": {"id_": "dafc84dd-d6a1-40ad-8d96-43d38d1c27cf", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "20b56c6a-7e30-434e-afef-c9506ecab9ec", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "6f3ea2d32f88c33c92925e25a04a33b11d0fe72d8ca37be103ad8bd3c8e2aab2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76c0efbe-1b9c-4960-9174-35d2ec36abeb", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "f4b987b28af80f9809bef30e28fb579522213b5485f0fec56e10426db6919290", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04e0cef6-36a4-4b76-84b0-f9a296bd310c", "node_type": "1", "metadata": {}, "hash": "2822307e7b55172163010672f790040c857559b2f05408cf98b54b5f73f56ccd", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Build\n\nDuring this Quickstart you will accomplish the following things:\n\n  * Load Parquet data to Snowflake using schema inference\n  * Setup access to Snowflake Marketplace data\n  * Create a Python UDF to convert temperature\n  * Create a data engineering pipeline with Python stored procedures to incrementally process data\n  * Orchestrate the pipelines with tasks\n  * Monitor the pipelines with Snowsight\n  * Deploy the Snowpark Python stored procedures via a CI/CD pipeline", "start_char_idx": 2, "end_char_idx": 489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04e0cef6-36a4-4b76-84b0-f9a296bd310c": {"__data__": {"id_": "04e0cef6-36a4-4b76-84b0-f9a296bd310c", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "545eeb11-1a1b-4638-bbae-0ea57477cb54", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "23eb1b793f1ab6c34f19c18e08ac38856e3fd11e627eed45c63fab42b551d4b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dafc84dd-d6a1-40ad-8d96-43d38d1c27cf", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "2297e910f78008ca96790fee35c9643fcf0d14a76321f95fc99f3edb39025253", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da2f741d-5e0a-45ea-bfe5-09c851bd80fb", "node_type": "1", "metadata": {}, "hash": "fcd905845e113282653a6384330386c4c25bbacfa8e224b925fd6497288ea487", "class_name": "RelatedNodeInfo"}}, "text": "Fork the Quickstart Repository and Enable GitHub Actions\n\nYou'll need to create a fork of the repository for this Quickstart in your\nGitHub account. Visit the [Data Engineering Pipelines with Snowpark Python\nassociated GitHub Repository](https://github.com/Snowflake-Labs/sfguide-data-\nengineering-with-snowpark-python) and click on the \"Fork\" button near the top\nright. Complete any required fields and click \"Create Fork\".\n\nBy default GitHub Actions disables any workflows (or CI/CD pipelines) defined\nin the forked repository. This repository contains a workflow to deploy your\nSnowpark Python UDF and stored procedures, which we'll use later on. So for\nnow enable this workflow by opening your forked repository in GitHub, clicking\non the `Actions` tab near the top middle of the page, and then clicking on the\n`I understand my workflows, go ahead and enable them` green button.\n\n!", "start_char_idx": 2, "end_char_idx": 887, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da2f741d-5e0a-45ea-bfe5-09c851bd80fb": {"__data__": {"id_": "da2f741d-5e0a-45ea-bfe5-09c851bd80fb", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e7b2c337-0599-40d9-b210-cec1d9a7a25b", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "6879ad69068e184ca0f32ef2236e5dd5b9bb3c8a0b202009ac216a9b16910662", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04e0cef6-36a4-4b76-84b0-f9a296bd310c", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "4773a40c89447c28134dcc021f3196c9aead43b287ee0c5c30c8fd93919e33cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07574e85-5069-4f67-be09-6cc940e3623a", "node_type": "1", "metadata": {}, "hash": "87b46b0f8d71ef722cde7e792d0b4ae90119e95530077873543f6c3dfb432d6c", "class_name": "RelatedNodeInfo"}}, "text": "Create GitHub Codespace\n\nFor this Quickstart we will be using [GitHub\nCodespaces](https://docs.github.com/en/codespaces/overview) for our\ndevelopment environment. Codespaces offer a hosted development environment\nwith a hosted, web-based VS Code environment. GitHub currently offers [60\nhours for free each month](https://github.com/features/codespaces) when using\na 2 node environment, which should be more than enough for this lab.\n\nTo create a GitHub Codespace, click on the green ` Code` button from the\nGitHub repository homepage. In the Code popup, click on the `Codespaces` tab\nand then on the green `Create codespace on main`.\n\n!\n\nThis will open a new tab and begin setting up your codespace. This will take a\nfew minutes as it sets up the entire environment for this Quickstart. Here is\nwhat is being done for you:\n\n  * Creating a container for your environment\n  * Installing Anaconda (miniconda)\n  * SnowSQL setup \n    * Installing SnowSQL\n    * Creating a directory and default config file for SnowSQL\n  * Anaconda setup \n    * Creating the Anaconda environment\n    * Installing the Snowpark Python library\n    * Installing the SnowCLI Python CLI\n  * VS Code setup \n    * Installing VS Code\n    * Configuring VS Code for the Python Anaconda environment\n    * Installing the Snowflake VS Code extension\n  * Starting a hosted, web-based VS Code editor\n\nOnce the codepsace has been created and started you should see a hosted web-\nbased version of VS Code with your forked repository set up! Just a couple\nmore things and we're ready to start.", "start_char_idx": 2, "end_char_idx": 1554, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07574e85-5069-4f67-be09-6cc940e3623a": {"__data__": {"id_": "07574e85-5069-4f67-be09-6cc940e3623a", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e4acd37-ba41-48ad-921f-9b65dc9f47b2", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "2ef377284889536c6770ed1fed2f6cb908fca12d096bc8db21fba9aae468d691", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da2f741d-5e0a-45ea-bfe5-09c851bd80fb", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "a460e2180ff05eac6e2eedfa93ba250fe73f6cd98ef645f27ad66eb3eea930d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3feed60a-bf50-449c-8f93-514c0066c30e", "node_type": "1", "metadata": {}, "hash": "8ac91d0c0fbc81544d892fa198429dfb07c304071414e90dad2d970148d63d14", "class_name": "RelatedNodeInfo"}}, "text": "Configure Snowflake Credentials\n\nWe will not be directly using [the SnowSQL command line\nclient](https://docs.snowflake.com/en/user-guide/snowsql.html) for this\nQuickstart, but we will be storing our Snowflake connection details in the\nSnowSQL config file located at `~/.snowsql/config`. A default config file was\ncreated for you during the codespace setup.\n\nThe easiest way to edit the default `~/.snowsql/config` file is directly from\nVS Code in your codespace. Type `Command-P`, type (or paste)\n`~/.snowsql/config` and hit return. The SnowSQL config file should now be\nopen. You just need to edit the file and replace the `accountname`,\n`username`, and `password` with your values. Then save and close the file.\n\n**Note:** The SnowCLI tool (and by extension this Quickstart) currently does\nnot work with Key Pair authentication. It simply grabs your username and\npassword details from the shared SnowSQL config file.", "start_char_idx": 2, "end_char_idx": 921, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3feed60a-bf50-449c-8f93-514c0066c30e": {"__data__": {"id_": "3feed60a-bf50-449c-8f93-514c0066c30e", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "604cb90f-a657-40c8-8353-c58df0a35b61", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "da8eaeedf249c44ecef360f07f8669f1726bfc86354fa54a3cdf120b8d730985", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07574e85-5069-4f67-be09-6cc940e3623a", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "6fd702e33e02d4d9bda47a90fecd5042c0d71db619ac5394e89394cc04312286", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef3c8757-f0b2-4a18-b7ea-a5c24daeaeb4", "node_type": "1", "metadata": {}, "hash": "a84fd23d5bde06f4d693eb7b45bef4c5f60950c143e1ee45856083fa7d488bbd", "class_name": "RelatedNodeInfo"}}, "text": "Verify Your Anaconda Environment is Activated\n\nDuring the codespace setup we created an Anaconda environment named\n`snowflake-demo`. And when VS Code started up it should have automatically\nactivated the environment in your terminal. You should see something like this\nin the terminal, and in particular you should see `(snowflake-demo)` before\nyour bash prompt.\n\n!\n\nIf for some reason it wasn't activiated simply run `conda activate snowflake-\ndemo` in your terminal.", "start_char_idx": 2, "end_char_idx": 470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef3c8757-f0b2-4a18-b7ea-a5c24daeaeb4": {"__data__": {"id_": "ef3c8757-f0b2-4a18-b7ea-a5c24daeaeb4", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62b59f10-0a7b-4149-8fdc-05b0d5fd1ef3", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "74bad7e1aca50edcef366405d5ee35a25063e14f16d400514524281effb78a6c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3feed60a-bf50-449c-8f93-514c0066c30e", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "215e56593230c55c2e1b58ca0e174d66258d83de00ec68080a841d780ee6ab3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41907683-eced-4a7b-a812-64f10df7f0d3", "node_type": "1", "metadata": {}, "hash": "ee8f377725f5d48af8d18a0ee6a39de553996878b40f9ee766ca71248c58027e", "class_name": "RelatedNodeInfo"}}, "text": "Snowflake Extensions for VS Code\n\nYou can run SQL queries against Snowflake in many different ways (through the\nSnowsight UI, SnowSQL, etc.) but for this Quickstart we'll be using the\nSnowflake extension for VS Code. For a brief overview of Snowflake's native\nextension for VS Code, please check out our [VS Code Marketplace Snowflake\nextension\npage](https://marketplace.visualstudio.com/items?itemName=snowflake.snowflake-\nvsc).", "start_char_idx": 2, "end_char_idx": 431, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41907683-eced-4a7b-a812-64f10df7f0d3": {"__data__": {"id_": "41907683-eced-4a7b-a812-64f10df7f0d3", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6cd45afe-f968-436a-8e81-2dd2b58194f5", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "d80aacc43387ed738aedec83f9b8221411a1ba8cecc3906307761ddf79f84fde", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef3c8757-f0b2-4a18-b7ea-a5c24daeaeb4", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "c69112d708a8b50761e9157b9d85fbaa899dd930642dddb1c1b6122ee120f974", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac422644-bd04-4d7d-9f64-3b8e071a4fc7", "node_type": "1", "metadata": {}, "hash": "619406affa8fbafe71d2ff298cf81161324107c8fba39514779bb68e7529373c", "class_name": "RelatedNodeInfo"}}, "text": "Run the Script\n\nTo set up all the objects we'll need in Snowflake for this Quickstart you'll\nneed to run the `steps/01_setup_snowflake.sql` script.\n\nStart by clicking on the Snowflake extension in the left navigation bar in VS\nCode. Then login to your Snowflake account with a user that has ACCOUNTADMIN\npermissions. Once logged in to Snowflake, open the\n`steps/01_setup_snowflake.sql` script in VS Code by going back to the file\nExplorer in the left navigation bar.\n\nTo run all the queries in this script, use the \"Execute All Statements\" button\nin the upper right corner of the editor window. Or, if you want to run them in\nchunks, you can highlight the ones you want to run and press CMD/CTRL+Enter.\n\nDuring this step we will be loading the raw Tasty Bytes POS and Customer\nloyalty data from raw Parquet files in `s3://sfquickstarts/data-engineering-\nwith-snowpark-python/` to our `RAW_POS` and `RAW_CUSTOMER` schemas in\nSnowflake. And you are going to be orchestrating this process from your laptop\nin Python using the Snowpark Python API. To put this in context, we are on\nstep **#2** in our data flow overview:\n\n!", "start_char_idx": 2, "end_char_idx": 1121, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac422644-bd04-4d7d-9f64-3b8e071a4fc7": {"__data__": {"id_": "ac422644-bd04-4d7d-9f64-3b8e071a4fc7", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f20c0e72-b4f0-4ada-bb96-6cdd95963ee5", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "4050a30415afb5c4d82d1b41fd6e353d092580748283551a5c6d5c4ab34de391", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41907683-eced-4a7b-a812-64f10df7f0d3", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "05690cfc38671821ac1559beb4b91e0655b4b9170e6e8f042909180727018020", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d91831d0-712f-48d8-be84-032035486903", "node_type": "1", "metadata": {}, "hash": "a12d763486b1531efc51df1960885f18a9487d5010d022499ba220701eb07402", "class_name": "RelatedNodeInfo"}}, "text": "Run the Script\n\nTo load the raw data, execute the `steps/02_load_raw.py` script. This can be\ndone a number of ways in VS Code, from a terminal or directly by VS Code. For\nthis demo you will need to execute the Python scripts from the terminal. So go\nback to the terminal in VS Code, make sure that your `snowflake-demo` conda\nenvironment is active, then run the following commands (which assume that your\nterminal has the root of your repository open):\n\n    \n    \n    cd steps\n    python 02_load_raw.py\n    \n\nWhile that is running, please open the script in VS Code and continue on this\npage to understand what is happening.", "start_char_idx": 2, "end_char_idx": 626, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d91831d0-712f-48d8-be84-032035486903": {"__data__": {"id_": "d91831d0-712f-48d8-be84-032035486903", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7deb220d-4a1f-4237-80b5-33f5c49d0e2c", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "05dd0df413069db5afc5253f578b2c918247cc34f1c634a1eed56aa94c3ce2e9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac422644-bd04-4d7d-9f64-3b8e071a4fc7", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "f0620d7732dc8bfa788660fa1b7ce4a779c21d7b8d8808117a685c603972199f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1da3857-695e-48a7-a157-2525c080587d", "node_type": "1", "metadata": {}, "hash": "84706ccb5e44c583b8d79aab34b460ef8fac8de4a97a0761a9a65bdcfb512c5f", "class_name": "RelatedNodeInfo"}}, "text": "Running Snowpark Python Locally\n\nIn this step you will be running the Snowpark Python code locally from your\nlaptop. At the bottom of the script is a block of code that is used for local\ndebugging (under the `if __name__ == \"__main__\":` block):\n\n    \n    \n    # For local debugging\n    if __name__ == \"__main__\":\n        # Add the utils package to our path and import the snowpark_utils function\n        import os, sys\n        current_dir = os.getcwd()\n        parent_dir = os.path.dirname(current_dir)\n        sys.path.append(parent_dir)\n    \n        from utils import snowpark_utils\n        session = snowpark_utils.get_snowpark_session()\n    \n        load_all_raw_tables(session)\n    #    validate_raw_tables(session)\n    \n        session.close()\n    \n\nA few things to point out here. First, the Snowpark session is being created\nin the `utils/snowpark_utils.py` module. It has multiple methods for obtaining\nyour credentials, and for this Quickstart it pulls them from the SnowSQL\nconfig file located at `~/.snowsql/config`. For more details please check out\nthe code for the [utils/snowpark_utils.py\nmodule](https://github.com/Snowflake-Labs/sfguide-data-engineering-with-\nsnowpark-python/blob/main/utils/snowpark_utils.py).\n\nThen after getting the Snowpark session it calls the\n`load_all_raw_tables(session)` method which does the heavy lifting. The next\nfew sections will point out the key parts.\n\nFinally, almost all of the Python scripts in this Quickstart include a local\ndebugging block. Later on we will create Snowpark Python stored procedures and\nUDFs and those Python scripts will have a similar block. So this pattern is\nimportant to understand.", "start_char_idx": 2, "end_char_idx": 1663, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1da3857-695e-48a7-a157-2525c080587d": {"__data__": {"id_": "d1da3857-695e-48a7-a157-2525c080587d", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "78440e52-aaff-470c-8781-fd2b5b3fc92d", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "0fc8b393ce89b3f42392335482b9dd9fcf80b6d7700fbc55658b0ac59b1293f7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d91831d0-712f-48d8-be84-032035486903", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "885ec8b91707bf7a5bf93baf7b875039e3b39edf97d8f8721acbff88f53edf8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c915dd27-2f1d-4a68-af5b-a311cb8d411d", "node_type": "1", "metadata": {}, "hash": "b7e95a74b45bbd1f661d3ca257406a9b42f6309598eafeedea246d8c3dc08a16", "class_name": "RelatedNodeInfo"}}, "text": "Viewing What Happened in Snowflake\n\nThe [Query History](https://docs.snowflake.com/en/user-guide/ui-snowsight-\nactivity.html#query-history) in Snowflake is a very power feature, that logs\nevery query run against your Snowflake account, no matter which tool or\nprocess initiated it. And this is especially helpful when working with client\ntools and APIs.\n\nThe Python script you just ran did a small amount of work locally, basically\njust orchestrating the process by looping through each table and issuing the\ncommand to Snowflake to load the data. But all of the heavy lifting ran inside\nSnowflake! This push-down is a hallmark of the Snowpark API and allows you to\nleverage the scalability and compute power of Snowflake!\n\nLog in to your Snowflake account and take a quick look at the SQL that was\ngenerated by the Snowpark API. This will help you better understand what the\nAPI is doing and will help you debug any issues you may run into.\n\n!", "start_char_idx": 2, "end_char_idx": 946, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c915dd27-2f1d-4a68-af5b-a311cb8d411d": {"__data__": {"id_": "c915dd27-2f1d-4a68-af5b-a311cb8d411d", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbb76a11-9160-489e-a013-51bee5761cc8", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "aadd75c00461a1517cb68277ed25dc8351e3d9e0511a787dd0cf2c9717968d6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1da3857-695e-48a7-a157-2525c080587d", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "91280c02a41de96688489aee64abab749c1ffd6e613d064ef5148dd3573138fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1da23db-1cdf-4c07-9463-d456e05cc406", "node_type": "1", "metadata": {}, "hash": "7732c30bac8b9a8aeef9f950f0aa9d0d182347a70cd5132f1ebb92162a7808da", "class_name": "RelatedNodeInfo"}}, "text": "Schema Inference\n\nOne very helpful feature in Snowflake is the ability to infer the schema of\nfiles in a stage that you wish to work with. This is accomplished in SQL with\nthe [`INFER_SCHEMA()`](https://docs.snowflake.com/en/sql-\nreference/functions/infer_schema.html) function. The Snowpark Python API does\nthis for you automatically when you call the `session.read()` method. Here is\nthe code snippet:\n\n    \n    \n        # we can infer schema using the parquet read option\n        df = session.read.option(\"compression\", \"snappy\") \\\n                                .parquet(location)", "start_char_idx": 2, "end_char_idx": 587, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1da23db-1cdf-4c07-9463-d456e05cc406": {"__data__": {"id_": "f1da23db-1cdf-4c07-9463-d456e05cc406", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0c28fc5-05d8-41d3-9bd2-fcc5cfa65b4a", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "2e93623561f2978e282caeca6fadb71d09ff0aaa8b433993fd53236ed64c5e89", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c915dd27-2f1d-4a68-af5b-a311cb8d411d", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "12227d90f3f262ec1690646bc35b30b0cd0af89c8858caeb0d41bb97b7de38e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2c740c4-5275-49c1-9ca6-3961c9bb1245", "node_type": "1", "metadata": {}, "hash": "0a900cc10913f8878e5e33c22632c1cc9cec5c87e77a9fb32546fb0372f37d68", "class_name": "RelatedNodeInfo"}}, "text": "Data Ingestion with COPY\n\nIn order to load the data into a Snowflake table we will use the\n`copy_into_table()` method on a DataFrame. This method will create the target\ntable in Snowflake using the inferred schema (if it doesn't exist), and then\ncall the highly optimized Snowflake [`COPY INTO `\nCommand](https://docs.snowflake.com/en/sql-reference/sql/copy-into-\ntable.html). Here is the code snippet:\n\n    \n    \n        df.copy_into_table(\"{}\".format(tname))", "start_char_idx": 2, "end_char_idx": 462, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2c740c4-5275-49c1-9ca6-3961c9bb1245": {"__data__": {"id_": "e2c740c4-5275-49c1-9ca6-3961c9bb1245", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9dd41ac-8aa2-4467-97a9-88b9db0a84d8", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "f1c6dfaf985be3cf6322cd4742cfbdda673c46b49483cd8b0103c476cd53876f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1da23db-1cdf-4c07-9463-d456e05cc406", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "be07b187e73af590cd78de945d746c4936c5abef6621c00d1d67e7a9219435e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf83cc77-2620-4ce1-8f92-d71f9b153818", "node_type": "1", "metadata": {}, "hash": "9d6ba5f873d9d05889d6a7b5c3f5c55b655973d3d2c925527fa5ee963fb3c6d2", "class_name": "RelatedNodeInfo"}}, "text": "Snowflake's Table Format\n\nOne of the major advantages of Snowflake is being able to eliminate the need\nto manage a file-based data lake. And Snowflake was designed for this purpose\nfrom the beginning. In the step we are loading the raw data into a structured\nSnowflake managed table. But Snowflake tables can natively support structured\nand semi-structured data, and are stored in Snowflake's mature cloud table\nformat (which predates Hudi, Delta or Iceberg).\n\nOnce loaded into Snowflake the data will be securely stored and managed,\nwithout the need to worry about securing and managing raw files. Additionally\nthe data, whether raw or structured, can be transformed and queried in\nSnowflake using SQL or the language of your choice, without needing to manage\nseparate compute services like Spark.\n\nThis is a huge advantage for Snowflake customers.", "start_char_idx": 2, "end_char_idx": 851, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf83cc77-2620-4ce1-8f92-d71f9b153818": {"__data__": {"id_": "cf83cc77-2620-4ce1-8f92-d71f9b153818", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c74bef42-e58f-4600-b5a8-d2fbccf4fb0f", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "e8534cd64ba56556c7e1c4741c38cee272db289d22787a798fad303e4b367230", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2c740c4-5275-49c1-9ca6-3961c9bb1245", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "5d89d65760feaf89deb8a6dac2b6e924a935649a46dedfc945d34bc48a28cac6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9169c54-efc4-490a-b76d-8c183d465ca3", "node_type": "1", "metadata": {}, "hash": "e9555a780ab78c15a3fb4d1ccc8d601d26b52eb0e9719a326f177779285da368", "class_name": "RelatedNodeInfo"}}, "text": "Warehouse Elasticity (Dynamic Scaling)\n\nWith Snowflake there is only one type of user defined compute cluster, the\nVirtual Warehouse,\nregardless of the language you use to process that data (SQL, Python, Java,\nScala, Javascript, etc.). This makes working with data much simpler in\nSnowflake. And governance of the data is completely separated from the compute\ncluster, in other words there is no way to get around Snowflake governance\nregardless of the warehouse settings or language being used.\n\nAnd these virtual warehouses can be dynamically scaled, in under a second for\nmost sized warehouses! This means that in your code you can dynamically resize\nthe compute environment to increase the amount of capacity to run a section of\ncode in a fraction of the time, and then dynamically resized again to reduce\nthe amount of capacity. And because of our per-second billing (with a sixty\nsecond minimum) you won't pay any extra to run that section of code in a\nfraction of the time!\n\nLet's see how easy that is done. Here is the code snippet:\n\n    \n    \n        _ = session.sql(\"ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE\").collect()\n    \n        # Some data processing code\n    \n        _ = session.sql(\"ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XSMALL\").collect()\n    \n\nPlease also note that we included the `WAIT_FOR_COMPLETION` parameter in the\nfirst `ALTER WAREHOUSE` statement. Setting this parameter to `TRUE` will block\nthe return of the `ALTER WAREHOUSE` command until the resize has finished\nprovisioning all its compute resources. This way we make sure that the full\ncluster is available before processing any data with it.\n\nWe will use this pattern a few more times during this Quickstart, so it's\nimportant to understand.\n\nDuring this step we will be \"loading\" the raw weather data to Snowflake. But\n\"loading\" is the really the wrong word here. Because we're using Snowflake's\nunique data sharing capability we don't actually need to copy the data to our\nSnowflake account with a custom ETL process. Instead we can directly access\nthe weather data shared by Weather Source in the Snowflake Data Marketplace.\nTo put this in context, we are on step **#3** in our data flow overview:\n\n!", "start_char_idx": 2, "end_char_idx": 2233, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9169c54-efc4-490a-b76d-8c183d465ca3": {"__data__": {"id_": "c9169c54-efc4-490a-b76d-8c183d465ca3", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "093f2e9d-6aab-4dd1-b135-86b5d19a2cc4", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "e3f8edcedddffeae11c5a79830a1037494f41911f1dce68a7c9eaad111786463", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf83cc77-2620-4ce1-8f92-d71f9b153818", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "8486b85898ddd1415d31b642f02d6038aa37612dd84aedf3ea4a5e700b1fd78e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bebc0cc4-07ad-43c9-81f9-38e4cc77c141", "node_type": "1", "metadata": {}, "hash": "868f0a6371ade83718656beb0e9b6ec4737a2b78e5f24fa7e3ae7c1cb7018de8", "class_name": "RelatedNodeInfo"}}, "text": "Snowflake Data Marketplace\n\nWeather Source is a leading provider of global weather and climate data and\ntheir OnPoint Product Suite provides businesses with the necessary weather and\nclimate data to quickly generate meaningful and actionable insights for a wide\nrange of use cases across industries. Let's connect to the `Weather Source\nLLC: frostbyte` feed from Weather Source in the Snowflake Data Marketplace by\nfollowing these steps:\n\n  * Login to Snowsight\n  * Click on the `Marketplace` link in the left navigation bar\n  * Enter \"Weather Source LLC: frostbyte\" in the search box and click return\n  * Click on the \"Weather Source LLC: frostbyte\" listing tile\n  * Click the blue \"Get\" button \n    * Expand the \"Options\" dialog\n    * Change the \"Database name\" to read \"FROSTBYTE_WEATHERSOURCE\" (all capital letters)\n    * Select the \"HOL_ROLE\" role to have access to the new database\n  * Click on the blue \"Get\" button\n\nThat's it... we don't have to do anything from here to keep this data updated.\nThe provider will do that for us and data sharing means we are always seeing\nwhatever they have published. How amazing is that? Just think of all the\nthings you didn't have do here to get access to an always up-to-date, third-\nparty dataset!", "start_char_idx": 2, "end_char_idx": 1246, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bebc0cc4-07ad-43c9-81f9-38e4cc77c141": {"__data__": {"id_": "bebc0cc4-07ad-43c9-81f9-38e4cc77c141", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "57de6d4e-95d5-4f91-a123-e1a36afea9c1", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "3e3cab5cb51bea7e7fe58e0caf4013cd0f7ac5ce128fb970b3e5d41ba9473811", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9169c54-efc4-490a-b76d-8c183d465ca3", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "fb9b726780df419c5beb793f6c08eb83f76ffdffafa1b3be8a35dafecd637cc8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2f6183c-aade-475d-84ca-6e4ed9062030", "node_type": "1", "metadata": {}, "hash": "af4b2e9d7ed55677e997be11d8abbe8aedd1a7ba396df0396204716e76dfbd5b", "class_name": "RelatedNodeInfo"}}, "text": "Run the Script\n\nOpen the `steps/03_load_weather.sql` script in VS Code from the file Explorer\nin the left navigation bar, and run the script. Notice how easy it is to query\ndata shared through the Snowflake Marketplace! You access it just like any\nother table or view in Snowflake:\n\n    \n    \n    SELECT * FROM FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.POSTAL_CODES LIMIT 100;\n    \n\nDuring this step we will be creating a view to simplify the raw POS schema by\njoining together 6 different tables and picking only the columns we need. But\nwhat's really cool is that we're going to define that view with the Snowpark\nDataFrame API! Then we're going to create a Snowflake stream on that view so\nthat we can incrementally process changes to any of the POS tables. To put\nthis in context, we are on step **#4** in our data flow overview:\n\n!", "start_char_idx": 2, "end_char_idx": 831, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2f6183c-aade-475d-84ca-6e4ed9062030": {"__data__": {"id_": "b2f6183c-aade-475d-84ca-6e4ed9062030", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e5df34b-2a6c-41d3-8c54-564d339ae9a4", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "ae4ab154fc3c9cfdfbf50edef0f16e1d0182005e32d2d60d0fe1376015152261", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bebc0cc4-07ad-43c9-81f9-38e4cc77c141", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "8acc500f0f1b8828e3adf6f6163ebfb66a032dd21f3bb6c8ee7419b012cf6c6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c196c65-be91-486c-8da0-8823d1084831", "node_type": "1", "metadata": {}, "hash": "e74112a4ff2fbce8d9c01eff9160c9bd5663f406100d88103533e6be31c63b83", "class_name": "RelatedNodeInfo"}}, "text": "Run the Script\n\nTo create the view and stream, execute the `steps/04_create_pos_view.py`\nscript. Like we did in step 2, let's execute it from the terminal. So go back\nto the terminal in VS Code, make sure that your `snowflake-demo` conda\nenvironment is active, then run the following commands (which assume that your\nterminal has the root of your repository open):\n\n    \n    \n    cd steps\n    python 04_create_pos_view.py\n    \n\nWhile that is running, please open the script in VS Code and continue on this\npage to understand what is happening.", "start_char_idx": 2, "end_char_idx": 545, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c196c65-be91-486c-8da0-8823d1084831": {"__data__": {"id_": "9c196c65-be91-486c-8da0-8823d1084831", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f115bb9b-2b87-4288-953c-f20e45ff9fc1", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "e2b56e0d9759e4588c2506fddd2b94b5d5073169e72dbec9ba112744190879cb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2f6183c-aade-475d-84ca-6e4ed9062030", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "d8c381d0a489d4e00ff4f19724d46cd6bafc55f8d24382751b95e92c08630b29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0238ce9-0b10-4082-a853-8f63b1883232", "node_type": "1", "metadata": {}, "hash": "628d1033055cc5be899666a9bd11ba6d1a588ca43bf7873bd42cb602989f2ccd", "class_name": "RelatedNodeInfo"}}, "text": "Snowpark DataFrame API\n\nThe first thing you'll notice in the `create_pos_view()` function is that we\ndefine the Snowflake view using the Snowpark DataFrame API. After defining the\nfinal DataFrame, which captures all the logic we want in the view, we can\nsimply call the Snowpark `create_or_replace_view()` method. Here's the final\nline from the `create_pos_view()` function:\n\n    \n    \n        final_df.create_or_replace_view('POS_FLATTENED_V')\n    \n\nFor more details about the Snowpark Python DataFrame API, please check out our\n[Working with DataFrames in Snowpark\nPython](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-\nwith-dataframes.html) page.", "start_char_idx": 2, "end_char_idx": 675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0238ce9-0b10-4082-a853-8f63b1883232": {"__data__": {"id_": "b0238ce9-0b10-4082-a853-8f63b1883232", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "347315ec-253d-4b49-b83c-b32e27a95544", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "16df6f04ffd50a3c7a7b56c09dde54de1b2a58b61c81319365a8eaf92308aade", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c196c65-be91-486c-8da0-8823d1084831", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "317a2ebfad9a164a74a83acf62bea06a2c196f6cf0c4ccc4d0756c5ef1d6d904", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "418bc452-5f15-43c3-bf53-a23d59901b16", "node_type": "1", "metadata": {}, "hash": "e4ef72939ed6596f409cf2ac772c4832388b76f7295171923ec306e125942875", "class_name": "RelatedNodeInfo"}}, "text": "Streams for Incremental Processing (CDC)\n\nSnowflake makes processing data incrementally very easy. Traditionally the\ndata engineer had to keep track of a high watermark (usually a datetime\ncolumn) in order to process only new records in a table. This involved\ntracking and persisting that watermark somewhere and then using it in any\nquery against the source table. But with Snowflake streams all the heavy\nlifting is done for you by Snowflake. For more details please check out our\n[Change Tracking Using Table Streams](https://docs.snowflake.com/en/user-\nguide/streams.html) user guide.\n\nAll you need to do is create a [`STREAM`](https://docs.snowflake.com/en/sql-\nreference/sql/create-stream.html) object in Snowflake against your base table\nor view, then query that stream just like any table in Snowflake. The stream\nwill return only the changed records since the last DML option your performed.\nTo help you work with the changed records, Snowflake streams will supply the\nfollowing metadata columns along with the base table or view columns:\n\n  * METADATA$ACTION\n  * METADATA$ISUPDATE\n  * METADATA$ROW_ID\n\nFor more details about these stream metadata columns please check out the\n[Stream Columns](https://docs.snowflake.com/en/user-guide/streams-\nintro.html#stream-columns) section in our documentation.", "start_char_idx": 2, "end_char_idx": 1311, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "418bc452-5f15-43c3-bf53-a23d59901b16": {"__data__": {"id_": "418bc452-5f15-43c3-bf53-a23d59901b16", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bf9bfd70-d6f0-4f68-900b-50276268e19a", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "7b652a3e364d71b65855adaf32dae068820953c1245e080103717ebea6db4e27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0238ce9-0b10-4082-a853-8f63b1883232", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "c11bbf02ea924790e14e5108f9dc7aad9dc2f2161cf7097814ee316ebea08902", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da535acd-26c7-42ed-8e68-3e0e24c645f9", "node_type": "1", "metadata": {}, "hash": "777c6238f58e6520fd2c6e65442961bf445b5baaf4d387d84bb319cb857df199", "class_name": "RelatedNodeInfo"}}, "text": "Streams on views\n\nWhat's really cool about Snowflake's incremental/CDC stream capability is the\nability to create a stream on a view! In this example we are creating a stream\non a view which joins together 6 of the raw POS tables. Here is the code to do\nthat:\n\n    \n    \n    def create_pos_view_stream(session):\n        session.use_schema('HARMONIZED')\n        _ = session.sql('CREATE OR REPLACE STREAM POS_FLATTENED_V_STREAM \\\n                            ON VIEW POS_FLATTENED_V \\\n                            SHOW_INITIAL_ROWS = TRUE').collect()\n    \n\nNow when we query the `POS_FLATTENED_V_STREAM` stream to find changed records,\nSnowflake is actually looking for changed records in any of the 6 tables\nincluded in the view. For those who have tried to build incremental/CDC\nprocesses around denormalized schemas like this, you will appreciate the\nincredibly powerful feature that Snowflake provides here.\n\nFor more details please check out the [Streams on\nViews](https://docs.snowflake.com/en/user-guide/streams-intro.html#streams-on-\nviews) section in our documentation.\n\nDuring this step we will be creating and deploying our first Snowpark Python\nobject to Snowflake, a user-defined function (or UDF). To begin with the UDF\nwill be very basic, but in a future step we'll update it to include a third-\nparty Python package. Also in this step you will be introduced to the new\nSnowCLI, a new developer command line tool. SnowCLI makes building and\ndeploying Snowpark Python objects to Snowflake a consistent experience for the\ndeveloper. More details below on SnowCLI. To put this in context, we are on\nstep **#5** in our data flow overview:\n\n!", "start_char_idx": 2, "end_char_idx": 1650, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da535acd-26c7-42ed-8e68-3e0e24c645f9": {"__data__": {"id_": "da535acd-26c7-42ed-8e68-3e0e24c645f9", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f563e7ed-2918-4695-81fd-08cf02679a66", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "1bf67a86b46c604ffff10564ecd714bf883ccf396e0c6168008e491fa60dc871", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "418bc452-5f15-43c3-bf53-a23d59901b16", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "bcc21ef42b0e5e905c1a8cc65be857f2d9b28f09c0ee74b9bd190e8b7a0a5216", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84686a4b-f84e-4fcd-84b8-af52047746a7", "node_type": "1", "metadata": {}, "hash": "1a9b424d2f103173c125d893285492b27cf51fd5d00e13dba1a6934043616b6b", "class_name": "RelatedNodeInfo"}}, "text": "Running the UDF Locally\n\nTo test the UDF locally, you will execute the\n`steps/05_fahrenheit_to_celsius_udf/app.py` script. Like we did in the\nprevious steps, we'll execute it from the terminal. So go back to the terminal\nin VS Code, make sure that your `snowflake-demo` conda environment is active,\nthen run the following commands (which assume that your terminal has the root\nof your repository open):\n\n    \n    \n    cd steps/05_fahrenheit_to_celsius_udf\n    python app.py 35\n    \n\nWhile you're developing the UDF you can simply run it locally in VS Code. And\nif your UDF doesn't need to query data from Snowflake, this process will be\nentirely local.", "start_char_idx": 2, "end_char_idx": 654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84686a4b-f84e-4fcd-84b8-af52047746a7": {"__data__": {"id_": "84686a4b-f84e-4fcd-84b8-af52047746a7", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e95ef82a-dd67-42ab-b785-0b8eb04d22fb", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "5eb3aa1bc9c7fa1959a058f5b1556956d040f31042c94e5211650068e7e8147f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da535acd-26c7-42ed-8e68-3e0e24c645f9", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "a8e98e072ef620348dd1e22da84879e03dcea763e94d20cb7870493dcda6453e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "664c805b-40fc-41d5-92e9-a94a3755e319", "node_type": "1", "metadata": {}, "hash": "a35e3b2a256f13d1ac11432624d557065f965b79017ed6adccc9292033fca05e", "class_name": "RelatedNodeInfo"}}, "text": "Deploying the UDF to Snowflake\n\nTo deploy your UDF to Snowflake we will use the SnowCLI tool. The SnowCLI tool\nwill do all the heavy lifting of packaging up your application, copying it to\na Snowflake stage, and creating the object in Snowflake. Like we did in the\nprevious steps, we'll execute it from the terminal. So go back to the terminal\nin VS Code, make sure that your `snowflake-demo` conda environment is active,\nthen run the following commands (which assume that your terminal has the root\nof your repository open):\n\n    \n    \n    cd steps/05_fahrenheit_to_celsius_udf\n    snow function create\n    \n\nWhile that is running, please open the script in VS Code and continue on this\npage to understand what is happening.", "start_char_idx": 2, "end_char_idx": 727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "664c805b-40fc-41d5-92e9-a94a3755e319": {"__data__": {"id_": "664c805b-40fc-41d5-92e9-a94a3755e319", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f1dcfb1e-6fd0-435c-b103-87c877d79d18", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "4cd03a233c176d7172df1f83df5e0aed29506f6889678856d157ca430ee7a1a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84686a4b-f84e-4fcd-84b8-af52047746a7", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "973f5eaae6cfebb24757d5c86750a926d2ab1fdfc5108695201cfe8261846390", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2bfc3a6c-ddcf-4b53-b1ea-7c500b1fa03e", "node_type": "1", "metadata": {}, "hash": "41e1060e8b5d50279d3b9a241f9b65569989c837428443656c684745ba352b7d", "class_name": "RelatedNodeInfo"}}, "text": "Running the UDF in Snowflake\n\nIn order to run the UDF in Snowflake you have a few options. Any UDF in\nSnowflake can be invoked through SQL as follows:\n\n    \n    \n    SELECT ANALYTICS.FAHRENHEIT_TO_CELSIUS_UDF(35);\n    \n\nAnd with the SnowCLI utility you can also invoke the UDF from the terminal in\nVS Code as follows:\n\n    \n    \n    snow function execute -f \"fahrenheit_to_celsius_udf(35)\"\n    \n\nThat will result in the SnowCLI tool generating the SQL query above and\nrunning it against your Snowflake account.", "start_char_idx": 2, "end_char_idx": 512, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2bfc3a6c-ddcf-4b53-b1ea-7c500b1fa03e": {"__data__": {"id_": "2bfc3a6c-ddcf-4b53-b1ea-7c500b1fa03e", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "30fcf9a8-04b9-4774-a290-840e5169a5e0", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "d375790a500952701ada733b7369c3495fa9fb192b4ed841fb3c280add34aa29", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "664c805b-40fc-41d5-92e9-a94a3755e319", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "a3e3e2678f26c5725f4823e16a457c8d6bf1b11f2c8faf40551444f373eeeee4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f68991ce-455e-4d5d-9f9f-7863d96ca8a7", "node_type": "1", "metadata": {}, "hash": "476cb00647bf08281fbc3e4eece271f0607292736809d3249575c6263d8d5587", "class_name": "RelatedNodeInfo"}}, "text": "Overview of the SnowCLI Tool\n\nThe SnowCLI tool is a command\nline tool for developers, and is executed as `snow` from the command line.\n\n**Note** \\- Do not confuse this with the\nSnowSQL command line\ntool which is a client for connecting to Snowflake to execute SQL queries and\nperform all DDL and DML operations, and is executed as `snowsql` from the\ncommand line.\n\nSnowCLI simplifies the development and deployment of the following Snowflake\nobjects:\n\n  * Snowpark Python UDFs\n  * Snowpark Python Stored Procedures\n  * Streamlit Applications\n\nFor this Quickstart we will be focused on the first two. And for Snowpark\nPython UDFs and sprocs in particular, the SnowCLI does all the heavy lifting\nof deploying the objects to Snowflake. Here's a brief summary of the steps the\nSnowCLI deploy command does for you:\n\n  * Dealing with third-party packages \n    * For packages that can be accessed directly from our Anaconda channel it will add them to the `PACKAGES` list in the `CREATE PROCEDURE` or `CREATE FUNCTION` SQL command\n    * For packages which are not currently available in our Anaconda channel it will download the code and include them in the project zip file\n  * Creating a zip file of everything in your project\n  * Copying that project zip file to your Snowflake stage\n  * Creating the Snowflake function or stored procedure object\n\nThis also allows you to develop and test your Python application without\nhaving to worry about wrapping it in a corresponding Snowflake database\nobject.\n\n**Note** \\- As of 9/1/2023, the [SnowCLI Tool](https://github.com/Snowflake-\nLabs/snowcli) is still in preview.", "start_char_idx": 2, "end_char_idx": 1611, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f68991ce-455e-4d5d-9f9f-7863d96ca8a7": {"__data__": {"id_": "f68991ce-455e-4d5d-9f9f-7863d96ca8a7", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd6d4c43-2e78-4a77-a30c-e1101d7f267c", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "1db9b4d03128faf27cda357c633546a7305a92e09b98206c00e1fa02ba2f99c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2bfc3a6c-ddcf-4b53-b1ea-7c500b1fa03e", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "0823f1e041ab7e9e944c6290cd2e5377bc37defd1575c05c9d1355cb36a2b6d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "932b775e-8193-4648-b194-7a77e3f90baa", "node_type": "1", "metadata": {}, "hash": "b291d3d35d08808ab442b09efcba88a25ed585f5f7cbc4798d68e18751bd2aa9", "class_name": "RelatedNodeInfo"}}, "text": "More on Snowpark Python UDFs\n\nIn this step we deployed a very simple Python UDF to Snowflake. In a future\nstep will update it to use a third-party package. And because we deployed it\nto Snowflake with the SnowCLI command you didn't have to worry about the SQL\nDDL Syntax to create the object in Snowflake. But for reference please check\nout our [Writing Python UDFs](https://docs.snowflake.com/en/developer-\nguide/udf/python/udf-python.html) developer guide.\n\nHere is the SQL query that the SnowCLI tool generated to deploy the function:\n\n    \n    \n    CREATE OR REPLACE  FUNCTION fahrenheit_to_celsius_udf(temp_f float)\n             RETURNS float\n             LANGUAGE PYTHON\n             RUNTIME_VERSION=3.8\n             IMPORTS=('@HOL_DB.ANALYTICS.deployments/fahrenheit_to_celsius_udftemp_f_float/app.zip')\n             HANDLER='app.main'\n             PACKAGES=();\n    \n\nDuring this step we will be creating and deploying our first Snowpark Python\nstored procedure (or sproc) to Snowflake. This sproc will merge changes from\nthe `HARMONIZED.POS_FLATTENED_V_STREAM` stream into our target\n`HARMONIZED.ORDERS` table. To put this in context, we are on step **#6** in\nour data flow overview:\n\n!", "start_char_idx": 2, "end_char_idx": 1196, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "932b775e-8193-4648-b194-7a77e3f90baa": {"__data__": {"id_": "932b775e-8193-4648-b194-7a77e3f90baa", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5658da6d-6827-4cfd-8f88-86b2dc87a1e2", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "3a3812a31961f707731b3ff7cd8ff905c93142a40fa80e93c8b2dd1bd38749d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f68991ce-455e-4d5d-9f9f-7863d96ca8a7", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "add1bc3ca771a8956a72658e88793ca61d4d31c318a3b903a40d9843df6cecfe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e210a659-6bd1-4edf-ac06-7c9045bf2575", "node_type": "1", "metadata": {}, "hash": "1eeb010fbe0d6449bb7602b1fa9a8bb0e5bdd78bce09eb0af0ab47a627753e7c", "class_name": "RelatedNodeInfo"}}, "text": "Running the Sproc Locally\n\nTo test the procedure locally, you will execute the\n`steps/06_orders_update_sp/app.py` script. Like we did in the previous steps,\nwe'll execute it from the terminal. So go back to the terminal in VS Code,\nmake sure that your `snowflake-demo` conda environment is active, then run the\nfollowing commands (which assume that your terminal has the root of your\nrepository open):\n\n    \n    \n    cd steps/06_orders_update_sp\n    python app.py\n    \n\nWhile you're developing the sproc you can simply run it locally in VS Code.\nThe Python code will run locally on your laptop, but the Snowpark DataFrame\ncode will issue SQL queries to your Snowflake account.", "start_char_idx": 2, "end_char_idx": 678, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e210a659-6bd1-4edf-ac06-7c9045bf2575": {"__data__": {"id_": "e210a659-6bd1-4edf-ac06-7c9045bf2575", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "183844fd-a2cb-42a9-836b-13a5cd5bb9b9", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "ba4da67ec1322b2b6deae128e402274c2c112e1b971ef845068b454098f198d8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "932b775e-8193-4648-b194-7a77e3f90baa", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "e36aecdf73a36197ae73366e936804fe3fa9148452fa7c1f5e46ad12bd0bc13f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59e0119e-dba2-42e5-86b2-9abef45b4ff8", "node_type": "1", "metadata": {}, "hash": "222aa948f909ebd10c176cf5cda1f08d05bfa6b4cf0d02af643c98676113bd70", "class_name": "RelatedNodeInfo"}}, "text": "Deploying the Sproc to Snowflake\n\nTo deploy your sproc to Snowflake we will use the SnowCLI tool. Like we did in\nthe previous steps, we'll execute it from the terminal. So go back to the\nterminal in VS Code, make sure that your `snowflake-demo` conda environment is\nactive, then run the following commands (which assume that your terminal has\nthe root of your repository open):\n\n    \n    \n    cd steps/06_orders_update_sp\n    snow procedure create\n    \n\nWhile that is running, please open the script in VS Code and continue on this\npage to understand what is happening.", "start_char_idx": 2, "end_char_idx": 571, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59e0119e-dba2-42e5-86b2-9abef45b4ff8": {"__data__": {"id_": "59e0119e-dba2-42e5-86b2-9abef45b4ff8", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e71769f-c97b-465c-ace2-833cc64f6b4f", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "e4d2171c66bf6015c0151c2d5da94ee879be9c310ffbb89611edf4cfe67e9779", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e210a659-6bd1-4edf-ac06-7c9045bf2575", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "757004411020cb5802ee145df1b7d217005ae774b9ea831b8b3e503eb0ae4a3e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f0b889f-8a6b-4302-bea8-29b61576b350", "node_type": "1", "metadata": {}, "hash": "85d3cd2c4cc6194f09e7b4101b3cc48f361ac47e5713cd11915c41ce9f7afa7f", "class_name": "RelatedNodeInfo"}}, "text": "Running the Sproc in Snowflake\n\nIn order to run the sproc in Snowflake you have a few options. Any sproc in\nSnowflake can be invoked through SQL as follows:\n\n    \n    \n    CALL ORDERS_UPDATE_SP();\n    \n\nAnd with the SnowCLI utility you can also invoke the UDF from the terminal in\nVS Code as follows:\n\n    \n    \n    snow procedure execute -p \"orders_update_sp()\"\n    \n\nThat will result in the SnowCLI tool generating the SQL query above and\nrunning it against your Snowflake account.", "start_char_idx": 2, "end_char_idx": 485, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f0b889f-8a6b-4302-bea8-29b61576b350": {"__data__": {"id_": "7f0b889f-8a6b-4302-bea8-29b61576b350", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f73f02a0-ba7d-4d26-9361-4c039781b924", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "123fdfe8b71221c0b127625cbf5519dc0eef8aba4cf63aaa551f12369a6ad99b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59e0119e-dba2-42e5-86b2-9abef45b4ff8", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "84da9a1944c796e35c3721f59bea17639e7cd22187ff41e831a3e49eb7f2494b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e10b80ab-d049-4e06-8391-33eb010089d9", "node_type": "1", "metadata": {}, "hash": "0fd9a9d89539845579f574ef288a02afa512d9c31d5d5cf932534b677b56fb04", "class_name": "RelatedNodeInfo"}}, "text": "More on Snowpark Python Sprocs\n\nIn this step we deployed a Python sproc to Snowflake. And because we deployed\nit to Snowflake with the SnowCLI command you didn't have to worry about the\nSQL DDL Syntax to create the object in Snowflake. But for reference please\ncheck out our [Writing Stored Procedures in Snowpark\n(Python)](https://docs.snowflake.com/en/sql-reference/stored-procedures-\npython.html) guide.\n\nHere is the SQL query that the SnowCLI tool generated to deploy the procedure:\n\n    \n    \n    CREATE OR REPLACE  PROCEDURE orders_update_sp()\n             RETURNS string\n             LANGUAGE PYTHON\n             RUNTIME_VERSION=3.8\n             IMPORTS=('@HOL_DB.HARMONIZED.deployments/orders_update_sp/app.zip')\n             HANDLER='app.main'\n             PACKAGES=('snowflake-snowpark-python','toml')\n            EXECUTE AS CALLER;", "start_char_idx": 2, "end_char_idx": 844, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e10b80ab-d049-4e06-8391-33eb010089d9": {"__data__": {"id_": "e10b80ab-d049-4e06-8391-33eb010089d9", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2ee8b75-fd02-4176-8760-848b15e9d6a9", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "03a6c419d910383b05bcd7c6125adb8e21da5bbb3c6a671f75668da5aa58c721", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f0b889f-8a6b-4302-bea8-29b61576b350", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "3e8470f9cee5491dcfc9fac88cae9d3e42cee04dd8b2a399fca7e49e6a8d1f45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd6703cd-c79b-4e2c-b870-1da40ebd0fb5", "node_type": "1", "metadata": {}, "hash": "52cf344a156db074987611ca6312a9f1e4d21b8cc30fe192171f14c761dd0641", "class_name": "RelatedNodeInfo"}}, "text": "More on the Snowpark API\n\nIn this step we're starting to really use the Snowpark DataFrame API for data\ntransformations. The Snowpark API provides the same functionality as the\n[Spark SQL\nAPI](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html).\nTo begin with you need to create a Snowpark session object. Like PySpark, this\nis accomplished with the `Session.builder.configs().create()` methods. When\nrunning locally, we use the `utils.snowpark_utils.get_snowpark_session()`\nhelper function to create the session object for us. But when deployed to\nSnowflake, the session object is provisioned for you automatically by\nSnowflake. And when building a Snowpark Python sproc the contract is that the\nfirst argument to the entry point (or handler) function is a Snowpark session.\n\nThe first thing you'll notice in the `steps/06_orders_update_sp/app.py` script\nis that we have some functions which use SQL to create objects in Snowflake\nand to check object status. To issue a SQL statement to Snowflake with the\nSnowpark API you use the `session.sql()` function, like you'd expect. Here's\none example:\n\n    \n    \n    def create_orders_stream(session):\n        _ = session.sql(\"CREATE STREAM IF NOT EXISTS HARMONIZED.ORDERS_STREAM ON TABLE HARMONIZED.ORDERS \\\n                        SHOW_INITIAL_ROWS = TRUE;\").collect()\n    \n\nThe second thing to point out is how we're using DataFrames to merge changes\nfrom the source view to the target table. The Snowpark DataFrame API provides\na `merge()` method which will ultimately generate a `MERGE` command in\nSnowflake.\n\n    \n    \n        source = session.table('HARMONIZED.POS_FLATTENED_V_STREAM')\n        target = session.table('HARMONIZED.ORDERS')\n    \n        # TODO: Is the if clause supposed to be based on \"META_UPDATED_AT\"?\n        cols_to_update = {c: source[c] for c in source.schema.names if \"METADATA\" not in c}\n        metadata_col_to_update = {\"META_UPDATED_AT\": F.current_timestamp()}\n        updates = {**cols_to_update, **metadata_col_to_update}\n    \n        # merge into DIM_CUSTOMER\n        target.merge(source, target['ORDER_DETAIL_ID'] == source['ORDER_DETAIL_ID'], \\\n                            [F.when_matched().update(updates), F.when_not_matched().insert(updates)])\n    \n\nAgain, for more details about the Snowpark Python DataFrame API, please check\nout our [Working with DataFrames in Snowpark\nPython](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-\nwith-dataframes.html) page.\n\nDuring this step we will be creating and deploying our second Snowpark Python\nsproc to Snowflake. This sproc will join the `HARMONIZED.ORDERS` data with the\nWeather Source data to create a final, aggregated table for analysis named\n`ANALYTICS.DAILY_CITY_METRICS`. We will process the data incrementally from\nthe `HARMONIZED.ORDERS` table using another Snowflake Stream. And we will\nagain use the Snowpark DataFrame `merge()` method to merge/upsert the data. To\nput this in context, we are on step **#7** in our data flow overview:\n\n!", "start_char_idx": 2, "end_char_idx": 3032, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd6703cd-c79b-4e2c-b870-1da40ebd0fb5": {"__data__": {"id_": "cd6703cd-c79b-4e2c-b870-1da40ebd0fb5", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e85fb5d8-87c2-42c7-8559-0daf40607a34", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "3d25ac200555e0aeb708ecaf237d2119e7c263a0f904ca38b3a604b008261688", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e10b80ab-d049-4e06-8391-33eb010089d9", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "a95e1c181b962a217452139b9ce1ca9abb2412eae7edcb1162ea105e2acf67f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63192a43-5a8e-460e-8039-adcddca6b484", "node_type": "1", "metadata": {}, "hash": "f98a3bc4937b4bd62b1707e5ba780ca7873cbfaedf968d84e3de337b794699b1", "class_name": "RelatedNodeInfo"}}, "text": "Running the Sproc Locally\n\nTo test the procedure locally, you will execute the\n`steps/07_daily_city_metrics_update_sp/app.py` script. Like we did in the\nprevious steps, we'll execute it from the terminal. So go back to the terminal\nin VS Code, make sure that your `snowflake-demo` conda environment is active,\nthen run the following commands (which assume that your terminal has the root\nof your repository open):\n\n    \n    \n    cd steps/07_daily_city_metrics_update_sp\n    python app.py\n    \n\nWhile you're developing the sproc you can simply run it locally in VS Code.\nThe Python code will run locally on your laptop, but the Snowpark DataFrame\ncode will issue SQL queries to your Snowflake account.", "start_char_idx": 2, "end_char_idx": 702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63192a43-5a8e-460e-8039-adcddca6b484": {"__data__": {"id_": "63192a43-5a8e-460e-8039-adcddca6b484", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "10e6c215-3511-4944-8ff1-dfbc9ee37dab", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "44ba2a7bb6a15615572415237716324b9fa9873a7b6e28789904e19610109bf2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd6703cd-c79b-4e2c-b870-1da40ebd0fb5", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "18a7be79a2370945b8329b722a5868bfdfeed9c10d3d33a5e9992407db20f3d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "847c1f20-1db1-4e93-968a-c158ef1ed826", "node_type": "1", "metadata": {}, "hash": "f45bbc28f85ac18e78fa1424cce028b6a695c66727d53082681d1d54bace2db1", "class_name": "RelatedNodeInfo"}}, "text": "Deploying the Sproc to Snowflake\n\nTo deploy your sproc to Snowflake we will use the SnowCLI tool. Like we did in\nthe previous steps, we'll execute it from the terminal. So go back to the\nterminal in VS Code, make sure that your `snowflake-demo` conda environment is\nactive, then run the following commands (which assume that your terminal has\nthe root of your repository open):\n\n    \n    \n    cd steps/07_daily_city_metrics_update_sp\n    snow procedure create\n    \n\nWhile that is running, please open the script in VS Code and continue on this\npage to understand what is happening.", "start_char_idx": 2, "end_char_idx": 583, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "847c1f20-1db1-4e93-968a-c158ef1ed826": {"__data__": {"id_": "847c1f20-1db1-4e93-968a-c158ef1ed826", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd2a6354-857b-4f29-b213-010226709082", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "86f87d0f51bfa2f84f1b2625e72f8a16941277f9d843ace2fbbfe047e72d09bd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63192a43-5a8e-460e-8039-adcddca6b484", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "345a458f6b260ff75ecf94998c385378a34eeefb6261baee0493ca97e21c8a64", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c1e93f8-ccb6-42bf-b74d-f6fcccc62497", "node_type": "1", "metadata": {}, "hash": "6d885cdfed22832da2c0892e25ece34dea8db9a2d574139f1826fd0ac9fe0e6f", "class_name": "RelatedNodeInfo"}}, "text": "Running the Sproc in Snowflake\n\nIn order to run the sproc in Snowflake you have a few options. Any sproc in\nSnowflake can be invoked through SQL as follows:\n\n    \n    \n    CALL DAILY_CITY_METRICS_UPDATE_SP();\n    \n\nAnd with the SnowCLI utility you can also invoke the UDF from the terminal in\nVS Code as follows:\n\n    \n    \n    snow procedure execute -p \"daily_city_metrics_update_sp()\"\n    \n\nThat will result in the SnowCLI tool generating the SQL query above and\nrunning it against your Snowflake account.", "start_char_idx": 2, "end_char_idx": 509, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c1e93f8-ccb6-42bf-b74d-f6fcccc62497": {"__data__": {"id_": "6c1e93f8-ccb6-42bf-b74d-f6fcccc62497", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54dc5a26-f401-492a-9571-2af41969fe79", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "9836318acd6d95b0b0c9a77e4d0a1b6bec544601daa4f1e73c947552b7d132f6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "847c1f20-1db1-4e93-968a-c158ef1ed826", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "947fefe141417cd9fe829a869bcd9380392cbad68afd2823828a6afbd464be33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c210266-1fc7-486f-96c6-9a2f2f7766d6", "node_type": "1", "metadata": {}, "hash": "d5cb8c483e0866780acd0fd95295b3ed592bd0ba4cc493bd89127109d559cb5b", "class_name": "RelatedNodeInfo"}}, "text": "Data Modeling Best Practice\n\nWhen modeling data for analysis a best practice has been to clearly define and\nmanage the schema of the table. In step 2, when we loaded raw data from\nParquet we took advantage of Snowflake's schema detection feature to create a\ntable with the same schema as the Parquet files. In this step we are\nexplicitly defining the schema in DataFrame syntax and using that to create\nthe table.\n\n    \n    \n    def create_daily_city_metrics_table(session):\n        SHARED_COLUMNS= [T.StructField(\"DATE\", T.DateType()),\n                                            T.StructField(\"CITY_NAME\", T.StringType()),\n                                            T.StructField(\"COUNTRY_DESC\", T.StringType()),\n                                            T.StructField(\"DAILY_SALES\", T.StringType()),\n                                            T.StructField(\"AVG_TEMPERATURE_FAHRENHEIT\", T.DecimalType()),\n                                            T.StructField(\"AVG_TEMPERATURE_CELSIUS\", T.DecimalType()),\n                                            T.StructField(\"AVG_PRECIPITATION_INCHES\", T.DecimalType()),\n                                            T.StructField(\"AVG_PRECIPITATION_MILLIMETERS\", T.DecimalType()),\n                                            T.StructField(\"MAX_WIND_SPEED_100M_MPH\", T.DecimalType()),\n                                        ]\n        DAILY_CITY_METRICS_COLUMNS = [*SHARED_COLUMNS, T.StructField(\"META_UPDATED_AT\", T.TimestampType())]\n        DAILY_CITY_METRICS_SCHEMA = T.StructType(DAILY_CITY_METRICS_COLUMNS)\n    \n        dcm = session.create_dataframe([[None]*len(DAILY_CITY_METRICS_SCHEMA.names)], schema=DAILY_CITY_METRICS_SCHEMA) \\\n                            .na.drop() \\\n                            .write.mode('overwrite').save_as_table('ANALYTICS.DAILY_CITY_METRICS')\n        dcm = session.table('ANALYTICS.DAILY_CITY_METRICS')", "start_char_idx": 2, "end_char_idx": 1886, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c210266-1fc7-486f-96c6-9a2f2f7766d6": {"__data__": {"id_": "0c210266-1fc7-486f-96c6-9a2f2f7766d6", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "917be18f-8d2a-412b-8f05-c5a599d3343e", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "a27fb97171d3ee36bd75ff0cf1f1d7ce85a6f16213b800bcb784418b2d26a697", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c1e93f8-ccb6-42bf-b74d-f6fcccc62497", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "de15fad8e1a0074cb3aa2b40e378dd59e83ecd9e2952d5d67c138d6814d8135f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ad3e607-5b1a-48de-b12f-40160ca7db63", "node_type": "1", "metadata": {}, "hash": "177dabbb77b7a4481bd83b8d1669a22176f1b29c03a231413185233fbf5e1d45", "class_name": "RelatedNodeInfo"}}, "text": "Complex Aggregation Query\n\nThe `merge_daily_city_metrics()` function contains a complex aggregation query\nwhich is used to join together and aggregate the data from our POS and Weather\nSource. Take a look at the series of complex series of joins and aggregations\nthat are expressed, and how we're even leveraging the Snowpark UDF we created\nin step #5!\n\nThe complex aggregation query is then merged into the final analytics table\nusing the Snowpark `merge()` method. If you haven't already, check out your\nSnowflake Query history and see which queries were generated by the Snowpark\nAPI. In this case you will see that the Snowpark API took all the complex\nlogic, including the merge and created a single Snowflake query to execute!\n\nDuring this step we will be orchestrating our new Snowpark pipelines with\nSnowflake's native orchestration feature named Tasks. We will create two\ntasks, one for each stored procedure, and chain them together. We will then\nrun the tasks. To put this in context, we are on step **#8** in our data flow\noverview:\n\n!", "start_char_idx": 2, "end_char_idx": 1049, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ad3e607-5b1a-48de-b12f-40160ca7db63": {"__data__": {"id_": "0ad3e607-5b1a-48de-b12f-40160ca7db63", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fe0a61b-3435-4f36-a819-4c6bdb894523", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "3649ee019ebca95c4051c727a41df26bbe959c816b2180890709565ff13334e3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c210266-1fc7-486f-96c6-9a2f2f7766d6", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "a677c7397133f47e3dfc8913af6bb096987fabca2eab7766037eaa31c59ec98d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7114d7d5-1d90-4a0f-8fed-75e6fba75344", "node_type": "1", "metadata": {}, "hash": "9b257309ecb9198dc279b09be406c669bb1dff41608548d23a444f494864782e", "class_name": "RelatedNodeInfo"}}, "text": "Run the Script\n\nSince this is a SQL script we will be using our native VS Code extension to\nexecute it. So simply open the `steps/08_orchestrate_jobs.sql` script in VS\nCode and run the whole thing using the \"Execute All Statements\" button in the\nupper right corner of the editor window.\n\nWhile that is running, please read through the script in VS Code and continue\non this page to understand what is happening.", "start_char_idx": 2, "end_char_idx": 413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7114d7d5-1d90-4a0f-8fed-75e6fba75344": {"__data__": {"id_": "7114d7d5-1d90-4a0f-8fed-75e6fba75344", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f063f5c-be2a-49fc-a7fa-8ece9cf447b1", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "2af6cf4ca90fb87820a3996d8c02a6a4e4179b6b93733f69a4e5354c5082228d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ad3e607-5b1a-48de-b12f-40160ca7db63", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "210d8e6410c90a2f6bd1ffb643a3c190755bb3f3d5769baffb8eebb106b2fe51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12724248-74e7-454c-b0c3-42a4a83269c8", "node_type": "1", "metadata": {}, "hash": "1942dc7946eb1b98600a5cc17c9ea7a06b2c3d40840dae430043a818adbac803", "class_name": "RelatedNodeInfo"}}, "text": "Running the Tasks\n\nIn this step we did not create a schedule for our task DAG, so it will not run\non its own at this point. So in this script you will notice that we manually\nexecute the DAG, like this:\n\n    \n    \n    EXECUTE TASK ORDERS_UPDATE_TASK;\n    \n\nTo see what happened when you ran this task just now, highlight and run (using\nCMD/CTRL+Enter) this commented query in the script:\n\n    \n    \n    SELECT *\n    FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(\n        SCHEDULED_TIME_RANGE_START=>DATEADD('DAY',-1,CURRENT_TIMESTAMP()),\n        RESULT_LIMIT => 100))\n    ORDER BY SCHEDULED_TIME DESC\n    ;\n    \n\nYou will notice in the task history output that it skipped our task\n`ORDERS_UPDATE_TASK`. This is correct, because our\n`HARMONIZED.POS_FLATTENED_V_STREAM` stream doesn't have any data. We'll add\nsome new data and run them again in the next step.", "start_char_idx": 2, "end_char_idx": 857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12724248-74e7-454c-b0c3-42a4a83269c8": {"__data__": {"id_": "12724248-74e7-454c-b0c3-42a4a83269c8", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85c83823-db05-44a9-80b4-f7d5c55e1d73", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "97953ae55800b096aafbd0616d54133d45520f032ff40be34923f22f3c270740", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7114d7d5-1d90-4a0f-8fed-75e6fba75344", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "247bcd3a7cae48bfbf9f039b851b0d6321df048a562b2dfaeb0fb560c365239f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "571ecbf1-142b-4195-bd3e-e8761924cf5a", "node_type": "1", "metadata": {}, "hash": "22693a3fbb839bccc90803cf713e97682302ac38c65444d401a9b307fd1a5305", "class_name": "RelatedNodeInfo"}}, "text": "More on Tasks\n\nTasks are Snowflake's native scheduling/orchestration feature. With a task you\ncan execute any one of the following types of SQL code:\n\n  * Single SQL statement\n  * Call to a stored procedure\n  * Procedural logic using Snowflake Scripting Developer Guide\n\nFor this Quickstart we'll call our Snowpark stored procedures. Here is the SQL\nDDL code to create the second task:\n\n    \n    \n    CREATE OR REPLACE TASK DAILY_CITY_METRICS_UPDATE_TASK\n    WAREHOUSE = HOL_WH\n    AFTER ORDERS_UPDATE_TASK\n    WHEN\n      SYSTEM$STREAM_HAS_DATA('ORDERS_STREAM')\n    AS\n    CALL ANALYTICS.DAILY_CITY_METRICS_UPDATE_SP();\n    \n\nA few things to point out. First you specify which Snowflake virtual warehouse\nto use when running the task with the `WAREHOUSE` clause. The `AFTER` clause\nlets you define the relationship between tasks, and the structure of this\nrelationship is a Directed Acyclic Graph (or DAG) like most orchestration\ntools provide. The `AS` clause let's you define what the task should do when\nit runs, in this case to call our stored procedure.\n\nThe `WHEN` clause is really cool. We've already seen how streams work in\nSnowflake by allowing you to incrementally process data. We've even seen how\nyou can create a stream on a view (which joins many tables together) and\ncreate a stream on that view to process its data incrementally! Here in the\n`WHEN` clause we're calling a system function `SYSTEM$STREAM_HAS_DATA()` which\nreturns true if the specified stream has new data. With the `WHEN` clause in\nplace the virtual warehouse will only be started up when the stream has new\ndata. So if there's no new data when the task runs then your warehouse won't\nbe started up and you won't be charged. You will only be charged when there's\nnew data to process. Pretty cool, huh?\n\nAs mentioned above we did not define a `SCHEDULE` for the root task, so this\nDAG will not run on its own. That's fine for this Quickstart, but in a real\nsituation you would define a schedule. See [CREATE\nTASK](https://docs.snowflake.com/en/sql-reference/sql/create-task.html) for\nthe details.\n\nAnd for more details on Tasks see [Introduction to\nTasks](https://docs.snowflake.com/en/user-guide/tasks-intro.html).", "start_char_idx": 2, "end_char_idx": 2199, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "571ecbf1-142b-4195-bd3e-e8761924cf5a": {"__data__": {"id_": "571ecbf1-142b-4195-bd3e-e8761924cf5a", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c407ece-0af7-420a-8daf-b29a6c3b07a2", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "fc69e5a8fb9c3ba32f568b43dd82ef3549ac09d39ed320d00df6efd9649c9095", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12724248-74e7-454c-b0c3-42a4a83269c8", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "c0559a68a4f0f5c3073e11a850c19bf80f1de2cc9c412edcee2d20d7045a1e78", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3732f2dd-fad0-4aa8-920b-7187e54ae973", "node_type": "1", "metadata": {}, "hash": "87dd651c829c23d77355cee59b2715ca48cfc3570e4687b9d4c16ca6c57b6c5f", "class_name": "RelatedNodeInfo"}}, "text": "Task Metadata\n\nSnowflake keeps metadata for almost everything you do, and makes that metadata\navailable for you to query (and to create any type of process around). Tasks\nare no different, Snowflake maintains rich metadata to help you monitor your\ntask runs. Here are a few sample SQL queries you can use to monitor your tasks\nruns:\n\n    \n    \n    -- Get a list of tasks\n    SHOW TASKS;\n    \n    -- Task execution history in the past day\n    SELECT *\n    FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(\n        SCHEDULED_TIME_RANGE_START=>DATEADD('DAY',-1,CURRENT_TIMESTAMP()),\n        RESULT_LIMIT => 100))\n    ORDER BY SCHEDULED_TIME DESC\n    ;\n    \n    -- Scheduled task runs\n    SELECT\n        TIMESTAMPDIFF(SECOND, CURRENT_TIMESTAMP, SCHEDULED_TIME) NEXT_RUN,\n        SCHEDULED_TIME,\n        NAME,\n        STATE\n    FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY())\n    WHERE STATE = 'SCHEDULED'\n    ORDER BY COMPLETED_TIME DESC;", "start_char_idx": 2, "end_char_idx": 928, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3732f2dd-fad0-4aa8-920b-7187e54ae973": {"__data__": {"id_": "3732f2dd-fad0-4aa8-920b-7187e54ae973", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e18eba2-7b31-4f5e-8194-f3c49b04577a", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "c1febe611189b53a4b3c8c9bb8db736f183d707cb758207bac1934bf8124ee53", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "571ecbf1-142b-4195-bd3e-e8761924cf5a", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "83fbc59a81b8644174ca91593bc30352c568b69f76e0a538b7241eec218805e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68fd2dfe-e540-4626-8e34-4627fd6883aa", "node_type": "1", "metadata": {}, "hash": "7048e7ca423162507aa4d8daf822b0238a6ffdc1f14b124c820530bec79d9b25", "class_name": "RelatedNodeInfo"}}, "text": "Monitoring Tasks\n\nSo while you're free to create any operational or monitoring process you wish,\nSnowflake provides some rich task observability features in our Snowsight UI.\nTry it out for yourself by following these steps:\n\n  1. In the Snowsight navigation menu, click **Data** \u00bb **Databases**.\n  2. In the right pane, using the object explorer, navigate to a database and schema.\n  3. For the selected schema, select and expand **Tasks**.\n  4. Select a task. Task information is displayed, including **Task Details** , **Graph** , and **Run History** sub-tabs.\n  5. Select the **Graph** tab. The task graph appears, displaying a hierarchy of child tasks.\n  6. Select a task to view its details.\n\nHere's what the task graph looks like:\n\n!\n\nAnd here's an example of the task run history:\n\n!\n\nFor more details, and to learn about viewing account level task history,\nplease check out our [Viewing Task\nHistory](https://docs.snowflake.com/en/user-guide/ui-snowsight-tasks.html)\ndocumentation.\n\nDuring this step we will be adding new data to our POS order tables and then\nrunning our entire end-to-end pipeline to process the new data. And this\nentire pipeline will be processing data incrementally thanks to Snowflake's\nadvanced stream/CDC capabilities. To put this in context, we are on step\n**#9** in our data flow overview:\n\n!", "start_char_idx": 2, "end_char_idx": 1329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68fd2dfe-e540-4626-8e34-4627fd6883aa": {"__data__": {"id_": "68fd2dfe-e540-4626-8e34-4627fd6883aa", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c89a58e2-49ab-44bb-b5ba-de444fea94ac", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "9ef1b885a094003a211651c7fcfcef20b1d5908f29c1c095ecbed43ae1b830b9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3732f2dd-fad0-4aa8-920b-7187e54ae973", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "7d361bc00554a59cb972b653e8c09c752d257e5bcfd8ade04a40a1d351da287b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4af9bd85-b29e-4eb3-823e-eb099d22d9ef", "node_type": "1", "metadata": {}, "hash": "3d5b1b748e388ae43158c7ab7735435331b802d66874952df9b71ae53f003f60", "class_name": "RelatedNodeInfo"}}, "text": "Run the Script\n\nSince this is a SQL script we will be using our native VS Code extension to\nexecute it. So simply open the `steps/09_process_incrementally.sql` script in\nVS Code and run the whole thing using the \"Execute All Statements\" button in\nthe upper right corner of the editor window.\n\nWhile that is running, let's briefly discuss what's happening. As in step #2,\nwe're going to load data from Parquet into our raw POS tables. In step #2 we\nloaded all the order data except for the 2022 data for `ORDER_HEADER` and\n`ORDER_DETAIL`. So now we're going to load the remaining data.\n\nThis time we will be doing the data loading through SQL instead of Python, but\nthe process is the same. We'll resize the warehouse, scaling up so that we can\nload the data faster and then scaling back down after when we're done. After\nthe new data is loaded we will also run the task DAG again. And this time both\ntasks will run and process the new data.", "start_char_idx": 2, "end_char_idx": 942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4af9bd85-b29e-4eb3-823e-eb099d22d9ef": {"__data__": {"id_": "4af9bd85-b29e-4eb3-823e-eb099d22d9ef", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f6ef32e-8282-4782-b688-a5584d519c41", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "2b4e2093da4b2b8db55a63c45031f59c8677225a5c95c8c3cacf05d66344f540", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68fd2dfe-e540-4626-8e34-4627fd6883aa", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "4fb90e8641aeb33787cb7e9dfdbb16cb0b4f042fb7691748eebcc517c7843a3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7abbc120-615b-4839-baa3-cfa9d9d1abae", "node_type": "1", "metadata": {}, "hash": "35c5b83d8edae8f7e0f8e0c76ec7cafc638082e7bdde28787ac5d1b304460cf4", "class_name": "RelatedNodeInfo"}}, "text": "Viewing the Task History\n\nLike the in the previous step, to see what happened when you ran this task\nDAG, highlight and run (using CMD/CTRL+Enter) this commented query in the\nscript:\n\n    \n    \n    SELECT *\n    FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(\n        SCHEDULED_TIME_RANGE_START=>DATEADD('DAY',-1,CURRENT_TIMESTAMP()),\n        RESULT_LIMIT => 100))\n    ORDER BY SCHEDULED_TIME DESC\n    ;\n    \n\nThis time you will notice that the `ORDERS_UPDATE_TASK` task will not be\nskipped, since the `HARMONIZED.POS_FLATTENED_V_STREAM` stream has new data. In\na few minutes you should see that both the `ORDERS_UPDATE_TASK` task and the\n`DAILY_CITY_METRICS_UPDATE_TASK` task completed successfully.", "start_char_idx": 2, "end_char_idx": 697, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7abbc120-615b-4839-baa3-cfa9d9d1abae": {"__data__": {"id_": "7abbc120-615b-4839-baa3-cfa9d9d1abae", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "019fed00-da53-4202-8a61-228b8cc8f1d5", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "044862e8bb39cceabe34e39d57f2d8da8a2c41a4e452d537bce084e4cde8904c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4af9bd85-b29e-4eb3-823e-eb099d22d9ef", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "59b94be7606d91ee5e75a099420829a35c689d4e8c748786f6912ba3cc781dbd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7f22a94-b0ac-44d0-b0ad-c851eba9adb6", "node_type": "1", "metadata": {}, "hash": "8531272144e7a39d211b1a05356aae3341dfef4e15dfbba4788843ab45a2b800", "class_name": "RelatedNodeInfo"}}, "text": "Query History for Tasks\n\nOne important thing to understand about tasks, is that the queries which get\nexecuted by the task won't show up with the default Query History UI settings.\nIn order to see the queries that just ran you need to do the following:\n\n  * Remove filters at the top of this table, including your username, as later scheduled tasks will run as \"System\":\n\n!\n\n  * Click \"Filter\", and add filter option \u2018Queries executed by user tasks' and click \"Apply Filters\":\n\n!\n\nYou should now see all the queries run by your tasks! Take a look at each of\nthe MERGE commands in the Query History to see how many records were processed\nby each task. And don't forget to notice that we processed the whole pipeline\njust now, and did so incrementally!\n\nDuring this step we will be making a change to our\n`FAHRENHEIT_TO_CELSIUS_UDF()` UDF and then deploying it via a CI/CD pipeline.\nWe will be updating the `FAHRENHEIT_TO_CELSIUS_UDF()` UDF to use a third-party\nPython package, pushing it to your forked GitHub repo, and finally deploying\nit using the SnowCLI in a GitHub Actions workflow! To put this in context, we\nare on step **#10** in our data flow overview:\n\n!", "start_char_idx": 2, "end_char_idx": 1166, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7f22a94-b0ac-44d0-b0ad-c851eba9adb6": {"__data__": {"id_": "f7f22a94-b0ac-44d0-b0ad-c851eba9adb6", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c11c2700-a60a-4ea1-b81b-78ff5298bcc5", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "9d4182606a4cf2d84313d5c776f08a94729d01a959f861d97736f53d9573f309", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7abbc120-615b-4839-baa3-cfa9d9d1abae", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "dad99f56f28031632aa5074deb250536fc190d4810e6b2e6e1a34eb28870111e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac00025c-986e-4388-aa4d-bbe113b04aad", "node_type": "1", "metadata": {}, "hash": "2a0318704a7bdbb3c4b780430b474aab196898b4b905c02e0f39635dda4f16b3", "class_name": "RelatedNodeInfo"}}, "text": "Update the Fahrenheit to Celsius UDF\n\nWe will be replacing our hard-coded temperature conversion with a package from\n`scipy`. First we will make a few changes to the\n`steps/05_fahrenheit_to_celsius_udf/app.py` script. In this file we will be\nadding an `import` command and replacing the body of the `main()` function. So\nopen the `steps/05_fahrenheit_to_celsius_udf/app.py` script in VS Code and\nreplace this section:\n\n    \n    \n    import sys\n    \n    def main(temp_f: float) -> float:\n        return (float(temp_f) - 32) * (5/9)\n    \n\nWith this:\n\n    \n    \n    import sys\n    from scipy.constants import convert_temperature\n    \n    def main(temp_f: float) -> float:\n        return convert_temperature(float(temp_f), 'F', 'C')\n    \n\nDon't forget to save your changes.\n\nThe second change we need to make is to add `scipy` to our `requirements.txt`\nfile. Open the `steps/05_fahrenheit_to_celsius_udf/requirements.txt` file in\nVS Code, add a newline with `scipy` on it and save it.", "start_char_idx": 2, "end_char_idx": 982, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac00025c-986e-4388-aa4d-bbe113b04aad": {"__data__": {"id_": "ac00025c-986e-4388-aa4d-bbe113b04aad", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7275ffbc-4ceb-4b8d-a295-5a80cc46b027", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "7cb6210fe4a99438f7dbb617154f8c879d7e0ff4c2d6ca44b89ffd9a66db4e6c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7f22a94-b0ac-44d0-b0ad-c851eba9adb6", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "2a0efdd5fd157f229d4d106f66538325b219a50b31d366e56a53a63430eec8c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a203f1dc-d00a-4151-b09d-a070bbfc246c", "node_type": "1", "metadata": {}, "hash": "bcd6e9def74e78180fefc65cf054fed0d2dbe4c11d0ebc92dea329af7c70305b", "class_name": "RelatedNodeInfo"}}, "text": "Test your Changes Locally\n\nTo test the UDF locally, you will execute the\n`steps/05_fahrenheit_to_celsius_udf/app.py` script. Like we did in previous\nsteps, we'll execute it from the terminal. So go back to the terminal in VS\nCode, make sure that your `snowflake-demo` conda environment is active, then\nrun the following commands (which assume that your terminal has the root of\nyour repository open):\n\n    \n    \n    cd steps/05_fahrenheit_to_celsius_udf\n    pip install -r requirements.txt\n    python app.py 35\n    \n\nNotice that this time we're also running pip install to make sure that our\ndependent packages are installed. Once your function runs successfully we'll\nbe ready to deploy it via CI/CD!", "start_char_idx": 2, "end_char_idx": 703, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a203f1dc-d00a-4151-b09d-a070bbfc246c": {"__data__": {"id_": "a203f1dc-d00a-4151-b09d-a070bbfc246c", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e4e3074-122c-4aa6-887f-bb3306d80bb3", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "0a33e3cfb33e4dee5099100c62093404f4c9b06a40fde5b6721eca49356dbadd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac00025c-986e-4388-aa4d-bbe113b04aad", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "eff4d39ef8c342be1ee5ecef27f131817d9a16cf2ae0f2f5c499b1b1d2b9776a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8413286-8464-445e-a43e-451f8e2b4df5", "node_type": "1", "metadata": {}, "hash": "46d79d62d37e3e599eaf22948c2146f88fa2014cc96d33c719fcf95c896b7b1b", "class_name": "RelatedNodeInfo"}}, "text": "Configuring Your Forked GitHub Project\n\nIn order for your GitHub Actions workflow to be able to connect to your\nSnowflake account you will need to store your Snowflake credentials in GitHub.\nAction Secrets in GitHub are used to securely store values/variables which\nwill be used in your CI/CD pipelines. In this step we will create secrets for\neach of the parameters used by SnowCLI.\n\nFrom the repository, click on the `Settings` tab near the top of the page.\nFrom the Settings page, click on the `Secrets and variables` then `Actions`\ntab in the left hand navigation. The `Actions` secrets should be selected. For\neach secret listed below click on `New repository secret` near the top right\nand enter the name given below along with the appropriate value (adjusting as\nappropriate).\n\nSecret name\n\n|\n\nSecret value  \n  \n---|---  \n  \nSNOWSQL_ACCOUNT\n\n|\n\nmyaccount  \n  \nSNOWSQL_USER\n\n|\n\nmyusername  \n  \nSNOWSQL_PWD\n\n|\n\nmypassword  \n  \nSNOWSQL_ROLE\n\n|\n\nHOL_ROLE  \n  \nSNOWSQL_WAREHOUSE\n\n|\n\nHOL_WH  \n  \nSNOWSQL_DATABASE\n\n|\n\nHOL_DB  \n  \n**Tip** \\- For more details on how to structure the account name in\nSNOWSQL_ACCOUNT, see the account name discussion in [the Snowflake Python\nConnector install guide](https://docs.snowflake.com/en/user-guide/python-\nconnector-install.html#step-2-verify-your-installation).\n\nWhen you're finished adding all the secrets, the page should look like this:\n\n!\n\n**Tip** \\- For an even better solution to managing your secrets, you can\nleverage [GitHub Actions\nEnvironments](https://docs.github.com/en/actions/reference/environments).\nEnvironments allow you to group secrets together and define protection rules\nfor each of your environments.", "start_char_idx": 2, "end_char_idx": 1666, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8413286-8464-445e-a43e-451f8e2b4df5": {"__data__": {"id_": "c8413286-8464-445e-a43e-451f8e2b4df5", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9fa01623-0348-40cd-8b5c-87ca69e5678b", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "ba39e16afc904f3d06f23f50f148f55fb9629b01d9f137c4f5e34a57b455d7fb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a203f1dc-d00a-4151-b09d-a070bbfc246c", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "c7324d00a9c07de393d55f401bce7f196c9ec6f1298d0fe04c221c8c32cbd591", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c935c321-644a-48da-8435-af62f5f3b83c", "node_type": "1", "metadata": {}, "hash": "25e9ea5152d980c5edf1af971923ef95517961222f54dee8bf1b74f542a65459", "class_name": "RelatedNodeInfo"}}, "text": "Push Changes to Forked Repository\n\nNow that we have a changes ready and tested, and our Snowflake credentials\nstored in GitHub, let's commit them to our local repository and then push them\nto your forked repository. This can certainly be done from the command line,\nbut in this step we'll do so through VS Code to make it easy.\n\nStart by opening the \"Source Control\" extension in the left hand nav bar, you\nshould see two files with changes. Click the `+` (plus) sign at the right of\neach file name to stage the changes. Then enter a message in the \"Message\" box\nand click the blue `Commit` button to commit the changes locally. Here's what\nit should look like before you click the button:\n\n!\n\nAt this point those changes are only committed locally and have not yet been\npushed to your forked repository in GitHub. To do that, simply click the blue\n`Sync Changes` button to push these commits to GitHub. Here's what it should\nlook like before you click the button:\n\n!", "start_char_idx": 2, "end_char_idx": 969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c935c321-644a-48da-8435-af62f5f3b83c": {"__data__": {"id_": "c935c321-644a-48da-8435-af62f5f3b83c", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a05e471-4072-42f4-8360-96a8211333ba", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "26fc6c3ea650290e8ac85d633d6ceee145da71aac340dfa97168914ae1d5b2de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8413286-8464-445e-a43e-451f8e2b4df5", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "0ae6270884bd0245113c1b5f67c44434eaa21638ea8e448dc92a193308a93fba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82386efe-a813-4e7c-bb5a-975fa8f69c66", "node_type": "1", "metadata": {}, "hash": "6d57962c0c36e019e9a01484bb7e10369ee036a0636b65fc37421ebc639fd92d", "class_name": "RelatedNodeInfo"}}, "text": "Viewing GitHub Actions Workflow\n\nThis repository is already set up with a very simple GitHub Actions CI/CD\npipeline. You can review the code for the workflow by opening the\n`.github/workflows/build_and_deploy.yaml` file in VS Code.\n\nAs soon as you pushed the changes to your GitHub forked repo the workflow\nkicked off. To view the results go back to the homepage for your GitHub\nrepository and do the following:\n\n  * From the repository, click on the `Actions` tab near the top middle of the page\n  * In the left navigation bar click on the name of the workflow `Deploy Snowpark Apps`\n  * Click on the name of most recent specific run (which should match the comment you entered)\n  * From the run overview page click on the `deploy` job and then browse through the output from the various steps. In particular you might want to review the output from the `Deploy Snowpark apps` step.\n\n!\n\nThe output of the `Deploy Snowpark apps` step should be familiar to you by\nnow, and should be what you saw in the terminal in VS Code when you ran\nSnowCLI in previous steps. The one thing that may be different is the order of\nthe output, but you should be able to see what's happening.\n\nOnce you're finished with the Quickstart and want to clean things up, you can\nsimply run the `steps/11_teardown.sql` script. Since this is a SQL script we\nwill be using our native VS Code extension to execute it. So simply open the\n`steps/11_teardown.sql` script in VS Code and run the whole thing using the\n\"Execute All Statements\" button in the upper right corner of the editor\nwindow.\n\nWow, we have covered a lot of ground during this Quickstart! By now you have\nbuilt a robust data engineering pipeline using Snowpark Python stored\nprocedures. This pipeline processes data incrementally, is orchestrated with\nSnowflake tasks, and is deployed via a CI/CD pipeline. You also learned how to\nuse Snowflake's new developer CLI tool and Visual Studio Code extension!\nHere's a quick visual recap:\n\n!\n\nBut we've really only just scratched the surface of what's possible with\nSnowpark. Hopefully you now have the building blocks, and examples, you need\nto get started building your own data engineering pipeline with Snowpark\nPython. So, what will you build now?", "start_char_idx": 2, "end_char_idx": 2234, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82386efe-a813-4e7c-bb5a-975fa8f69c66": {"__data__": {"id_": "82386efe-a813-4e7c-bb5a-975fa8f69c66", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "db102b6f-ab71-4ee9-a851-6aa5d8355447", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "76532c8fd9a9a06741de32650b29ba9b4415ab9033feb9a06cd4f11ecce35398", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c935c321-644a-48da-8435-af62f5f3b83c", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "2074459d28b690ee37d3444d72077600fb0470a2a74c2f672a51da663476095e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbf1d756-a7b4-4c33-a41a-de7df05619b4", "node_type": "1", "metadata": {}, "hash": "3d7e9089e37d341a7f282cfc76902f003c84efc3d3b15c43b8438f04db89f700", "class_name": "RelatedNodeInfo"}}, "text": "What we've covered\n\nWe've covered a ton in this Quickstart, and here are the highlights:\n\n  * Snowflake's Table Format\n  * Data ingestion with COPY\n  * Schema inference\n  * Data sharing/marketplace (instead of ETL)\n  * Streams for incremental processing (CDC)\n  * Streams on views\n  * Python UDFs (with third-party packages)\n  * Python Stored Procedures\n  * Snowpark DataFrame API\n  * Snowpark Python programmability\n  * Warehouse elasticity (dynamic scaling)\n  * Visual Studio Code Snowflake native extension (PuPr, Git integration)\n  * SnowCLI (PuPr)\n  * Tasks (with Stream triggers)\n  * Task Observability\n  * GitHub Actions (CI/CD) integration", "start_char_idx": 2, "end_char_idx": 649, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbf1d756-a7b4-4c33-a41a-de7df05619b4": {"__data__": {"id_": "bbf1d756-a7b4-4c33-a41a-de7df05619b4", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "16f67917-3ecf-4982-8412-fe0c652d1c2a", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "8e45e1354e30df2882b67bce934ad03ba6d56017c82541e1ba49de7edb9741b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82386efe-a813-4e7c-bb5a-975fa8f69c66", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "32d15a7a4e237eb1d0e5fde36136d1adfb83357ea46e021a35ab0923b0f1998f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d846d164-8f4e-4759-bcbb-2a42a0e202a6", "node_type": "1", "metadata": {}, "hash": "0f416f95267b126487a89918e3ea9e202f6169f5c34c776b530c5292422850dc", "class_name": "RelatedNodeInfo"}}, "text": "Related Resources\n\nAnd finally, here's a quick recap of related resources:\n\n  * Full Demo on Snowflake Demo Hub\n  * Source Code on GitHub\n  * Snowpark Developer Guide for Python\n    * Writing Python UDFs\n    * Writing Stored Procedures in Snowpark (Python)\n    * Working with DataFrames in Snowpark Python\n  * Related Tools \n    * Snowflake Visual Studio Code Extension\n    * SnowCLI Tool", "start_char_idx": 2, "end_char_idx": 390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d846d164-8f4e-4759-bcbb-2a42a0e202a6": {"__data__": {"id_": "d846d164-8f4e-4759-bcbb-2a42a0e202a6", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8fc458a1-63ee-4526-bc09-8227c21bd13d", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "6b6c8e4c1bac955d9d840016523c06b5358302b2773ae11da47e9451198e79f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbf1d756-a7b4-4c33-a41a-de7df05619b4", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "17004868f8998f645c418ba227b577d3b19409878eed239aa19f264f5acceb6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84d45653-9d38-430a-a57d-99c96dbe8a2a", "node_type": "1", "metadata": {}, "hash": "4d34a97274e010732bdbac2396a1a02bd1d862b0364f842f105f7dd02619219c", "class_name": "RelatedNodeInfo"}}, "text": "Prerequisites\n\nThis guide assumes you have a basic working knowledge of Python, SQL and dbt", "start_char_idx": 2, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84d45653-9d38-430a-a57d-99c96dbe8a2a": {"__data__": {"id_": "84d45653-9d38-430a-a57d-99c96dbe8a2a", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "71ae0e58-1d2b-40a2-bddb-1b5cabcd82cb", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "421785549a7f4216c55615e0029af5e43194f8680d2d221fec29f08438be4562", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d846d164-8f4e-4759-bcbb-2a42a0e202a6", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "567d29d95b399598a083c0f5cf5715d9851f81b060e756ffd62d1f8e3075973a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69cb76e5-83bc-47ec-87ce-e937aef3d408", "node_type": "1", "metadata": {}, "hash": "afb52f58862af9bb51addfae23b54a15ad610cd1fba05be869dac1c032c5e676", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Learn\n\n  * how to use an opensource tool like Airflow to create a data scheduler\n  * how do we write a DAG and upload it onto Airflow\n  * how to build scalable pipelines using dbt, Airflow and Snowflake\n  * How to use Snowpark to interact with your Snowflake data using Python", "start_char_idx": 2, "end_char_idx": 290, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69cb76e5-83bc-47ec-87ce-e937aef3d408": {"__data__": {"id_": "69cb76e5-83bc-47ec-87ce-e937aef3d408", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "64092970-53da-4b4f-9ec7-a65a0ca58551", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "01d7a7cc560a000461c2556b7f7ed06c14496fa0c4766157e671961ff3d89f7e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84d45653-9d38-430a-a57d-99c96dbe8a2a", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "6f0b9fd0bf9dc521c62255d7f45a783702df98b321a78b1d5664ae187d2aceb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "862fa2fc-fbf8-4a67-b2c2-7cc63c6cd22b", "node_type": "1", "metadata": {}, "hash": "7e823c9a73e2c79a01e057d8ea61385aff5b8f7b94cd3ba21c61ec5f07e52439", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Need\n\nYou will need the following things before beginning:\n\n  1. Snowflake\n  2. **A Snowflake Account.**\n  3. **A Snowflake User created with appropriate permissions.** This user will need permission to create objects in the DEMO_DB database.\n  4. **Snowpark Enabled**\n  5. GitHub\n  6. **A GitHub Account.** If you don't already have a GitHub account you can create one for free. Visit the Join GitHub page to get started.\n  7. Integrated Development Environment (IDE)\n  8. **Your favorite IDE with Git integration.** If you don't already have a favorite IDE that integrates with Git I would recommend the great, free, open-source Visual Studio Code.\n  9. Docker Desktop\n  10. **Docker Desktop on your laptop.** We will be running Airflow as a container. Please install Docker Desktop on your desired OS by following the Docker setup instructions.\n  11. OpenAI API key\n  12. **Optional Step to Enable Chatbot Functionality**\n  13. Astro CLI\n  14. **The Astro CLI Installed.** We will be using the Astro CLI to create our Airflow environments. Please install the Astro CLI on your desired OS by following the Astro CLI setup instructions", "start_char_idx": 2, "end_char_idx": 1150, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "862fa2fc-fbf8-4a67-b2c2-7cc63c6cd22b": {"__data__": {"id_": "862fa2fc-fbf8-4a67-b2c2-7cc63c6cd22b", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43a1c15d-f815-4260-bc04-3bf73e295bf5", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "b9694561b2b33efbeb7b00879a4be12a36f7e1ed95bfa7816c3982fd904130f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69cb76e5-83bc-47ec-87ce-e937aef3d408", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "5e631177392f1d0764f8d22aafdbd17ee55bb9b4922869b7acb6d0a26b26ef76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca1d1c0e-7faa-431a-94a0-f92d55064823", "node_type": "1", "metadata": {}, "hash": "b1e7de37101e4f432e77fca45fb4d6e5fc001b90ab0c7dc9329a218e81c349fd", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Build\n\n  * A simple working Airflow pipeline with dbt and Snowflake\n  * A slightly more complex Airflow pipeline that incorporates Snowpark to analyze your data with Python\n\nFirst, let us create a folder by running the command below\n\n    \n    \n    mkdir dbt_airflow && cd dbt_airflow\n    \n\nNext, we will use the Astro CLI to create a new Astro project by running the\nfollowing command. An Astro project contains the set of files necessary to run\nAirflow, including dedicated folders for your DAG files, plugins, and\ndependencies.\n\n    \n    \n    astro dev init\n    \n\nNow, navigate into the DAG's folder that the Astro CLI created, and create a\nnew folder called dbt by running the following command.\n\n    \n    \n    mkdir dbt && cd dbt\n    \n\nNext, run the following command to install dbt and create all the necessary\nfolders for your project. It will prompt you for a name for your project,\nenter \u2018cosmosproject'.\n\n    \n    \n    dbt init\n    \n\nYour tree repository should look like this\n\n!Folderstructure\n\nNow that we have gotten our repo up, it is time to configure and set up our\ndbt project.\n\nBefore we begin, let's take some time to understand what we are going to do\nfor our dbt project.\n\nAs can be seen in the diagram below, we have 3 csv files `bookings_1`,\n`bookings_2` and `customers` . We are going to seed these csv files into\nSnowflake as tables. This will be detailed later.\n\nFollowing this, we are going to use dbt to merge `bookings_1` and `bookings_2`\ntables into `combined_bookings`. Then, we are going to join the\n`combined_bookings` and `customer` table on customer_id to form the\n`prepped_data` table.\n\nFinally, we are going to perform our analysis and transformation on the\n`prepped_data` by creating 2 views.\n\n  1. `hotel_count_by_day.sql`: This will create a hotel_count_by_day view in the ANALYSIS schema in which we will count the number of hotel bookings by day.\n  2. `thirty_day_avg_cost.sql`: This will create a thirty_day_avg_cost view in the ANALYSIS schema in which we will do a average cost of booking for the last 30 days.\n\n!dbt_structure\n\nFirst, let's go to the Snowflake console and run the script below after\nreplacing the field with your password of choice. What this does is create a\ndbt_user and a dbt_dev_role and after which we set up a database for dbt_user.", "start_char_idx": 2, "end_char_idx": 2313, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca1d1c0e-7faa-431a-94a0-f92d55064823": {"__data__": {"id_": "ca1d1c0e-7faa-431a-94a0-f92d55064823", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43a1c15d-f815-4260-bc04-3bf73e295bf5", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "b9694561b2b33efbeb7b00879a4be12a36f7e1ed95bfa7816c3982fd904130f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "862fa2fc-fbf8-4a67-b2c2-7cc63c6cd22b", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "b2497aa270206092b0e0dd87f5de0b1e65d65c9a537c22779c9010c5fe71e23b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f07a55b-a751-4ba8-8097-f9a9e9e65fec", "node_type": "1", "metadata": {}, "hash": "391c7b8f51d49ddccaad0b9eac85c73684d664802e7a37154fa1e369db29f975", "class_name": "RelatedNodeInfo"}}, "text": "Finally, we are going to perform our analysis and transformation on the\n`prepped_data` by creating 2 views.\n\n  1. `hotel_count_by_day.sql`: This will create a hotel_count_by_day view in the ANALYSIS schema in which we will count the number of hotel bookings by day.\n  2. `thirty_day_avg_cost.sql`: This will create a thirty_day_avg_cost view in the ANALYSIS schema in which we will do a average cost of booking for the last 30 days.\n\n!dbt_structure\n\nFirst, let's go to the Snowflake console and run the script below after\nreplacing the field with your password of choice. What this does is create a\ndbt_user and a dbt_dev_role and after which we set up a database for dbt_user.\n\n    \n    \n    USE ROLE SECURITYADMIN;\n    \n    CREATE OR REPLACE ROLE dbt_DEV_ROLE COMMENT='dbt_DEV_ROLE';\n    GRANT ROLE dbt_DEV_ROLE TO ROLE SYSADMIN;\n    \n    CREATE OR REPLACE USER dbt_USER PASSWORD=''\n    \tDEFAULT_ROLE=dbt_DEV_ROLE\n    \tDEFAULT_WAREHOUSE=dbt_WH\n    \tCOMMENT='dbt User';\n        \n    GRANT ROLE dbt_DEV_ROLE TO USER dbt_USER;\n    \n    -- Grant privileges to role\n    USE ROLE ACCOUNTADMIN;\n    \n    GRANT CREATE DATABASE ON ACCOUNT TO ROLE dbt_DEV_ROLE;\n    \n    /*---------------------------------------------------------------------------\n    Next we will create a virtual warehouse that will be used\n    ---------------------------------------------------------------------------*/\n    USE ROLE SYSADMIN;\n    \n    --Create Warehouse for dbt work\n    CREATE OR REPLACE WAREHOUSE dbt_DEV_WH\n      WITH WAREHOUSE_SIZE = 'XSMALL'\n      AUTO_SUSPEND = 120\n      AUTO_RESUME = true\n      INITIALLY_SUSPENDED = TRUE;\n    \n    GRANT ALL ON WAREHOUSE dbt_DEV_WH TO ROLE dbt_DEV_ROLE;\n    \n    \n\nLet's login with the `dbt_user` and create the database `DEMO_dbt` by running\nthe command\n\n    \n    \n    CREATE OR REPLACE DATABASE DEMO_dbt\n    \n    \n\nThen, in the new `Demo_dbt` database, copy and paste the following sql\nstatements to create our `bookings_1`, `bookings_2` and `customers` tables\nwithin Snowflake\n\n    \n    \n    CREATE TABLE bookings_1 (\n        id INTEGER,\n        booking_reference INTEGER,\n        hotel STRING,\n        booking_date DATE,\n        cost INTEGER\n    );\n    CREATE TABLE bookings_2 (\n        id INTEGER,\n        booking_reference INTEGER,\n        hotel STRING,\n        booking_date DATE,\n        cost INTEGER\n    );\n    CREATE TABLE customers (\n        id INTEGER,\n        first_name STRING,\n        last_name STRING,\n        birthdate DATE,\n        membership_no INTEGER\n    );\n    \n    \n\nAfter you're done, you should have a folder structure that looks like the\nbelow:\n\n!airflow\n\nNow, let's go back to our project `cosmosproject` > `dbt`that we set up\npreviously.\n\nWe will set up a couple configurations for the respective files below. Please\nnote for the `dbt_project.yml` you just need to replace the models section\n\npackages.yml (Create in `cosmosproject` folder if not already present)\n\n    \n    \n    packages:\n      - package: dbt-labs/dbt_utils\n        version: [\">=1.0.0\", \"<2.0.0\"]\n    \n\ndbt_project.yml\n\n    \n    \n    models:\n      my_new_project:\n          # Applies to all files under models/example/\n          transform:\n              schema: transform\n              materialized: view\n          analysis:\n              schema: analysis\n              materialized: view\n    \n\nNext, we will install the `dbt-labs/dbt_utils` that we had placed inside\n`packages.yml`. This can be done by running the command `dbt deps` from the\n`cosmosproject` folder.\n\nWe will now create a file called `custom_demo_macros.sql` under the `macros`\nfolder and input the below sql\n\n    \n    \n    {% macro generate_schema_name(custom_schema_name, node) -%}\n        {%- set default_schema = target.schema -%}\n        {%- if custom_schema_name is none -%}\n            {{ default_schema }}\n        {%- else -%}\n            {{ custom_schema_name | trim }}\n        {%- endif -%}\n    {%- endmacro %}\n    \n    \n    {% macro set_query_tag() -%}\n      {% set new_query_tag = model.name %} {# always use model name #}\n      {% if new_query_tag %}\n        {% set original_query_tag = get_current_query_tag() %}\n        {{ log(\"Setting query_tag to '\" ~ new_query_tag ~ \"'. Will reset to '\" ~ original_query_tag ~ \"' after materialization.\")", "start_char_idx": 1636, "end_char_idx": 5878, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f07a55b-a751-4ba8-8097-f9a9e9e65fec": {"__data__": {"id_": "7f07a55b-a751-4ba8-8097-f9a9e9e65fec", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43a1c15d-f815-4260-bc04-3bf73e295bf5", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "b9694561b2b33efbeb7b00879a4be12a36f7e1ed95bfa7816c3982fd904130f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca1d1c0e-7faa-431a-94a0-f92d55064823", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "de2cccfdaf88d7bdabea4de2ceac29f0447d04a8c716c57648627bc8d2cdffc3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b927f684-ca7f-4fff-8b5d-211041461a41", "node_type": "1", "metadata": {}, "hash": "d29299ac8cda918bcf07bcb509eee743a2e628e28482eaddcdbf214ce7cdb085", "class_name": "RelatedNodeInfo"}}, "text": "We will now create a file called `custom_demo_macros.sql` under the `macros`\nfolder and input the below sql\n\n    \n    \n    {% macro generate_schema_name(custom_schema_name, node) -%}\n        {%- set default_schema = target.schema -%}\n        {%- if custom_schema_name is none -%}\n            {{ default_schema }}\n        {%- else -%}\n            {{ custom_schema_name | trim }}\n        {%- endif -%}\n    {%- endmacro %}\n    \n    \n    {% macro set_query_tag() -%}\n      {% set new_query_tag = model.name %} {# always use model name #}\n      {% if new_query_tag %}\n        {% set original_query_tag = get_current_query_tag() %}\n        {{ log(\"Setting query_tag to '\" ~ new_query_tag ~ \"'. Will reset to '\" ~ original_query_tag ~ \"' after materialization.\") }}\n        {% do run_query(\"alter session set query_tag = '{}'\".format(new_query_tag)) %}\n        {{ return(original_query_tag)}}\n      {% endif %}\n      {{ return(none)}}\n    {% endmacro %}\n    \n\nNow we are done setting up our dbt environment. Your file structure should\nlook like the below screenshot:\n\n!airflow\n\nIn this section, we will be prepping our sample csv data files alongside the\nassociated sql models.\n\nTo start, let us first create 3 excel files under the folder `data` inside the\ndbt folder.\n\nbookings_1.csv\n\n    \n    \n    id,booking_reference,hotel,booking_date,cost\n    1,232323231,Pan Pacific,2021-03-19,100\n    1,232323232,Fullerton,2021-03-20,200\n    1,232323233,Fullerton,2021-04-20,300\n    1,232323234,Jackson Square,2021-03-21,400\n    1,232323235,Mayflower,2021-06-20,500\n    1,232323236,Suncity,2021-03-19,600\n    1,232323237,Fullerton,2021-08-20,700\n    \n\nbookings_2.csv\n\n    \n    \n    id,booking_reference,hotel,booking_date,cost\n    2,332323231,Fullerton,2021-03-19,100\n    2,332323232,Jackson Square,2021-03-20,300\n    2,332323233,Suncity,2021-03-20,300\n    2,332323234,Jackson Square,2021-03-21,300\n    2,332323235,Fullerton,2021-06-20,300\n    2,332323236,Suncity,2021-03-19,300\n    2,332323237,Berkly,2021-05-20,200\n    \n\ncustomers.csv\n\n    \n    \n    id,first_name,last_name,birthdate,membership_no\n    1,jim,jone,1989-03-19,12334\n    2,adrian,lee,1990-03-10,12323\n    \n\nOur folder structure should be like as below\n\n!airflow\n\nCreate 2 folders `analysis` and `transform` in the models folder. Please\nfollow the sections below for analysis and transform respectively.", "start_char_idx": 5123, "end_char_idx": 7475, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b927f684-ca7f-4fff-8b5d-211041461a41": {"__data__": {"id_": "b927f684-ca7f-4fff-8b5d-211041461a41", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6abead3b-1d43-4181-89c5-688e14e555e2", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "cbfb5459633ff388be4d09b533513ecb4a3d613ab3c75d2f6aa0798217789564", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f07a55b-a751-4ba8-8097-f9a9e9e65fec", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "1cf050d505c0eae6d117da453590c1611bc8f56bfa9a163f911bf49f5b5bf6b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ef84d32-5a52-4b18-bafe-573e6078caba", "node_type": "1", "metadata": {}, "hash": "a42f6343fc4f2cff0fc4ed7e8b003576568a280b51a487b493a34c3a62578bcb", "class_name": "RelatedNodeInfo"}}, "text": "dbt models for transform folder\n\nInside the `transform` folder, we will have 3 SQL files\n\n  1. `combined_bookings.sql`: This will combine the 2 bookings CSV files we had above and create the `COMBINED_BOOKINGS` view in the `TRANSFORM` schema.\n\ncombined_bookings.sql\n\n    \n    \n    {{ dbt_utils.union_relations(\n        relations=[ref('bookings_1'), ref('bookings_2')]\n    ) }}\n    \n\n  2. `customer.sql`: This will create a `CUSTOMER` view in the `TRANSFORM` schema.\n\ncustomer.sql\n\n    \n    \n    SELECT ID \n        , FIRST_NAME\n        , LAST_NAME\n        , birthdate\n    FROM {{ ref('customers') }}\n    \n\n  3. `prepped_data.sql`: This will create a `PREPPED_DATA` view in the `TRANSFORM` schema in which it will perform an inner join on the `CUSTOMER` and `COMBINED_BOOKINGS` views from the steps above.\n\nprepped_data.sql\n\n    \n    \n    SELECT A.ID \n        , FIRST_NAME\n        , LAST_NAME\n        , birthdate\n        , BOOKING_REFERENCE\n        , HOTEL\n        , BOOKING_DATE\n        , COST\n    FROM {{ref('customer')}}  A\n    JOIN {{ref('combined_bookings')}} B\n    on A.ID = B.ID", "start_char_idx": 2, "end_char_idx": 1085, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ef84d32-5a52-4b18-bafe-573e6078caba": {"__data__": {"id_": "8ef84d32-5a52-4b18-bafe-573e6078caba", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aaa9bb82-bba0-40ad-8bc6-3571e4698bc0", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "a076a69990ae4696b6c33c58a61650cb5628930a2e207d7bef537e4c029a8e7b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b927f684-ca7f-4fff-8b5d-211041461a41", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "24b97240eeb68223be09a68161af7bd4f772fa278fe68e527f465b6d5bf18bdd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "554f3eb7-194f-416f-8f1c-fb43c0cc8b0b", "node_type": "1", "metadata": {}, "hash": "b31e25a37d45f6bd22a69cab2634b0ed0672ce7ab31920ded54f43648c67e33a", "class_name": "RelatedNodeInfo"}}, "text": "dbt models for analysis folder\n\nNow let's move on to the `analysis` folder. Change to the `analysis` folder\nand create these 2 SQL files\n\n  1. `hotel_count_by_day.sql`: This will create a hotel_count_by_day view in the `ANALYSIS` schema in which we will count the number of hotel bookings by day.\n\n    \n    \n    SELECT\n      BOOKING_DATE,\n      HOTEL,\n      COUNT(ID) as count_bookings\n    FROM {{ ref('prepped_data') }}\n    GROUP BY\n      BOOKING_DATE,\n      HOTEL\n    \n\n  2. `thirty_day_avg_cost.sql`: This will create a thirty_day_avg_cost view in the `ANALYSIS` schema in which we will do a average cost of booking for the last 30 days.\n\n    \n    \n    SELECT\n      BOOKING_DATE,\n      HOTEL,\n      COST,\n      AVG(COST) OVER (\n        ORDER BY BOOKING_DATE ROWS BETWEEN 29 PRECEDING AND CURRENT ROW\n      ) as \"30_DAY_AVG_COST\",\n      COST -   AVG(COST) OVER (\n        ORDER BY BOOKING_DATE ROWS BETWEEN 29 PRECEDING AND CURRENT ROW\n      ) as \"DIFF_BTW_ACTUAL_AVG\"\n    FROM {{ ref('prepped_data') }}\n    \n\nYour file structure should look like the below screenshot. We have now\nfinished our dbt models and can proceed to working on using Airflow to manage\nthem.\n\n!airflow\n\nNow going back to your Airflow directory, open up the requirements.txt file\nthat the Astro CLI created. Copy and paste the following text block to install\nthe Cosmos and\nSnowflake libraries for Airflow. Cosmos will be used to turn each dbt model\ninto a task/task group complete with retries, alerting, etc.\n\n    \n    \n    astronomer-cosmos\n    apache-airflow-providers-snowflake\n    \n\nNext, open up the Dockerfile in your Airflow folder and copy and paste the\nfollowing code block to overwrite your existing Dockerfile. These changes will\ncreate a virtual environment for dbt along with the adapter to connect to\nSnowflake. It's recommended to use a virtual environment because dbt and\nAirflow can have conflicting dependencies.\n\n    \n    \n    # syntax=quay.io/astronomer/airflow-extensions:latest\n    \n    FROM quay.io/astronomer/astro-runtime:9.1.0-python-3.9-base\n    \n    RUN python -m venv dbt_venv && source dbt_venv/bin/activate && pip install --no-cache-dir dbt-snowflake && deactivate\n    \n\nNow that our Airflow environment is set up, lets create our DAG! Instead of\nusing the conventional DAG definition methods, we'll be using Cosmos' dbtDAG\nclass to create a DAG based on our dbt models. This allows us to turn our dbt\nprojects into Apache Airflow DAGs and Task Groups with a few lines of code. To\ndo so, create a new file in the `dags` folder called `my_cosmos_dag.py` and\ncopy and paste the following code block into the file.\n\n    \n    \n    from datetime import datetime\n    import os\n    from cosmos import DbtDag, ProjectConfig, ProfileConfig, ExecutionConfig\n    from cosmos.profiles import SnowflakeUserPasswordProfileMapping\n    from pathlib import Path\n    \n    dbt_project_path = Path(\"/usr/local/airflow/dags/dbt/cosmosproject\")\n    \n    profile_config = ProfileConfig(profile_name=\"default\",\n                                   target_name=\"dev\",\n                                   profile_mapping=SnowflakeUserPasswordProfileMapping(conn_id=\"snowflake_default\", \n                                                        profile_args={\n                                                            \"database\": \"demo_dbt\",\n                                                            \"schema\": \"public\"\n                                                            },\n                                                        ))\n    \n    \n    dbt_snowflake_dag = DbtDag(project_config=ProjectConfig(dbt_project_path,),\n                        operator_args={\"install_deps\": True},\n                        profile_config=profile_config,\n                        execution_config=ExecutionConfig(dbt_executable_path=f\"{os.environ['AIRFLOW_HOME']}/dbt_venv/bin/dbt\",),\n                        schedule_interval=\"@daily\",\n                        start_date=datetime(2023, 9, 10),\n                        catchup=False,\n                        dag_id=\"dbt_snowflake_dag\",)\n    \n    \n\nFirst, we import the various Cosmos libraries\n\n\u2022 DbtDag: This is a class that allows you to create an Apache Airflow Directed\nAcyclic Graph (DAG) for a dbt (Data Build Tool) project. The DAG will execute\nthe dbt project according to the specified configuration.", "start_char_idx": 2, "end_char_idx": 4332, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "554f3eb7-194f-416f-8f1c-fb43c0cc8b0b": {"__data__": {"id_": "554f3eb7-194f-416f-8f1c-fb43c0cc8b0b", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aaa9bb82-bba0-40ad-8bc6-3571e4698bc0", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "a076a69990ae4696b6c33c58a61650cb5628930a2e207d7bef537e4c029a8e7b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ef84d32-5a52-4b18-bafe-573e6078caba", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "998b066d4d3d841702a67aa086fd9a6d4ee2c1bee9943aee209551837724270f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9330e42-bd95-4513-a9f0-2b1283dc0a79", "node_type": "1", "metadata": {}, "hash": "eb7008a6d54a0bd5f7d301c1ed74795f174744e225276a4a1647beaa4fe48ec5", "class_name": "RelatedNodeInfo"}}, "text": "The DAG will execute\nthe dbt project according to the specified configuration.\n\n\u2022 ProjectConfig: This class is used to specify the configuration for the dbt\nproject that the DbtDag will execute by pointing it to the path tfor your dbt\nproject.\n\n\u2022 ProfileConfig: This class is used to specify the configuration for the\ndatabase profile that dbt will use when executing the project. This includes\nthe profile name, target name, and any necessary mapping to Airflow\nconnections.\n\n\u2022 ExecutionConfig: This class is used to specify any additional configuration\nfor executing the dbt project. We'll be pointing it to the virtual environment\nwe created at `{os.environ['AIRFLOW_HOME']}/dbt_venv/bin/dbt` in our\n`Dockerfile` to execute in.\n\n\u2022 PostgresUserPasswordProfileMapping and SnowflakeUserPasswordProfileMapping:\nThese classes are used to map Airflow connections to dbt profiles for\nPostgreSQL and Snowflake databases, respectively. This allows you to manage\nyour database credentials in Airflow and use them in dbt.\n\nAfter the imports, a ProfileConfig object is created, which is used to define\nthe configuration for the Snowflake connection. The\nSnowflakeUserPasswordProfileMapping class is used to map the Snowflake\nconnection in Airflow to a dbt profile. The DbtDag object is then created.\nThis object represents an Airflow DAG that will execute a dbt project. It\ntakes several parameters:\n\n`project_config` specifies the path to the dbt project we created at\n`/usr/local/airflow/dags/dbt/cosmosprojectoperator_args` is used to pass\narguments to the dbt operator. Here, it's specifying that dependencies should\nbe installed. `profile_config` is the profile configuration defined earlier,\nwhich will be used to execute the dbt models in Snowflake. `execution_config`\nspecifies the path to our virtual environment to executue our dbt code in.\n`schedule_interval`, `start_date`, `catchup`, and `dag_id` are standard\nAirflow DAG parameters. You can use any of the standard Airflow DAG parameters\nin a dbtDAG as well.\n\nWithin your Airflow `dbt_airflow` directory, enter the below command to start\nyour Airflow environment\n\n    \n    \n    astro dev start", "start_char_idx": 4254, "end_char_idx": 6402, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9330e42-bd95-4513-a9f0-2b1283dc0a79": {"__data__": {"id_": "b9330e42-bd95-4513-a9f0-2b1283dc0a79", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a71869a8-8ef7-49e2-b64f-d365d3df7ca1", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "c7bf945ae1a5b07d832beb317ee4a7852dc928feb8b9ef7a4bded823a5f09a7e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "554f3eb7-194f-416f-8f1c-fb43c0cc8b0b", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "690771af8c1ffcd845a6ad6ad3a179a9171b364f12b6ffdf95beb40681670718", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2559101-6a2f-4664-8845-098727c5cbdd", "node_type": "1", "metadata": {}, "hash": "7b620c2f014d2fa12c64da5e9730ea58402141b49df445a6911ffa9e0a1fc190", "class_name": "RelatedNodeInfo"}}, "text": "Running our cosmos_dag!\n\nWe will now run our DAG `cosmos_dag` to see our dbt models in action! If you\nclick the big blue play button on the top left of the screen, you'll see your\ntasks start to run your dbt transformations within your Snowflake database. If\neverything goes smoothly, your Snowflake environment should look like the\nfollowing screenshot:\n\n!airflow\n\nOur `Transform` and `Analysis` views have been created successfully! Open them\nto see the results of our analysis, and check out the other tables to see how\ndata was transformed using dbt.\n\nNow that we've gotten our dbt DAG set up, lets extend it by adding Snowpark\nfor some data analysis with Python. To do this, we'll need to change some\nexisting files and add new requirements to our local airflow environment.\nWhile we do this, lets stop our Airflow environment by running the following\ncommand so we can restart it later with our changes incorporated.\n\n    \n    \n    astro dev stop\n    \n\nFirst, go to your `packages.txt` file in your root directory and add `build-\nessential` to it, then save. The build-essential package in Linux systems is a\nreference for all the packages needed to compile a Debian package. We'll be\nusing it to create a Python 3.8 Virtual Environment to run our Snowpark code\nin, since Snowpark uses Python 3.8, and Airflow only supports Python versions\n3.9 and above.\n\nNext, we'll need to import the Snowpark provider to create our Snowpark task.\nWhile in development the provider package is not yet in pypi. For this demo,\ndownload the `astro_provider_snowflake-0.0.0-py3-none-any.whl` file from the\nthis [link](https://github.com/astronomer/airflow-snowpark-\ndemo/tree/main/include). Then, copy the downloaded file into your include\ndirectory in your `DBT_Airflow` folder. In the future, this will be a part of\nthe base Snowflake provider, but in the meantime you can use this .whl file in\nany other projects that require it.\n\nAfter that, we'll need to add the .whl file to our `requirements.txt`. Copy\nand paste the following line into your `requirements.txt` file to do so.\n\n    \n    \n    /tmp/astro_provider_snowflake-0.0.0-py3-none-any.whl\n    \n\nFinally, we'll need to create a `requirements-snowpark.txt` to install some\nnecessary packages into the Python VirtualEnv we'll be creating. To do so,\ncreate a file called `requirements-snowpark.txt` in your root Airflow\ndirectory and copy and paste the following code block into it:\n\n    \n    \n    psycopg2-binary\n    snowflake_snowpark_python[pandas]==1.5.1\n    virtualenv\n    /tmp/astro_provider_snowflake-0.0.0-py3-none-any.whl\n    \n\nThese packages will allow us to interact with Snowpark through the virtual\nenvironment we're creating.\n\nNow that we've got our Snowpark provider present, we'll need to edit our\nDockerfile to install it, and spin up the Snowpark Python VirtualEnv. Copy and\npaste the following code block into your Dockerfile to add the necessary\ncommands.\n\n    \n    \n    # syntax=quay.io/astronomer/airflow-extensions:latest\n    \n    FROM quay.io/astronomer/astro-runtime:9.1.0-python-3.9-base\n    \n    COPY include/astro_provider_snowflake-0.0.0-py3-none-any.whl /tmp\n    \n    # Create the virtual environment\n    PYENV 3.8 snowpark requirements-snowpark.txt\n    \n    # Install packages into the virtual environment\n    COPY requirements-snowpark.txt /tmp\n    RUN python3.8 -m pip install -r /tmp/requirements-snowpark.txt\n    \n    \n    RUN python -m venv dbt_venv && source dbt_venv/bin/activate && pip install --no-cache-dir dbt-snowflake && pip install --no-cache-dir dbt-postgres && deactivate\n    \n\nThe first line tells Docker to use the Astronomer provided BuildKit that\nenables us to create virtual environments with the PYENV command. Then we COPY\nin the `.whl` file and use it to create a Python 3.8 VirtualEnv called\nsnowpark.", "start_char_idx": 2, "end_char_idx": 3805, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2559101-6a2f-4664-8845-098727c5cbdd": {"__data__": {"id_": "d2559101-6a2f-4664-8845-098727c5cbdd", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a71869a8-8ef7-49e2-b64f-d365d3df7ca1", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "c7bf945ae1a5b07d832beb317ee4a7852dc928feb8b9ef7a4bded823a5f09a7e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9330e42-bd95-4513-a9f0-2b1283dc0a79", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "f86e22c4f986a33d4e38015f2fa1dd57d9050c7fa074c66a8820a94b362e14cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f30b7af8-c113-42de-ab8b-f1447c523663", "node_type": "1", "metadata": {}, "hash": "bdb7297c16acb82236d16b4c5a405b62dd4608fb7dc75056d2970edcfb2b10f9", "class_name": "RelatedNodeInfo"}}, "text": "Then we COPY\nin the `.whl` file and use it to create a Python 3.8 VirtualEnv called\nsnowpark.\n\nIn order to use Cosmos and Snowpark together, we'll need to use Cosmos's\n`dbtTaskGroup` with a normal Airflow DAG instead of a `dbtDAG`. The definition\nfor this is almost identical to the `dbtDAG` approach, and allows us to add\nadditional tasks up or downstream of our dbt workflows. Instead of editing our\nexisting DAG, create a new file called `cosmosandsnowflake.py` in your DAG's\nfolder, and copy the following code into it:\n\n    \n    \n    from airflow.operators.dummy_operator import DummyOperator\n    from astro import sql as aql\n    from astro.files import File\n    from astro.sql.table import Table\n    from cosmos import DbtTaskGroup, ProjectConfig, ProfileConfig, ExecutionConfig\n    from cosmos.profiles import SnowflakeUserPasswordProfileMapping\n    from astronomer.providers.snowflake.utils.snowpark_helpers import SnowparkTable\n    from pathlib import Path\n    \n    dbt_project_path = Path(\"/usr/local/airflow/dags/dbt/cosmosproject\")\n    snowflake_objects = {'demo_database': 'DEMO',\n                         'demo_schema': 'DEMO',\n                         'demo_warehouse': 'COMPUTE_WH',\n                         'demo_xcom_stage': 'XCOM_STAGE',\n                         'demo_xcom_table': 'XCOM_TABLE',\n                         'demo_snowpark_wh': 'SNOWPARK_WH'\n    }\n    _SNOWFLAKE_CONN_ID = \"snowflake_default\"\n    \n    profile_config = ProfileConfig(\n        profile_name=\"default\",\n        target_name=\"dev\",\n        profile_mapping=SnowflakeUserPasswordProfileMapping(\n            conn_id=\"snowflake_default\",\n            profile_args={\n              \"database\": \"demo_dbt\",\n                \"schema\": \"public\"\n            },\n        )\n    )\n    \n    @dag(default_args={\n             \"snowflake_conn_id\": _SNOWFLAKE_CONN_ID,\n             \"temp_data_output\": \"table\",\n             \"temp_data_db\": snowflake_objects['demo_database'],\n             \"temp_data_schema\": snowflake_objects['demo_schema'],\n             \"temp_data_overwrite\": True,\n             \"database\": snowflake_objects['demo_database'],\n             \"schema\": snowflake_objects['demo_schema']\n             },\n        schedule_interval=\"@daily\",\n        start_date=datetime(2023, 9, 10),\n        catchup=False,\n        dag_id=\"dbt_snowpark\",\n    )\n    def dbt_snowpark_dag():\n        transform_data = DbtTaskGroup(\n            group_id=\"transform_data\",\n            project_config=ProjectConfig(dbt_project_path),\n            profile_config=profile_config,\n            execution_config=ExecutionConfig(dbt_executable_path=f\"{os.environ['AIRFLOW_HOME']}/dbt_venv/bin/dbt\"),\n            operator_args={\"install_deps\": True},\n        )\n    \n        intermediate = DummyOperator(task_id='intermediate')\n    \n        @task.snowpark_virtualenv(python_version='3.8', requirements=['snowflake-ml-python==1.0.9'])\n        def findbesthotel(snowflake_objects:dict): \n            \n            df = snowpark_session.sql(\"\"\"\n                SELECT *\n                FROM DEMO_DBT.PUBLIC.THIRTY_DAY_AVG_COST\n            \"\"\").to_pandas()\n            highest_cost_hotel = df[df['COST'] == df['COST'].max()]['HOTEL']\n    \n            highest_cost_hotel_str = str(highest_cost_hotel)\n            print(highest_cost_hotel)\n    \n            return highest_cost_hotel_str\n        \n    \n        besthotel = findbesthotel(snowflake_objects)\n        transform_data >> intermediate >> besthotel\n    \n    dbt_snowpark_dag = dbt_snowpark_dag()\n    \n\nThis DAG adds a new Snowpark task called `findbesthotel`, which means it is\nexecuted in a virtual environment with Snowpark and other specified\ndependencies installed. In this case, we installed the `snowflake-ml-\npython==1.0.9` package to install pandas and other popular data science\npackages for data analysis. The `findbesthotel` task connects to a Snowflake\ndatabase and fetches data from the `THIRTY_DAY_AVG_COST` table in the `PUBLIC`\nschema of the `DEMO_DBT` database. It then converts this data into a pandas\nDataFrame and finds the hotel with the highest cost. The name of this hotel is\nconverted to a string, printed, and then returned by the task.", "start_char_idx": 3712, "end_char_idx": 7874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f30b7af8-c113-42de-ab8b-f1447c523663": {"__data__": {"id_": "f30b7af8-c113-42de-ab8b-f1447c523663", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a71869a8-8ef7-49e2-b64f-d365d3df7ca1", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "c7bf945ae1a5b07d832beb317ee4a7852dc928feb8b9ef7a4bded823a5f09a7e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2559101-6a2f-4664-8845-098727c5cbdd", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "164b55ef5037d17d9633caf76085f90c0297957d0afba6fa9848d6ed9249753c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4dac82da-e676-4141-97ed-9b576e358bf9", "node_type": "1", "metadata": {}, "hash": "47db8d431d953cdbf26885d44ddfc17274d0839bdfdd29baf6d8a88d5d465a3d", "class_name": "RelatedNodeInfo"}}, "text": "In this case, we installed the `snowflake-ml-\npython==1.0.9` package to install pandas and other popular data science\npackages for data analysis. The `findbesthotel` task connects to a Snowflake\ndatabase and fetches data from the `THIRTY_DAY_AVG_COST` table in the `PUBLIC`\nschema of the `DEMO_DBT` database. It then converts this data into a pandas\nDataFrame and finds the hotel with the highest cost. The name of this hotel is\nconverted to a string, printed, and then returned by the task. All of our dbt\ntransformation set up stays almost identical, aside from changing from using\ndbtDAG to dbtTaskGroup.\n\nNow that we've add Snowpark to our environment and written our DAG, it's time\nto restart our Airflow environment and run it! Restart your Airflow\nenvironment with the following terminal command\n\n    \n    \n    astro dev start\n    \n\nLogin to the Airflow UI the same way as before, and you should see a new dag\ncalled `dbt_snowpark`. Since we already set up our Snowflake connection\nbefore, we can just run this new DAG immediately by clicking its blue play\nbutton. Then, click on the DAG and open up its graph view to watch it run. It\nshould look like the example below:\n\n!snowparkdag\n\nAfter the DAG has finished running, select the `findbesthotel` and open its\nlog file. If all has gone well, you'll see the most expensive hotel to stay at\nprinted out for your convenience!\n\n!airflow", "start_char_idx": 7383, "end_char_idx": 8774, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4dac82da-e676-4141-97ed-9b576e358bf9": {"__data__": {"id_": "4dac82da-e676-4141-97ed-9b576e358bf9", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "51b5bdb9-9108-4a0f-a89a-467bc99288b7", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "a459431026f73256b45fc4122b714cac8d6057e2e2db97f4750e3a75f6ae9de9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f30b7af8-c113-42de-ab8b-f1447c523663", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "788870588dfef550cacd58312923107e8a225a3a876d8a4c2187d4fbc30c07b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66ea6cb5-aff7-4fab-a6bc-cc462947f5c6", "node_type": "1", "metadata": {}, "hash": "8dbd0c25cd608cfa3755a2d4c5fcd3f4bfc880f576598b3d59106a7256738de1", "class_name": "RelatedNodeInfo"}}, "text": "View Streamlit Dashboard\n\nWe can now view our analyzed data on a Streamlit\ndashboard. To do this, go to terminal and enter the following bash command to\nconnect into the Airflow webserver container.\n\n    \n    \n    astro dev bash -w\n    \n\nThen, run the following command to start a streamlit application.\n\n    \n    \n    cd include/streamlit/src\n    python -m streamlit run ./streamlit_app.py\n    \n\nAfter you've done so, you can view your data dashboard by navigating to\nhttp://localhost:8501/ in your browser! If you'd like to enable the ability to\nask questions about your data, you'll need to add an OpenAI API key in the\n.env file and restart your Airflow environment.", "start_char_idx": 2, "end_char_idx": 672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66ea6cb5-aff7-4fab-a6bc-cc462947f5c6": {"__data__": {"id_": "66ea6cb5-aff7-4fab-a6bc-cc462947f5c6", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5557f947-a70c-409c-8946-926445b10890", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "fa6b560c50177c0196de340edf5342bf24d7377dee33080b093538d7c48a007a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4dac82da-e676-4141-97ed-9b576e358bf9", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "b73d5b7f2eeb525910b783239145e43929c4bc105a0faa42cf84e47a3ac51551", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93600d8f-e327-4d83-9039-135861ba9035", "node_type": "1", "metadata": {}, "hash": "911fbde607f6afe5b448dcfd3f2ecde49f1a830d26e862236b8c5ccdeb856de1", "class_name": "RelatedNodeInfo"}}, "text": "View Streamlit Dashboard\n\nWe can now view our analyzed data on a Streamlit\ndashboard. To do this, go to terminal and enter the following bash command to\nconnect into the Airflow webserver container.\n\n    \n    \n    astro dev bash -w\n    \n\nThen, run the following command to start a streamlit application.\n\n    \n    \n    cd include/streamlit/src\n    python -m streamlit run ./streamlit_app.py\n    \n\nAfter you've done so, you can view your data dashboard by navigating to\nhttp://localhost:8501/ in your browser! If you'd like to enable the ability to\nask questions about your data, you'll need to add an OpenAI API key in the\n.env file and restart your Airflow environment.\n\nCongratulations! You have created your first Apache Airflow DAG with dbt,\nCosmos, Snowflake, and Snowpark! We encourage you to continue with your free\ntrial by loading your own sample or production data and by using some of the\nmore advanced capabilities of Airflow and Snowflake not covered in this lab.", "start_char_idx": 2, "end_char_idx": 978, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93600d8f-e327-4d83-9039-135861ba9035": {"__data__": {"id_": "93600d8f-e327-4d83-9039-135861ba9035", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "980b3526-c63a-4596-bc4d-66c986b49b32", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "37300a4344097c022cbf8a286a9473a14e1f5b76c0509bd4de249176ed65e78a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66ea6cb5-aff7-4fab-a6bc-cc462947f5c6", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "bde164474b2429c041dd0c3f586449f2b4eef92a6574e96607652979009aa0ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a051959a-dd48-4b5f-92dc-7d02e091099a", "node_type": "1", "metadata": {}, "hash": "116f8fde2bcdbcd3ead2e309cf4e38a7591279b1499b8ae6437374f31515da4d", "class_name": "RelatedNodeInfo"}}, "text": "Additional Resources:\n\n  * Join our dbt community Slack which contains more than 18,000 data practitioners today. We have a dedicated slack channel #db-snowflake to Snowflake related content.\n  * Quick tutorial on how to write a simple Airflow DAG\n  * Documentation on how to use Cosmos to render dbt workflows in Airflow Cosmos", "start_char_idx": 2, "end_char_idx": 330, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a051959a-dd48-4b5f-92dc-7d02e091099a": {"__data__": {"id_": "a051959a-dd48-4b5f-92dc-7d02e091099a", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a64f3653-738f-4768-902d-fccf3e1be5a3", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "de8d4c2a3e9f9b123a63fd317b189a9eb054a6878660e0d97bbdaf0dcecf58a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93600d8f-e327-4d83-9039-135861ba9035", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "0b89541eedccb02d447fdfbedcaff7665f4e9930a93372ca1a8f144c34bcf849", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9562397c-d024-4942-ad37-cb919365016e", "node_type": "1", "metadata": {}, "hash": "61ae0074128d73861bac3a4682b988cf51d07e5524d4cc99d5203f2dad6a52c0", "class_name": "RelatedNodeInfo"}}, "text": "What we've covered:\n\n  * How to set up Airflow, dbt & Snowflake\n  * How to create a dbt DAG using Cosmos to run dbt models using Airflow", "start_char_idx": 2, "end_char_idx": 138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9562397c-d024-4942-ad37-cb919365016e": {"__data__": {"id_": "9562397c-d024-4942-ad37-cb919365016e", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b9fbf0d3-6f2c-4fc5-bfe4-45c1cc377beb", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "f9ba6128b6249adaeca662b6818f419df4546387024e11be606c4d317fa66331", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a051959a-dd48-4b5f-92dc-7d02e091099a", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "179ee0a35ad7794463dce001df2c28a422f6e0e28ec76f054ef052561f2e7029", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f80f3ca-b081-4102-be03-92d5d6e8af32", "node_type": "1", "metadata": {}, "hash": "01e2ab4071dcac95528e499f704b2b71556ed7cdd81d65337cd86195327fca35", "class_name": "RelatedNodeInfo"}}, "text": "Background\n\nData engineering is a broad discipline which includes data ingestion, data\ntransformation, and data consumption, along with the accompanying SDLC best\npractices (i.e. DevOps). Data engineers employ different tools and approaches\ndepending on the phase. For this Quickstart we will focus on the data\ntransformation phase in particular.\n\nData transformation involves taking source data which has been ingested into\nyour data platform and cleansing it, combining it, and modeling it for\ndownstream use. Historically the most popular way to transform data has been\nwith the SQL language and data engineers have built data transformation\npipelines using SQL often with the help of ETL/ELT tools. But recently many\nfolks have also begun adopting the DataFrame API in languages like Python for\nthis task. For the most part a data engineer can accomplish the same data\ntransformations with either approach, and deciding between the two is mostly a\nmatter of preference and particular use cases. That being said, there are use\ncases where a particular data transform can't be expressed in SQL and a\ndifferent approach is needed. The most popular approach for these use cases is\nPython along with a DataFrame API.", "start_char_idx": 2, "end_char_idx": 1217, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f80f3ca-b081-4102-be03-92d5d6e8af32": {"__data__": {"id_": "4f80f3ca-b081-4102-be03-92d5d6e8af32", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76763497-e58f-4819-8640-da876e9da6b4", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "64949fbaa0ca616231b11d8a9c4044f0e28e540fc827b78e7e7250e96961ef97", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9562397c-d024-4942-ad37-cb919365016e", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "674c07cad2a167b5d0601c421f2716c51740403582c8537dcfc07ee8399b67ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c62a42e-f52c-4cb3-a01e-eeb47d1f5020", "node_type": "1", "metadata": {}, "hash": "23d34b232658a5f303a8aeb15388e107d431b4cb16bd1d0b76192928e5d84c1c", "class_name": "RelatedNodeInfo"}}, "text": "dbt\n\nEnter dbt. dbt is one of the most popular data transformation tools today. And\nuntil now dbt has been entirely a SQL-based transformation tool. But with the\nrelease of dbt version 1.3, it's now possible to create both SQL and Python\nbased models in dbt! Here's how dbt explains it:\n\ndbt Python (\"dbt-py\") models will help you solve use cases that can't be\nsolved with SQL. You can perform analyses using tools available in the open\nsource Python ecosystem, including state-of-the-art packages for data science\nand statistics. Before, you would have needed separate infrastructure and\norchestration to run Python transformations in production. By defining your\nPython transformations in dbt, they're just models in your project, with all\nthe same capabilities around testing, documentation, and lineage. ([dbt Python\nmodels](https://docs.getdbt.com/docs/building-a-dbt-project/building-\nmodels/python-models))", "start_char_idx": 2, "end_char_idx": 915, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c62a42e-f52c-4cb3-a01e-eeb47d1f5020": {"__data__": {"id_": "8c62a42e-f52c-4cb3-a01e-eeb47d1f5020", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c35d9e44-90b5-4766-9d50-a2c38a050015", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "f2fb25e897962df85c330bb5ff227e4168122fe3aabd94b42f085022c9f73b43", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f80f3ca-b081-4102-be03-92d5d6e8af32", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "3b095aa222d7ba49fa5ed64b1fa9ca0ea294884c04703f44015fc9a561a1046a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3cc6d81e-15f0-43fd-9812-1d43d6a44c37", "node_type": "1", "metadata": {}, "hash": "ec606005c6b0325196b039a278c5ea13ef0913823edc1926435704641ec99887", "class_name": "RelatedNodeInfo"}}, "text": "Snowflake\n\nPython based dbt models are made possible by [Snowflake's new native Python\nsupport and Snowpark API for Python](https://www.snowflake.com/blog/snowpark-\npython-innovation-available-all-snowflake-customers/) (Snowpark Python for\nshort). Snowpark Python includes the following exciting capabilities:\n\n  * Python (DataFrame) API\n  * Python Scalar User Defined Functions (UDFs)\n  * Python UDF Batch API (Vectorized UDFs)\n  * Python Table Functions (UDTFs)\n  * Python Stored Procedures\n  * Integration with Anaconda\n\nWith Snowflake's Snowpark Python capabilities, you no longer need to maintain,\nsecure and pay for separate infrastructure/services to run Python code as it\ncan now be run directly within Snowflake's Enterprise grade data platform! For\nmore details check out [the Snowpark Developer Guide for\nPython](ttps://docs.snowflake.com/en/developer-\nguide/snowpark/python/index.html).\n\nThis guide will provide step-by-step instructions for how to get started with\nSnowflake Snowpark Python and dbt's new Python-based models.", "start_char_idx": 2, "end_char_idx": 1040, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3cc6d81e-15f0-43fd-9812-1d43d6a44c37": {"__data__": {"id_": "3cc6d81e-15f0-43fd-9812-1d43d6a44c37", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "09eb4de1-b223-451d-83fe-6555c0de6742", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "f8f1699c968c5f2c94a43f5fd0924bee4096f28b8366359ed83de6f51b43251d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c62a42e-f52c-4cb3-a01e-eeb47d1f5020", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "c78881e4551ccda4093d2e8d9a7adc3b1e6057443e609617601b246e9a8878b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0da4f774-4ba2-4797-9d77-330d5f98eb7a", "node_type": "1", "metadata": {}, "hash": "1bde6941b587b66b12e48b93cae991ab851370248015c39ac7e7849dc179c632", "class_name": "RelatedNodeInfo"}}, "text": "Prerequisites\n\nThis guide assumes that you have a basic working knowledge of dbt, Python, and\nAnaconda.", "start_char_idx": 2, "end_char_idx": 105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0da4f774-4ba2-4797-9d77-330d5f98eb7a": {"__data__": {"id_": "0da4f774-4ba2-4797-9d77-330d5f98eb7a", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "163a2cc0-c33d-4e34-8c74-2d69e3247bea", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "19e657d2a769a3cdf1db8d2eda745f8c6fce30c99a77cd8a28f803b9eb202ba5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3cc6d81e-15f0-43fd-9812-1d43d6a44c37", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "05f8226111d608911a77831dd75810f7b72e112eb4827ba1f41ec7bf00124cc3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7fb51725-faa5-4e3b-9721-d41b25a6c315", "node_type": "1", "metadata": {}, "hash": "a5cd6c10cbdb41f7ce2b354a1ccad46f16097f1eec591198d7687095a0b09f11", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Learn\n\n  * The basics of Snowpark Python\n  * How to create Python-based models in dbt\n  * How to create and use Python UDFs in your dbt Python model\n  * How the integration between Snowpark Python and dbt's Python models works", "start_char_idx": 2, "end_char_idx": 240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7fb51725-faa5-4e3b-9721-d41b25a6c315": {"__data__": {"id_": "7fb51725-faa5-4e3b-9721-d41b25a6c315", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7f19304-dd58-4e75-a745-96f9105e9fe2", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "2656c6574131cc2b841f6230fc8ece3e2abe0ba0a084057476aa8d99e2b7da70", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0da4f774-4ba2-4797-9d77-330d5f98eb7a", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "b3c90c8c5b75bc9064c6f38aa54ab369928558fbc98cf806eff34713bc4a5f1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f29aea69-bf90-4b99-af81-1b8606123fd2", "node_type": "1", "metadata": {}, "hash": "12088edc484650309ed691318853a4457020e56e301f0350cb4860dd51819ef3", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Need\n\nYou will need the following things before beginning:\n\n  1. Snowflake \n    1. **A Snowflake Account.**\n    2. **A Snowflake Database named DEMO_DB.**\n    3. **A Snowflake User created with appropriate permissions.** This user will need permission to create objects in the DEMO_DB database.\n  2. Anaconda \n    1. **Anaconda installed on your computer.** Check out the Anaconda Installation instructions for the details.\n  3. dbt \n    1. **dbt installed on your computer.** Python models were first introduced in dbt version 1.3, so make sure you install version 1.3 or newer of dbt. Please follow these steps (where `` is any name you want for the Anaconda environment): \n      1. `conda create -n  python=3.8`\n      2. `conda activate `\n      3. `pip install dbt-core dbt-snowflake` (or `pip install --upgrade dbt-core dbt-snowflake` if upgrading)\n  4. Integrated Development Environment (IDE) \n    1. **Your favorite IDE installed on your computer.** If you don't already have a favorite IDE I would recommend the great, free, open-source Visual Studio Code.\n\nThe following are optional but will help you debug your Python dbt models:\n\n  1. **Python extension installed in your IDE.** For VS Code, install the Python extension from Microsoft.", "start_char_idx": 2, "end_char_idx": 1262, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f29aea69-bf90-4b99-af81-1b8606123fd2": {"__data__": {"id_": "f29aea69-bf90-4b99-af81-1b8606123fd2", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b5c9401-281a-40b6-b72a-77d5206685ba", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "1aaa599f611f3952f5f2fbad314b0b693b6716ca524ff3cf6f7b0d8ac231dddf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fb51725-faa5-4e3b-9721-d41b25a6c315", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "286c18c0a9056c5583597805ab609c5c2b55aaa3ca9fbd67e11c1c674d7078b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97603c75-aa6f-451b-a7e8-5e3305477932", "node_type": "1", "metadata": {}, "hash": "ae9d27a5c36dccd266a8d0f7faea82f7f10290ca212729e1f8def2efc892f782", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Build\n\n  * A simple dbt project with Python-based models!\n\nThe easiest way to get started with dbt, and ensure you have the most up-to-\ndate dbt configurations, is to the run the `dbt init` command. The `dbt init`\nprocess will create two folders in the directory you run it from, a `logs`\nfolder and a folder with the same name as your project. So in a terminal\nchange to the directory where you want the new dbt project folder created and\nexecute `dbt init`. Follow the prompts to create your new project. For most of\nthe prompts enter the value appropriate to your environment, but for\n**database** and **schema** enter the values below:\n\n  * Enter a name for your project (letters, digits, underscore): ``\n  * Which database would you like to use? ... Enter a number: `[1] snowflake`\n  * account (https://.snowflakecomputing.com): ``\n  * user (dev username): ``\n  * Desired authentication type option (enter a number): `[1] password`\n  * password (dev password): ``\n  * role (dev role): ``\n  * warehouse (warehouse name): ``\n  * database (default database that dbt will build objects in): `DEMO_DB`\n  * schema (default schema that dbt will build objects in): `DEMO_SCHEMA`\n  * threads (1 or more) [1]: `1`\n\nSee the [dbt init\ndocumentation](https://docs.getdbt.com/reference/commands/init) for more\ndetails on what just happened, but after `dbt init` finished you will have a\nfunctional dbt project and the following default SQL models in the `models`\nfolder:\n\n    \n    \n    models\n    |-- example\n    |--|-- my_first_dbt_model.sql\n    |--|-- my_second_dbt_model.sql\n    |--|-- schema.yml\n    \n\n**Note** \\- The connection details you just entered have been stored in a dbt\nconnection profile in the default location: `~/.dbt/profiles.yml`. To learn\nmore about managing connection details with dbt profiles please see\n[configuring your profile](https://docs.getdbt.com/dbt-cli/configure-your-\nprofile).\n\nTo verify that everything is configured properly, open a terminal, cd to the\nfolder that `dbt init` created and then execute `dbt run`. dbt should execute\nsuccessfully and you should now have the following objects created in\nSnowflake in your `DEMO_DB.DEMO_SCHEMA` schema:\n\n  * A table named `my_first_dbt_model`\n  * A view named `my_second_dbt_model`\n\n**Note** \\- If the `dbt run` command did not complete successfully, it's most\nlikely something wrong with your connection details. Please review and update\nthose details in your dbt connection profile saved here:\n`~/.dbt/profiles.yml`. Then retry.", "start_char_idx": 2, "end_char_idx": 2519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97603c75-aa6f-451b-a7e8-5e3305477932": {"__data__": {"id_": "97603c75-aa6f-451b-a7e8-5e3305477932", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7cb08143-5e41-44d5-a8da-1e626eb271f1", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "74aec4c387959cbbb9ca6fbfbf8707ab66b737d24ec649835e29db6d49882bc9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f29aea69-bf90-4b99-af81-1b8606123fd2", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "cad001887ca581fd5a0b342377f3572941f2a419bceb924af91ecc2b82701b03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f47e422a-e24b-422e-9de2-53c16dbfba2b", "node_type": "1", "metadata": {}, "hash": "94b9eba3fa52eae63186afaaaefcf7a5b01ba7a06827834e3370d3d4dd3445ad", "class_name": "RelatedNodeInfo"}}, "text": "Overview\n\nNow that you've created your dbt project and run it once successfully, it's\ntime to create our first Python model! But before we do, here's a brief\noverview of how to create a Python model in dbt (from [dbt Python\nmodels](https://docs.getdbt.com/docs/building-a-dbt-project/building-\nmodels/python-models)):\n\nEach Python model lives in a `.py` file in your `models/` folder. It defines a\nfunction named **`model()`** , which takes two parameters:\n\n  * `dbt`: A class compiled by dbt Core, unique to each model, that enables you to run your Python code in the context of your dbt project and DAG.\n  * `session`: A class representing the connection to the Python backend on your data platform. The session is needed to read in tables as DataFrames, and to write DataFrames back to tables. In PySpark, by convention, the `SparkSession` is named `spark`, and available globally. For consistency across platforms, we always pass it into the `model` function as an explicit argument named `session`.\n\nThe `model()` function must return a single DataFrame.", "start_char_idx": 2, "end_char_idx": 1061, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f47e422a-e24b-422e-9de2-53c16dbfba2b": {"__data__": {"id_": "f47e422a-e24b-422e-9de2-53c16dbfba2b", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fed2e603-ed06-4762-89b6-3deb2c5935b5", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "9a21e211aa6971aa858ad949628daaeb9cea32566fc6351467cf9226a03b7857", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97603c75-aa6f-451b-a7e8-5e3305477932", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "a7cce8159b767177bf760cb74d968053f85285867b5301a855e3ab85843cea50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fee303f5-616e-4bb2-85b4-6920906f46ad", "node_type": "1", "metadata": {}, "hash": "5c530ff24d6f97e46976ecb93b73f66d9351071eba9944a54d3ba389435019cf", "class_name": "RelatedNodeInfo"}}, "text": "Create my_first_python_model\n\nIn the `models/example` folder, create a new file named\n`my_first_python_model.py` and copy & paste the following content to the new\nfile:\n\n    \n    \n    def model(dbt, session):\n        # Must be either table or incremental (view is not currently supported)\n        dbt.config(materialized = \"table\")\n    \n        # DataFrame representing an upstream model\n        df = dbt.ref(\"my_first_dbt_model\")\n     \n        return df\n    \n\nFinally, save the file and execute `dbt run` again. If everything ran\nsuccessfully you just ran your very first Python model in dbt! It's that\nsimple. Here are a few things to note at this point:\n\n  * No Jinja! dbt Python models don't use Jinja to render compiled code.\n  * You don't have to explicitly import the Snowflake Snowpark Python library, dbt will do that for you. More on this in the next step.\n  * As mentioned above, every dbt Python model must define a method named `model` that has the following signature: `model(dbt, session)`.\n  * As of 10/17/2022 only `table` or `incremental` materializations are supported, which is why we configured it explicitly here.\n  * You can use `dbt.ref()` and `dbt.source()` just the same as their Jinja equivalents in SQL models. And you can refer to either Python or SQL models interchangeably!\n\n**Note** \\- For more details on accessing dbt project contexts from your\nPython models, please check out [Accessing project\ncontext](https://docs.getdbt.com/docs/building-a-dbt-project/building-\nmodels/python-models#accessing-project-context).\n\nSo what just happened you ran your dbt Python model? The single best thing to\nhelp you debug and understand what's happening is to look at your [Query\nHistory](https://docs.snowflake.com/en/user-guide/ui-snowsight-\nactivity.html#query-history) in Snowflake. Please take a minute now to review\nwhat happened in your Snowflake account, by reviewing your recent query\nhistory.", "start_char_idx": 2, "end_char_idx": 1926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fee303f5-616e-4bb2-85b4-6920906f46ad": {"__data__": {"id_": "fee303f5-616e-4bb2-85b4-6920906f46ad", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d7912ef-c68f-4bce-86c2-f04793fe1dc0", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "7b47e9c26bb137f22e15230d3d68ee3cc863595f0e7859be89c09dd677d7af1e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f47e422a-e24b-422e-9de2-53c16dbfba2b", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "9b020049c73ee3f131dda644625584fd941c3024c8c8330482d5e4f8892d6c9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d478305-3651-4df2-be1d-121b38db2ab9", "node_type": "1", "metadata": {}, "hash": "97162025223a15f5f9abdb8650dfe745f155f1cac3c9f2a283605a566266da70", "class_name": "RelatedNodeInfo"}}, "text": "Overview of dbt Executed Queries\n\nHere are the queries that dbt executed when you ran the\n`my_first_python_model` model. I've omitted the content of the stored\nprocedure in this section so that it's easier to see what's happening at a\nhigh level. In the next section we'll discuss what's happening inside the\nstored procedure.\n\n  1. List schemas\n    \n         show terse schemas in database DEMO_DB\n    \n\n  2. List objects\n    \n         show terse objects in DEMO_DB.DEMO_SCHEMA\n    \n\n  3. Create stored procedure\n    \n         CREATE OR REPLACE PROCEDURE DEMO_DB.DEMO_SCHEMA.my_first_python_model__dbt_sp ()\n     RETURNS STRING\n     LANGUAGE PYTHON\n     RUNTIME_VERSION = '3.8' -- TODO should this be configurable?\n     PACKAGES = ('snowflake-snowpark-python')\n     HANDLER = 'main'\n     EXECUTE AS CALLER\n     AS\n     $$\n    \n     \n    \n     $$;\n    \n\n  4. Call stored procedure\n    \n         CALL DEMO_DB.DEMO_SCHEMA.my_first_python_model__dbt_sp()\n    \n\n    1. Materialize model\n        \n                 CREATE  OR  REPLACE TABLE DEMO_DB.DEMO_SCHEMA.my_first_python_model AS  SELECT  *  FROM ( SELECT  *  FROM (DEMO_DB.DEMO_SCHEMA.my_first_dbt_model)\n        \n\n  5. Drop stored procedure\n    \n         drop procedure if exists DEMO_DB.DEMO_SCHEMA.my_first_python_model__dbt_sp()", "start_char_idx": 2, "end_char_idx": 1285, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d478305-3651-4df2-be1d-121b38db2ab9": {"__data__": {"id_": "1d478305-3651-4df2-be1d-121b38db2ab9", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "78f8c795-ea9e-4eae-8e07-5025fefbcb90", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "59f1ab12372275eb67bfc7c4710580afe54744b7532feb0d2e2ae8fe9819db0f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fee303f5-616e-4bb2-85b4-6920906f46ad", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "5019ddec9dedfabf7dcb99a79b4a4eacabec145078764940400aa1ff69f5742a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af7c6f2c-4fa2-43c7-a73e-525026cdb4df", "node_type": "1", "metadata": {}, "hash": "1eda4b925037a04fedb7faca17ff11a5c2a6df052de7747027c0098370073967", "class_name": "RelatedNodeInfo"}}, "text": "Overview of Stored Procedure Code Generated by dbt\n\nSo what exactly is happening in the stored procedure? This is really the\ncritical part to focus on in order to understand what dbt is doing with Python\nmodels. As we go through this, keep in mind that a core design principal for\ndbt Python models is that all of the Python code will be executed in\nSnowflake, and none will be run locally. Here's how dbt's documentation puts\nit:\n\nThe prerequisites for dbt Python models include using an adapter for a data\nplatform that supports a fully featured Python runtime. In a dbt Python model,\nall Python code is executed remotely on the platform. None of it is run by dbt\nlocally. We believe in clearly separating model definition from model\nexecution. In this and many other ways, you'll find that dbt's approach to\nPython models mirrors its longstanding approach to modeling data in SQL. (from\n[dbt Python models](https://docs.getdbt.com/docs/building-a-dbt-\nproject/building-models/python-models))\n\nWith that, let's look at our first Python dbt model again:\n\n    \n    \n    def model(dbt, session):\n        # Must be either table or incremental (view is not currently supported)\n        dbt.config(materialized = \"table\")\n    \n        # DataFrame representing an upstream model\n        df = dbt.ref(\"my_first_dbt_model\")\n     \n        return df\n    \n\nSo in order to run that in Snowflake we need a few things. First it needs to\nrun in the context of a stored procedure, and second the stored procedure\nneeds to contain everything necessary to run the model. This means that dbt\nneeds to generate Python code to handle the following things:\n\n  1. Provide a `dbt` and `session` object to our `model()` method. \n    1. The `dbt` object needs to provide access to dbt configuration and dag context with methods like `ref()`, `source()`, `config()`, `this()`, etc.\n    2. The `session` object is the Snowpark session\n  2. Provide the logic to materialize the resulting DataFrame returned by the `model()` method\n  3. Provide the overall orchestration of these pieces\n\nHere's the full content of the stored procedure generated by dbt (which was\nremoved in the overview above and replaced with the `<dbt compiled Python\ncode>` placeholder):\n\n    \n    \n    def model(dbt, session):\n        # Must be either table or incremental (view is not currently supported)\n        dbt.config(materialized = \"table\")\n    \n        # DataFrame representing an upstream model\n        df = dbt.ref(\"my_first_dbt_model\")\n     \n        return df\n    \n    \n    # This part is user provided model code\n    # you will need to copy the next section to run the code\n    # COMMAND ----------\n    # this part is dbt logic for get ref work, do not modify\n    \n    def ref(*args,dbt_load_df_function):\n        refs = {\"my_first_dbt_model\": \"DEMO_DB.DEMO_SCHEMA.my_first_dbt_model\"}\n        key = \".\".join(args)\n        return dbt_load_df_function(refs[key])\n    \n    \n    def source(*args, dbt_load_df_function):\n        sources = {}\n        key = \".", "start_char_idx": 2, "end_char_idx": 3013, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af7c6f2c-4fa2-43c7-a73e-525026cdb4df": {"__data__": {"id_": "af7c6f2c-4fa2-43c7-a73e-525026cdb4df", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "78f8c795-ea9e-4eae-8e07-5025fefbcb90", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "59f1ab12372275eb67bfc7c4710580afe54744b7532feb0d2e2ae8fe9819db0f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d478305-3651-4df2-be1d-121b38db2ab9", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "cd66d76284bd99a9fcc3969fdc0b10e071528e49ac6392c5f7379c3a2004a8e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a69ffa28-4676-4bc1-a148-f00adc53c1e0", "node_type": "1", "metadata": {}, "hash": "958fcb08d8eea667139322b2bba1ff9a3ad8d48be2021efd78af1ccfd2840ebe", "class_name": "RelatedNodeInfo"}}, "text": "\".join(args)\n        return dbt_load_df_function(refs[key])\n    \n    \n    def source(*args, dbt_load_df_function):\n        sources = {}\n        key = \".\".join(args)\n        return dbt_load_df_function(sources[key])\n    \n    \n    config_dict = {}\n    \n    \n    class config:\n        def __init__(self, *args, **kwargs):\n            pass\n    \n        @staticmethod\n        def get(key):\n            return config_dict.get(key)\n    \n    class this:\n        \"\"\"dbt.this() or dbt.this.identifier\"\"\"\n        database = 'DEMO_DB'\n        schema = 'DEMO_SCHEMA'\n        identifier = 'my_first_python_model'\n        def __repr__(self):\n            return 'DEMO_DB.DEMO_SCHEMA.my_first_python_model'\n    \n    \n    class dbtObj:\n        def __init__(self, load_df_function) -> None:\n            self.source = lambda x: source(x, dbt_load_df_function=load_df_function)\n            self.ref = lambda x: ref(x, dbt_load_df_function=load_df_function)\n            self.config = config\n            self.this = this()\n            self.is_incremental = False\n    \n    # COMMAND ----------\n    \n    # To run this in snowsight, you need to select entry point to be main\n    # And you may have to modify the return type to text to get the result back\n    # def main(session):\n    #     dbt = dbtObj(session.table)\n    #     df = model(dbt, session)\n    #     return df.collect()\n    \n    # to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python\n    # then you can do the following to run model\n    # dbt = dbtObj(session.table)\n    # df = model(dbt, session)\n    \n    \n    def materialize(session, df, target_relation):\n        # we have to make sure pandas is imported\n        import pandas\n        if isinstance(df, pandas.core.frame.DataFrame):\n            # session.write_pandas does not have overwrite function\n            df = session.createDataFrame(df)\n        df.write.mode(\"overwrite\").save_as_table(\"DEMO_DB.DEMO_SCHEMA.my_first_python_model\", create_temp_table=False)\n    \n    def main(session):\n        dbt = dbtObj(session.table)\n        df = model(dbt, session)\n        materialize(session, df, dbt.this)\n        return \"OK\"\n    \n\n**Note** \\- When building and debugging your dbt Python models, you can find\nthis Python code in the compiled version of the model by running `dbt compile`\n(or `dbt run`). The compiled files are written to the `target-path` folder,\nwhich by default is a folder named `target` in your dbt project folder.\n\nNow that you understand the basics of dbt Python models, let's add in another\nconcept, the user-defined function (or UDF for short). \"A UDF (user-defined\nfunction) is a user-written function that can be called from Snowflake in the\nsame way that a built-in function can be called. Snowflake supports UDFs\nwritten in multiple languages, including Python.\" (see [Introduction to Python\nUDFs](https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-\nintroduction.html) for more details). By creating and registering a UDF it can\nbe used in your DataFrame (or SQL) expression just like a built-in function.\n\nSo let's create a second dbt Python model, this time with a UDF. In the\n`models/example` folder, create a new file named `my_second_python_model.py`\nand copy & paste the following content to the new file:\n\n    \n    \n    from snowflake.snowpark.functions import udf\n    \n    def model(dbt, session):\n        # Must be either table or incremental (view is not currently supported)\n        dbt.config(materialized = \"table\")\n    \n        # User defined function\n        @udf\n        def add_one(x: int) -> int:\n            x = 0 if not x else x\n            return x + 1\n    \n        # DataFrame representing an upstream model\n        df = dbt.ref(\"my_first_dbt_model\")\n    \n        # Add a new column containing the id incremented by one\n        df = df.withColumn(\"id_plus_one\", add_one(df[\"id\"]))\n    \n        return df\n    \n\nAs you can see from the code above we're creating a very simple UDF named\n`add_one()` which adds one to the number passed to the function.", "start_char_idx": 2861, "end_char_idx": 6967, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a69ffa28-4676-4bc1-a148-f00adc53c1e0": {"__data__": {"id_": "a69ffa28-4676-4bc1-a148-f00adc53c1e0", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "78f8c795-ea9e-4eae-8e07-5025fefbcb90", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "59f1ab12372275eb67bfc7c4710580afe54744b7532feb0d2e2ae8fe9819db0f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af7c6f2c-4fa2-43c7-a73e-525026cdb4df", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "6b0e56a4c018dd2f64d8f3556ef204cced2a0e3bf5ab06e20030501b654c41c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2422a6aa-a9b7-4066-a51a-ecdf1829d9c3", "node_type": "1", "metadata": {}, "hash": "70490f505e48a68844557b7752528b7b622da34519d397909980aabe08de6ba3", "class_name": "RelatedNodeInfo"}}, "text": "Finally, save\nthe file and execute `dbt run` again (or just run this new model by executing\n`dbt run --model my_second_python_model`). If everything ran successfully you\njust ran your second Python model with a UDF in dbt!", "start_char_idx": 6968, "end_char_idx": 7190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2422a6aa-a9b7-4066-a51a-ecdf1829d9c3": {"__data__": {"id_": "2422a6aa-a9b7-4066-a51a-ecdf1829d9c3", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75d5e96c-a8cf-4bfa-b131-4b850ed8ae43", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "4dbc9ebacac8e84ed4bfdda3caab376e1bdc8d0261c53929ee9db7c5fae69a06", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a69ffa28-4676-4bc1-a148-f00adc53c1e0", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "5da0f7cb07bdc02f4680ac676db205285d884537b069791bbe549a8e208fa297", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a22afc1-351e-40a5-b820-d0d702c5cbb8", "node_type": "1", "metadata": {}, "hash": "e1aad6d934cb3de9a21da0eb1f3e735f70e53b3a8d19031931239c1c4f3d4c91", "class_name": "RelatedNodeInfo"}}, "text": "Aside: Where to Define the UDF\n\nRemember in the last section how we saw that your dbt Python model code is\nsupplemented with dbt's compiled code and then wrapped in a Snowflake stored\nprocedure? Well that's important to note as it has implications for where/how\nyou define a UDF in your Python model. In the example above we defined and\nregistered the function inside the `main()` handler function, using the `@udf`\ndecorator.\n\nIf you try and define and register the function outside of the `main()`\nhandler function you will get the following error: `Query outside the handler\nis not allowed in Stored Procedure. Please move query related functions into\nthe handler in function`. In order to move the function definition outside of\nthe `main()` handler function you must only define it there and then register\nit inside. Here is an example from dbt's documentation ([dbt Python\nmodels](https://docs.getdbt.com/docs/building-a-dbt-project/building-\nmodels/python-models)):\n\n    \n    \n    import snowflake.snowpark.types as T\n    import snowflake.snowpark.functions as F\n    import numpy\n    \n    def register_udf_add_random():\n        add_random = F.udf(\n            # use 'lambda' syntax, for simple functional behavior\n            lambda x: x + numpy.random.normal(),\n            return_type=T.FloatType(),\n            input_types=[T.FloatType()]\n        )\n        return add_random\n    \n    def model(dbt, session):\n    \n        dbt.config(\n            materialized = \"table\",\n            packages = [\"numpy\"]\n        )\n    \n        temps_df = dbt.ref(\"temperatures\")\n    \n        add_random = register_udf_add_random()\n    \n        # warm things up, who knows by how much\n        df = temps_df.withColumn(\"degree_plus_random\", add_random(\"degree\"))\n        return df\n    \n\nNotice that the function is defined outside the `main()` handler function and\nthen registered (and used) inside.\n\nNow let's take another deep dive into what happened when we run the second dbt\nmodel with a UDF. As before, please take a minute now to review what happened\nin your Snowflake account, by reviewing your recent query history ([Query\nHistory](https://docs.snowflake.com/en/user-guide/ui-snowsight-\nactivity.html#query-history)).", "start_char_idx": 2, "end_char_idx": 2218, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a22afc1-351e-40a5-b820-d0d702c5cbb8": {"__data__": {"id_": "2a22afc1-351e-40a5-b820-d0d702c5cbb8", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c817712f-d768-4d9f-a7fa-5deb72ddac90", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "d0fcdc220416ff8d024c1bc1375b6d13c5e42960068dd69f121466b5963ac45f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2422a6aa-a9b7-4066-a51a-ecdf1829d9c3", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "77b7a8ad118b4dbbf9f112e3179312e08b52baa9dadd8bb04b14584cdbe5adce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c85882ad-9aa3-4a3c-a743-b95c65aa1f3d", "node_type": "1", "metadata": {}, "hash": "f8c923b21a34d4dbd9883156702ebd8d15c4891cf1d145ee855fcf8690a0f318", "class_name": "RelatedNodeInfo"}}, "text": "Overivew of dbt Executed Queries\n\nHere are the queries that dbt executed when you ran the\n`my_second_python_model` model. You will notice that it's similar to the first\nmodel, but with some more queries to deal with creating the UDF. I've again\nomitted the content of the stored procedure in this section so that it's\neasier to see what's happening at a high level. But I won't walk through it\nthis time as it's very similar to the previous example, the only real\ndifference being the difference in model code.\n\n  1. List schemas\n    \n         show terse schemas in database DEMO_DB\n    \n\n  2. List objects\n    \n         show terse objects in DEMO_DB.DEMO_SCHEMA\n    \n\n  3. Create stored procedure\n    \n         CREATE OR REPLACE PROCEDURE DEMO_DB.DEMO_SCHEMA.my_second_python_model__dbt_sp ()\n     RETURNS STRING\n     LANGUAGE PYTHON\n     RUNTIME_VERSION = '3.8' -- TODO should this be configurable?\n     PACKAGES = ('snowflake-snowpark-python')\n     HANDLER = 'main'\n     EXECUTE AS CALLER\n     AS\n     $$\n    \n     \n    \n     $$;\n    \n\n  4. Call stored procedure\n    \n         CALL DEMO_DB.DEMO_SCHEMA.my_second_python_model__dbt_sp()\n    \n\n    1. Use the correct database\n        \n                 SELECT CURRENT_DATABASE()\n        \n\n    2. Create a temporary stage\n        \n                 create temporary stage if not exists DEMO_DB.DEMO_SCHEMA.SNOWPARK_TEMP_STAGE_4LBT18DLR8\n        \n\n    3. List the contents of the stage\n        \n                 ls '@\"DEMO_DB\".\"DEMO_SCHEMA\".SNOWPARK_TEMP_STAGE_4LBT18DLR8'\n        \n\n    4. Get the list of files in the stage\n        \n                 SELECT \"name\" FROM ( SELECT  *  FROM  TABLE ( RESULT_SCAN('')))\n        \n\n    5. Create the temporary function\n        \n                 CREATE TEMPORARY FUNCTION \"DEMO_DB\".\"DEMO_SCHEMA\".SNOWPARK_TEMP_FUNCTION_1Z5V5PPNVH(arg1 BIGINT)\n         RETURNS BIGINT\n         LANGUAGE PYTHON\n         RUNTIME_VERSION=3.8\n         PACKAGES=('cloudpickle==2.0.0')\n         HANDLER='compute'\n        \n         AS $$\n        \n         \n        \n         $$;\n        \n\n    6. Materialize the table\n        \n                 CREATE  OR  REPLACE TABLE  DEMO_DB.DEMO_SCHEMA.my_second_python_model AS  SELECT  *  FROM ( SELECT \"ID\", \"DEMO_DB\".\"DEMO_SCHEMA\".SNOWPARK_TEMP_FUNCTION_1Z5V5PPNVH(\"ID\") AS \"ID_PLUS_ONE\" FROM ( SELECT  *  FROM (DEMO_DB.DEMO_SCHEMA.my_first_dbt_model)))\n        \n\n  5. Drop stored procedure\n    \n         drop procedure if exists DEMO_DB.DEMO_SCHEMA.my_second_python_model__dbt_sp()\n    \n\nThe overall steps are the same, but notice that what happens during the stored\nprocedure execution (step #4) is different and more complex. This is because\nthe Snowpark library, running inside the Snowflake Python stored procedure has\nmore work to do with UDFs. It has to pickle the UDF content, create a\ntemporary stage, and finally create the UDF in Snowflake.", "start_char_idx": 2, "end_char_idx": 2858, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c85882ad-9aa3-4a3c-a743-b95c65aa1f3d": {"__data__": {"id_": "c85882ad-9aa3-4a3c-a743-b95c65aa1f3d", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3eff2c07-e375-4bda-8031-c81ef3314f89", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "cf6f1a9b7effbc5a8e4b1f5b192de70a8fc048f833c485565cf60f25b335b04f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a22afc1-351e-40a5-b820-d0d702c5cbb8", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "aeced7ff31f7db2dcd05717b52cf35e1e38609b59eb55dba255b359ad079fc0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b922dca-78a7-49cc-81e5-6702aebc9b02", "node_type": "1", "metadata": {}, "hash": "94fa5ac55edc271f29c4324a6c195b8196601f4a473fe298ea33ec46a285ad02", "class_name": "RelatedNodeInfo"}}, "text": "Conclusion\n\nHopefully you've seen how powerful the combination of dbt Python models and\nthe Snowflake platform can be! By supporting both SQL and Python based\ntransformations in dbt, data engineers can take advantage of both while\nbuilding robust data pipelines. While most of the time the choice between SQL\nor DataFrames is simply a matter of preference, as we discussed in the\nintroduction there are use cases where data transformations can't be expressed\nin SQL. In these cases data engineers can make use of tools available in the\nopen source Python ecosystem, including state-of-the-art packages for data\nscience and statistics.\n\nAnd best of all, the Snowflake platform enables this with native Python\nsupport and rich Snowpark API for Python. This eliminates the need for data\nengineers to maintain and pay for separate infrastructure/services to run\nPython code. Snowflake manages all of it for you with the ease of use that you\nwould expect from Snowflake!\n\nData engineering with Snowflake and dbt just got easier!", "start_char_idx": 2, "end_char_idx": 1025, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b922dca-78a7-49cc-81e5-6702aebc9b02": {"__data__": {"id_": "9b922dca-78a7-49cc-81e5-6702aebc9b02", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a8eaabf-e0df-4ee8-85af-aead151d7ebc", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "9adcc19f5e155554334ffc9b1420678685bfb85688a99c53b671690bfe8337a2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c85882ad-9aa3-4a3c-a743-b95c65aa1f3d", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "74c5cc5640b60c3507e9f50c8052357efe5bc420f6268994cd15ca466f4839f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "125feea5-8124-4058-8b1b-e4f43296f1d6", "node_type": "1", "metadata": {}, "hash": "1deae9677ad345579fc75ee5179687029cbf00bda89dcc9a16ec7d69de2ed6d8", "class_name": "RelatedNodeInfo"}}, "text": "Some Tips for Debugging dbt Python Models\n\nThere are a few ways to debug your dbt Python models.\n\nThe first tip is to build and debug via dbt. This is probably the default for\nmost people. With this approach you edit the dbt Python model code directly in\nthe dbt model file and execute it via the `dbt run` command (and it's often\nbest to use the `dbt run --model ` syntax to only run the model in\nconsideration). But this can be a bit time consuming and require you to flip\nback and forth between your dbt and Snowflake.\n\nThe second tip is to build and debug the model code directly in Python. Like I\nmentioned in Section 1, I personally like Visual Studio Code and the [the\nPython extension from\nMicrosoft](https://marketplace.visualstudio.com/items?itemName=ms-\npython.python), but you can use whatever Python IDE you prefer. Regardless,\nthe idea here is to copy/paste the contents of the dbt generated Python code\ninto a temporary Python script as a starting point then edit/debug until it's\nright and finally paste just the contents of the model back into your dbt\nmodel file. As we saw in section 4 of this Quickstart you can get the dbt\ngenerated Python code by compiling the model and looking at the compiled\nscript. There is even commented out code in the dbt generated Python code to\nhelp get you started.\n\nHope that helps!", "start_char_idx": 2, "end_char_idx": 1335, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "125feea5-8124-4058-8b1b-e4f43296f1d6": {"__data__": {"id_": "125feea5-8124-4058-8b1b-e4f43296f1d6", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "64a779fd-74a5-4fe1-8536-04937d022f0f", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "db4d6f2fb369ad56d0de2c562fd3007ebcfc2b788e131a4dd6062915f5afd959", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b922dca-78a7-49cc-81e5-6702aebc9b02", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "a63d56229a733dd935789e547f878481154f02b96dc87b4782df03c17dd1700a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c33a5a18-dc83-402e-94a1-b106c02e6570", "node_type": "1", "metadata": {}, "hash": "1cb0c9ad4a75eef404242a1ae9f11517a2bceed7549a008089de46d5242cce92", "class_name": "RelatedNodeInfo"}}, "text": "What We've Covered\n\n  * The basics of Snowpark Python\n  * How to create Python-based models in dbt\n  * How to create and use Python UDFs in your dbt Python model\n  * How the integration between Snowpark Python and dbt's Python models works", "start_char_idx": 2, "end_char_idx": 241, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c33a5a18-dc83-402e-94a1-b106c02e6570": {"__data__": {"id_": "c33a5a18-dc83-402e-94a1-b106c02e6570", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7732490d-fd9a-4f47-91ca-8c28687a3ff6", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "ff17511f9c4a11962d36d522d870aa0be79ac46cda213a454a85b920b90ef5e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "125feea5-8124-4058-8b1b-e4f43296f1d6", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "b52bc7b64c8f6d2247e0029127448111f44cd2bc356af9c845926c61f33cea1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f78f606e-2762-4d36-879a-d732473a858d", "node_type": "1", "metadata": {}, "hash": "71185ade78b2b71e88bd7f19fd59e7f3c1cf95865865257bfc89adfab073f71a", "class_name": "RelatedNodeInfo"}}, "text": "Related Resources\n\n  * Snowpark Developer Guide for Python\n  * Snowpark API Reference (Python)\n  * dbt Python models", "start_char_idx": 2, "end_char_idx": 118, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f78f606e-2762-4d36-879a-d732473a858d": {"__data__": {"id_": "f78f606e-2762-4d36-879a-d732473a858d", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3110282f-d8ee-4b21-8212-1e96891acd23", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "7df64cc276684e509927e0190cd028e581b5cd318fae3f76ace17e5a5207eed3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c33a5a18-dc83-402e-94a1-b106c02e6570", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "f20be59950a026143d6354c460c0f4504ca12bad216963f3f4759562ab195dee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86975a3e-7d1b-4457-bc94-9596cbfe77fc", "node_type": "1", "metadata": {}, "hash": "54a3d8addfd2f205b43051640b3794b1416854ff5d49ddadd5dac74fcffa1a53", "class_name": "RelatedNodeInfo"}}, "text": "What is Snowpark?\n\nSnowpark is the set of libraries and runtimes that securely enable developers\nto deploy and process Python code in Snowflake.\n\n**Client Side Libraries** \\- Snowpark libraries can be installed and\ndownloaded from any client-side notebook or IDE and are used for code\ndevelopment and deployment. Libraries include the Snowpark API for data\npipelines and apps and the Snowpark ML API for end to end machine learning.\n\n**Elastic Compute Runtimes** \\- Snowpark provides elastic compute runtimes for\nsecure execution of your code in Snowflake. Runtimes include Python, Java, and\nScala in virtual warehouses with CPU compute or Snowpark Container Services\n(public preview) to execute any language of choice with CPU or GPU compute.\n\nLearn more about Snowpark.", "start_char_idx": 2, "end_char_idx": 773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86975a3e-7d1b-4457-bc94-9596cbfe77fc": {"__data__": {"id_": "86975a3e-7d1b-4457-bc94-9596cbfe77fc", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cfab389d-3054-4336-8e55-26d73f4cea03", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "c7d7e06f6c658750931c78af45d5cdfe6820456b4901fbfcd946fb30faf7acfc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f78f606e-2762-4d36-879a-d732473a858d", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "0a3339a64204e83726c0e4ef4d7c8bada6d31e6346d6713d235e473d7802188b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5252813-0706-4e98-b55f-88c21063a4fd", "node_type": "1", "metadata": {}, "hash": "1f12078b53f441a5e545fff1eafd65a6a298f54cf3daedec0637492d55bf8512", "class_name": "RelatedNodeInfo"}}, "text": "What is Snowpark ML?\n\n[Snowpark ML](https://docs.snowflake.com/en/developer-guide/snowpark-\nml/index?_fsi=g3LX4YOG) includes the Python library and underlying\ninfrastructure for end-to-end ML workflows in Snowflake. With Snowpark ML,\ndata scientists and ML engineers can use familiar Python frameworks for\npreprocessing, feature engineering, and training models that can be managed\nentirely in Snowflake without any data movement, silos or governance trade-\noffs. Snowpark ML has 2 components: Snowpark ML Modeling for model development\nand Snowpark ML Operations including the Snowpark Model Registry (public\npreview) for model management and batch inference.\n\n!Snowpark\n\nThis quickstart will focus on\n\n  * Snowpark ML Modeling API, which enables the use of popular Python ML frameworks, such as scikit-learn and XGBoost, for feature engineering and model training without the need to move data out of Snowflake.\n  * Snowpark Model Registry, which provides scalable and secure model management of ML models in Snowflake, regardless of origin. Using these features, you can build and operationalize a complete ML workflow, taking advantage of Snowflake's scale and security features.\n\n**Feature Engineering and Preprocessing** \\- Improve performance and\nscalability with distributed execution for common scikit-learn preprocessing\nfunctions.\n\n**Model Training** \\- Accelerate model training for scikit-learn, XGBoost and\nLightGBM models without the need to manually create stored procedures or user-\ndefined functions (UDFs), and leverage distributed hyperparameter optimization\n(public preview).\n\n!Snowpark\n\n**Model Management and Batch Inference** \\- Manage several types of ML models\ncreated both within and outside Snowflake and execute batch inference.\n\n!Snowpark", "start_char_idx": 2, "end_char_idx": 1770, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5252813-0706-4e98-b55f-88c21063a4fd": {"__data__": {"id_": "c5252813-0706-4e98-b55f-88c21063a4fd", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b92cffaa-499b-418b-8857-39ccd5a2202c", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "2a758ba6fc90d53f2b06a837d959bc0cea3d80421dbac852022d2f9196b9a89c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "86975a3e-7d1b-4457-bc94-9596cbfe77fc", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "5d5074acc09a1c3fc82dbae067a4f7e9b3e3150045a135037c0ac5e4fdf36bdd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c999d6c-441a-4664-9efa-2c6fa51890df", "node_type": "1", "metadata": {}, "hash": "3b535de99de3e290a31d7ce93091031a044cbc86c3de20ddd66a6cce48cb97b8", "class_name": "RelatedNodeInfo"}}, "text": "What is Streamlit?\n\nStreamlit enables data scientists and Python developers to combine Streamlit's\ncomponent-rich, open-source Python library with the scale, performance, and\nsecurity of the Snowflake platform.\n\nLearn more about [Streamlit](https://www.snowflake.com/en/data-\ncloud/overview/streamlit-in-snowflake/).", "start_char_idx": 2, "end_char_idx": 318, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c999d6c-441a-4664-9efa-2c6fa51890df": {"__data__": {"id_": "3c999d6c-441a-4664-9efa-2c6fa51890df", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c8a10ff8-5321-47d6-90dc-8e262e259895", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "c6d24bb6777d326ef1c673b0fb604a211a7969d6844dbe56fb0ce771dcd16c03", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5252813-0706-4e98-b55f-88c21063a4fd", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "8fc2dd327180fa0b03e85a7a55f5cadfb7c256001b02b8ca30477d8f73cfd9ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33f79220-28b9-42a4-93a1-66fe1ea9c8e7", "node_type": "1", "metadata": {}, "hash": "f389c4fd737879aba915cae87a21b256bb40a627e4d057302d2263dbdd5e3be9", "class_name": "RelatedNodeInfo"}}, "text": "What You Will Learn\n\n  * How to analyze data and perform data engineering tasks using Snowpark DataFrames and APIs\n  * How to use open-source Python libraries from curated Snowflake Anaconda channel\n  * How to train ML model using Snowpark ML in Snowflake\n  * How to create Scalar and Vectorized Snowpark Python User-Defined Functions (UDFs) for online and offline inference respectively\n  * How to create Snowflake Tasks to automate data pipelines\n  * How to create Streamlit application that uses the Scalar UDF for inference based on user input", "start_char_idx": 2, "end_char_idx": 549, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33f79220-28b9-42a4-93a1-66fe1ea9c8e7": {"__data__": {"id_": "33f79220-28b9-42a4-93a1-66fe1ea9c8e7", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a6bf2401-22f1-4ba2-b289-c183ea1a9203", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "d9990b4e7912073c31c33de09fd86f4c506a5727b00b6a26c3abbf406d1bd0a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c999d6c-441a-4664-9efa-2c6fa51890df", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "e615edfc39a4dea79292fe5b7aac405e96a949c38877dfa973f69986d49419a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e834e703-b659-4e4b-8b86-a725252722d1", "node_type": "1", "metadata": {}, "hash": "81cd1a5b5471b9641e2fd6e1cd3922dc312dbbdb34553459f1b0f51ed0cf6c7e", "class_name": "RelatedNodeInfo"}}, "text": "Prerequisites\n\n  * Git installed\n  * Python 3.9 installed \n    * Note that you will be creating a Python environment with 3.9 in the **Get Started** step\n  * A Snowflake account with Anaconda Packages enabled by ORGADMIN. If you do not have a Snowflake account, you can register for a free trial account.\n  * A Snowflake account login with ACCOUNTADMIN role. If you have this role in your environment, you may choose to use it. If not, you will need to 1) Register for a free trial, 2) Use a different role that has the ability to create database, schema, tables, stages, tasks, user-defined functions, and stored procedures OR 3) Use an existing database and schema in which you are able to create the mentioned objects.\n\nIMPORTANT: Before proceeding, make sure you have a Snowflake account with\nAnaconda packages enabled by ORGADMIN as described\n[here](https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-\npackages#getting-started).", "start_char_idx": 2, "end_char_idx": 952, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e834e703-b659-4e4b-8b86-a725252722d1": {"__data__": {"id_": "e834e703-b659-4e4b-8b86-a725252722d1", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "88a94c02-45ac-4f28-9aca-67851e7c1bc4", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "5e5499cae169c1fc8ae35b5c3ec5857da68cb5c12344e1b524686f88eb8e4753", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33f79220-28b9-42a4-93a1-66fe1ea9c8e7", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "31bed298a27860cdbacbfad65a8aa1895e54390800f6a446ce51755bd3a56646", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce35d7d2-ce7f-4e50-8ac6-a981c68715c5", "node_type": "1", "metadata": {}, "hash": "c78ad7bf139138ec0fcf0ed0bc0f95c0d8295c6db7cd04fabb9173d05b54d7b9", "class_name": "RelatedNodeInfo"}}, "text": "Create Tables, Load Data and Setup Stages\n\nLog into [Snowsight](https://docs.snowflake.com/en/user-guide/ui-\nsnowsight.html#) using your credentials to create tables, load data from\nAmazon S3, and setup Snowflake internal stages.\n\nIMPORTANT:\n\n  * If you use different names for objects created in this section, be sure to update scripts and code in the following sections accordingly.\n  * For each SQL script block below, select all the statements in the block and execute them top to bottom.\n\nRun the following SQL commands to create the\n[warehouse](https://docs.snowflake.com/en/sql-reference/sql/create-\nwarehouse.html), [database](https://docs.snowflake.com/en/sql-\nreference/sql/create-database.html) and\nschema.\n\n    \n    \n    USE ROLE ACCOUNTADMIN;\n    \n    CREATE OR REPLACE WAREHOUSE DASH_L;\n    CREATE OR REPLACE DATABASE DASH_DB;\n    CREATE OR REPLACE SCHEMA DASH_SCHEMA;\n    \n    USE DASH_DB.DASH_SCHEMA;\n    \n\nRun the following SQL commands to create table **CAMPAIGN_SPEND** from data\nhosted on publicly accessible S3 bucket.\n\n    \n    \n    CREATE or REPLACE file format csvformat\n      skip_header = 1\n      type = 'CSV';\n    \n    CREATE or REPLACE stage campaign_data_stage\n      file_format = csvformat\n      url = 's3://sfquickstarts/ad-spend-roi-snowpark-python-scikit-learn-streamlit/campaign_spend/';\n    \n    CREATE or REPLACE TABLE CAMPAIGN_SPEND (\n      CAMPAIGN VARCHAR(60), \n      CHANNEL VARCHAR(60),\n      DATE DATE,\n      TOTAL_CLICKS NUMBER(38,0),\n      TOTAL_COST NUMBER(38,0),\n      ADS_SERVED NUMBER(38,0)\n    );\n    \n    COPY into CAMPAIGN_SPEND\n      from @campaign_data_stage;\n    \n\nRun the following SQL commands to create table **MONTHLY_REVENUE** from data\nhosted on publicly accessible S3 bucket.\n\n    \n    \n    CREATE or REPLACE stage monthly_revenue_data_stage\n      file_format = csvformat\n      url = 's3://sfquickstarts/ad-spend-roi-snowpark-python-scikit-learn-streamlit/monthly_revenue/';\n    \n    CREATE or REPLACE TABLE MONTHLY_REVENUE (\n      YEAR NUMBER(38,0),\n      MONTH NUMBER(38,0),\n      REVENUE FLOAT\n    );\n    \n    COPY into MONTHLY_REVENUE\n      from @monthly_revenue_data_stage;\n    \n\nRun the following SQL commands to create table **BUDGET_ALLOCATIONS_AND_ROI**\nthat holds the last six months of budget allocations and ROI.\n\n    \n    \n    CREATE or REPLACE TABLE BUDGET_ALLOCATIONS_AND_ROI (\n      MONTH varchar(30),\n      SEARCHENGINE integer,\n      SOCIALMEDIA integer,\n      VIDEO integer,\n      EMAIL integer,\n      ROI float\n    );\n    \n    INSERT INTO BUDGET_ALLOCATIONS_AND_ROI (MONTH, SEARCHENGINE, SOCIALMEDIA, VIDEO, EMAIL, ROI)\n    VALUES\n    ('January',35,50,35,85,8.22),\n    ('February',75,50,35,85,13.90),\n    ('March',15,50,35,15,7.34),\n    ('April',25,80,40,90,13.23),\n    ('May',95,95,10,95,6.246),\n    ('June',35,50,35,85,8.22);\n    \n\nRun the following commands to create Snowflake [internal\nstages](https://docs.snowflake.com/en/user-guide/data-load-local-file-system-\ncreate-stage) for storing Stored Procedures, UDFs, and ML model files.\n\n    \n    \n    CREATE OR REPLACE STAGE dash_sprocs;\n    CREATE OR REPLACE STAGE dash_models;\n    CREATE OR REPLACE STAGE dash_udfs;\n    \n\nOptionally, you can also open [setup.sql](https://github.com/Snowflake-\nLabs/sfguide-ad-spend-roi-snowpark-python-streamlit-scikit-\nlearn/blob/main/setup.sql) in Snowsight and run all SQL statements to create\nthe objects and load data from AWS S3.\n\nIMPORTANT: If you use different names for objects created in this section, be\nsure to update scripts and code in the following sections accordingly.\n\nThis section covers cloning of the GitHub repository and setting up your\nSnowpark for Python environment.", "start_char_idx": 2, "end_char_idx": 3665, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce35d7d2-ce7f-4e50-8ac6-a981c68715c5": {"__data__": {"id_": "ce35d7d2-ce7f-4e50-8ac6-a981c68715c5", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "90056080-e722-4354-afc0-ccce76552a66", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "48e10b6fbc6eb59ae7befd74d12952f3f2ee821640164850c669e3e4046863e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e834e703-b659-4e4b-8b86-a725252722d1", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "898553751ac34c86b9b6cc8dfeb618339f6b012331c99559e3aa1ee3f318430c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "271baa91-1e12-4763-bc0a-c94bf11c8df7", "node_type": "1", "metadata": {}, "hash": "e497db46a82307ee043853ec353b4f4b918b74ee76a13c8e911113c73001fceb", "class_name": "RelatedNodeInfo"}}, "text": "Clone GitHub Repository\n\nThe very first step is to clone the [GitHub\nrepository](https://github.com/Snowflake-Labs/sfguide-ad-spend-roi-snowpark-\npython-streamlit-scikit-learn). This repository contains all the code you will\nneed to successfully complete this QuickStart Guide.\n\nUsing HTTPS:\n\n    \n    \n    git clone https://github.com/Snowflake-Labs/sfguide-getting-started-dataengineering-ml-snowpark-python.git\n    \n\nOR, using SSH:\n\n    \n    \n    git clone git@github.com:Snowflake-Labs/sfguide-getting-started-dataengineering-ml-snowpark-python.git", "start_char_idx": 2, "end_char_idx": 554, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "271baa91-1e12-4763-bc0a-c94bf11c8df7": {"__data__": {"id_": "271baa91-1e12-4763-bc0a-c94bf11c8df7", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "10cddbf2-38d8-4e46-b8cf-94fa7561d4e1", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "8fcc46d3eb408769bf03f079f9578911f571a04878b099c9a55747f32959d947", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce35d7d2-ce7f-4e50-8ac6-a981c68715c5", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "2781f13a6e9e5aaaf364daf3a731c76844071f68a2c1bfc0770a25b67d000969", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c79111e-5991-4052-b7ef-924f5419bbf0", "node_type": "1", "metadata": {}, "hash": "51be957c90e7d96ceb3167cbe04d9ef6f77afe1c961ab340ff494a0aac54f6bd", "class_name": "RelatedNodeInfo"}}, "text": "Snowpark for Python\n\nTo complete the **Data Engineering** and **Machine Learning** steps, you have\nthe option to either install everything locally (option 1) or use Hex (option\n2) as described below.", "start_char_idx": 2, "end_char_idx": 201, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c79111e-5991-4052-b7ef-924f5419bbf0": {"__data__": {"id_": "8c79111e-5991-4052-b7ef-924f5419bbf0", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82d51892-61ce-437b-a2dd-067ec10b6155", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "722d0c40490bffb75f3c211eeeea6da5755595024ef4253d97f511b469ae755c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "271baa91-1e12-4763-bc0a-c94bf11c8df7", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "36a689eef3e64620e342513bfbd142dec3304e9799031babd666b7b7122c4844", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6b4d19e-b174-447f-85dc-8a8662fbd397", "node_type": "1", "metadata": {}, "hash": "ccddfb15a38849e1139433272b087318aaeb49d8f654292261439d92b78efe68", "class_name": "RelatedNodeInfo"}}, "text": "Option 1 \u2013 Local Installation\n\n**Step 1:** Download and install the miniconda installer from\n. _(OR, you may use any other Python\nenvironment with Python 3.9, for example,_[\n_virtualenv_](https://virtualenv.pypa.io/en/latest/) _)_.\n\n**Step 2:** Open a new terminal window and execute the following commands in\nthe same terminal window.\n\n**Step 3:** Create Python 3.9 conda environment called **snowpark-de-ml** by\nrunning the following command in the same terminal window\n\n    \n    \n    conda create --name snowpark-de-ml -c https://repo.anaconda.com/pkgs/snowflake python=3.9\n    \n\n**Step 4:** Activate conda environment **snowpark-de-ml** by running the\nfollowing command in the same terminal window\n\n    \n    \n    conda activate snowpark-de-ml\n    \n\n**Step 5:** Install Snowpark Python, Snowpark ML, and other libraries in conda\nenvironment **snowpark-de-ml** from [Snowflake Anaconda\nchannel](https://repo.anaconda.com/pkgs/snowflake/) by running the following\ncommand in the same terminal window\n\n    \n    \n    conda install -c https://repo.anaconda.com/pkgs/snowflake snowflake-snowpark-python snowflake-ml-python pandas notebook cachetools\n    \n\n**Step 6:** Update [connection.json](https://github.com/Snowflake-\nLabs/sfguide-ml-model-snowpark-python-scikit-learn-\nstreamlit/blob/main/connection.json) with your Snowflake account details and\ncredentials.\n\nHere's a sample **_connection.json_** based on the object names mentioned in\n**Setup Environment** step.\n\n    \n    \n    {\n      \"account\"   : \"\",\n      \"user\"      : \"\",\n      \"password\"  : \"\",\n      \"role\"      : \"ACCOUNTADMIN\",\n      \"warehouse\" : \"DASH_L\",\n      \"database\"  : \"DASH_DB\",\n      \"schema\"    : \"DASH_SCHEMA\"\n    }\n    \n\nNote: For the **account** parameter above, specify your **account identifier**\nand do not include the snowflakecomputing.com domain name. Snowflake\nautomatically appends this when creating the connection. For more details on\nthat, [refer to the documentation](https://docs.snowflake.com/en/user-\nguide/admin-account-identifier.html).", "start_char_idx": 2, "end_char_idx": 2035, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6b4d19e-b174-447f-85dc-8a8662fbd397": {"__data__": {"id_": "a6b4d19e-b174-447f-85dc-8a8662fbd397", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3a6a55f1-6d9c-4dee-b23e-ddbf9db019ca", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "b389160e7882cef3d0b8fe3d9bac9d428a1b4c97701b3a9ba4890359835f37f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c79111e-5991-4052-b7ef-924f5419bbf0", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "8d91c2c210263df810447aeab8bad68e248e50b11255e6f1d4d7c2491ce7b555", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb7c2f4d-2930-4f36-9791-7bd99cf21736", "node_type": "1", "metadata": {}, "hash": "a69712af3759a6bd2161a6eba93bbb4335ea47266b5f3377faa355af3950ea41", "class_name": "RelatedNodeInfo"}}, "text": "Option 2 \u2013 Use Hex\n\nIf you choose to use your existing Hex account\nor [create a free 30-day trial\naccount](https://app.hex.tech/signup/quickstart-30), then Snowpark for Python\nis built-in so you don't have to create a Python environment and install\nSnowpark for Python along with other libraries locally on your laptop. This\nwill enable you to complete **Data Engineering** and **Machine Learning**\nsteps of this QuickStart Guide directly in Hex. (See the respective steps for\ndetails on loading the Data Engineering and Machine Learning notebooks in\nHex.)\n\nThe Notebook linked below covers the following data engineering tasks.\n\n  1. Establish secure connection from Snowpark Python to Snowflake\n  2. Load data from Snowflake tables into Snowpark DataFrames\n  3. Perform Exploratory Data Analysis on Snowpark DataFrames\n  4. Pivot and Join data from multiple tables using Snowpark DataFrames\n  5. Automate data pipeline tasks using Snowflake Tasks", "start_char_idx": 2, "end_char_idx": 950, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb7c2f4d-2930-4f36-9791-7bd99cf21736": {"__data__": {"id_": "bb7c2f4d-2930-4f36-9791-7bd99cf21736", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00af1a4e-94cf-40de-830d-d93570989fa4", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "831455ef71091f233921580d199f715ad39b6fb9e03c60adb2318cd8b580e249", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6b4d19e-b174-447f-85dc-8a8662fbd397", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "1708815dbdee4db39fb1362dd7aa5155fcbc791593c3ec166e45ecebe4ff154b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8075e816-806e-44aa-84a6-53e17241c595", "node_type": "1", "metadata": {}, "hash": "0229326d4f247144719abb260f6aa0dba6aa675fd5c1f514cc752573dce6d4df", "class_name": "RelatedNodeInfo"}}, "text": "Data Engineering Notebook in Jupyter or Visual Studio Code\n\nTo get started, follow these steps:\n\n  1. In a terminal window, browse to this folder and run `jupyter notebook` at the command line. (You may also use other tools and IDEs such Visual Studio Code.)\n  2. Open and run through the cells in Snowpark_For_Python_DE.ipynb\n\nIMPORTANT: Make sure in the Jupyter notebook the (Python) kernel is set to\n**_snowpark-de-ml_** \u2013 which is the name of the environment created in **Clone\nGitHub Repository** step.", "start_char_idx": 2, "end_char_idx": 509, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8075e816-806e-44aa-84a6-53e17241c595": {"__data__": {"id_": "8075e816-806e-44aa-84a6-53e17241c595", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "273d027a-3b3e-41c9-b6d6-ea8efc3a679c", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "60190e076b252ad2bbc5b9ccc401279caa2ab763f80d8f2949ab6d26daa51a48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb7c2f4d-2930-4f36-9791-7bd99cf21736", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "d0b4c30d603ac3485f5c9a976d0a21d8b907fbd85322d947dab0825f0b590ad8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e94555c-c2f3-4b25-80e5-1270ea419acd", "node_type": "1", "metadata": {}, "hash": "804592da07100d36fcb2f6a669804d688dcd98d526e2da03851ecc1bd4fc1209", "class_name": "RelatedNodeInfo"}}, "text": "Data Engineering Notebook in Hex\n\nIf you choose to use your existing Hex account\nor [create a free 30-day trial\naccount](https://app.hex.tech/signup/quickstart-30), follow these steps to\nload the notebook and create a data connection to connect to Snowflake from\nHex.\n\n  1. Import Snowpark_For_Python_DE.ipynb as a Project in your account. For more information on importing, refer to the docs.\n  2. Then, instead of using the connection.json to connect to Snowflake, create a Data Connection and use that in the Data Engineering Notebook as shown below.\n\n!HEX Data Connection\n\nNote: You can also create shared data connections for your projects and users\nin your workspace. For more details, refer to the\n[docs](https://learn.hex.tech/docs/administration/workspace_settings/workspace-\nassets#shared-data-connections).\n\n  3. Replace the following code snippet in the notebook\n\n    \n    \n    connection_parameters = json.load(open('connection.json'))\n    session = Session.builder.configs(connection_parameters).create()\n    \n\n**with...**\n\n    \n    \n    import hextoolkit\n    hex_snowflake_conn = hextoolkit.get_data_connection('YOUR_DATA_CONNECTION_NAME')\n    session = hex_snowflake_conn.get_snowpark_session()\n    session.sql('USE SCHEMA DASH_SCHEMA').collect()\n    \n\nYou can also operationalize the data transformations in the form of automated\ndata pipelines running in Snowflake.\n\nIn particular, in the [Data Engineering\nNotebook](https://github.com/Snowflake-Labs/sfguide-ad-spend-roi-snowpark-\npython-streamlit-scikit-learn/blob/main/Snowpark_For_Python_DE.ipynb), there's\na section that demonstrates how to optionally build and run the data\ntransformations as [Snowflake Tasks](https://docs.snowflake.com/en/user-\nguide/tasks-intro).\n\nFor reference purposes, here are the code snippets.", "start_char_idx": 2, "end_char_idx": 1795, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e94555c-c2f3-4b25-80e5-1270ea419acd": {"__data__": {"id_": "3e94555c-c2f3-4b25-80e5-1270ea419acd", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bc258d10-f3f1-4693-b227-973ff974f16d", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "f9f8cd03dd114e58ca09b1471f78173edb572be1b4a56c02394a5ff00a559b43", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8075e816-806e-44aa-84a6-53e17241c595", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "3d0511323fc585984836ed5c438eac31896e586ad940aa50edcf0913eacbc9da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a00af8b-eee8-4443-90ad-2d51f4a83d57", "node_type": "1", "metadata": {}, "hash": "640c450dc66364a5fbfa1a2e0e49466a5c066d186ee949587f476dd009d2959d", "class_name": "RelatedNodeInfo"}}, "text": "**Root/parent Task**\n\nThis task automates loading campain spend data and performing various\ntransformations.\n\n    \n    \n    def campaign_spend_data_pipeline(session: Session) -> str:\n      # DATA TRANSFORMATIONS\n      # Perform the following actions to transform the data\n    \n      # Load the campaign spend data\n      snow_df_spend_t = session.table('campaign_spend')\n    \n      # Transform the data so we can see total cost per year/month per channel using group_by() and agg() Snowpark DataFrame functions\n      snow_df_spend_per_channel_t = snow_df_spend_t.group_by(year('DATE'), month('DATE'),'CHANNEL').agg(sum('TOTAL_COST').as_('TOTAL_COST')).\\\n          with_column_renamed('\"YEAR(DATE)\"',\"YEAR\").with_column_renamed('\"MONTH(DATE)\"',\"MONTH\").sort('YEAR','MONTH')\n    \n      # Transform the data so that each row will represent total cost across all channels per year/month using pivot() and sum() Snowpark DataFrame functions\n      snow_df_spend_per_month_t = snow_df_spend_per_channel_t.pivot('CHANNEL',['search_engine','social_media','video','email']).sum('TOTAL_COST').sort('YEAR','MONTH')\n      snow_df_spend_per_month_t = snow_df_spend_per_month_t.select(\n          col(\"YEAR\"),\n          col(\"MONTH\"),\n          col(\"'search_engine'\").as_(\"SEARCH_ENGINE\"),\n          col(\"'social_media'\").as_(\"SOCIAL_MEDIA\"),\n          col(\"'video'\").as_(\"VIDEO\"),\n          col(\"'email'\").as_(\"EMAIL\")\n      )\n    \n      # Save transformed data\n      snow_df_spend_per_month_t.write.mode('overwrite').save_as_table('SPEND_PER_MONTH')\n    \n    # Register data pipelining function as a Stored Procedure so it can be run as a task\n    session.sproc.register(\n      func=campaign_spend_data_pipeline,\n      name=\"campaign_spend_data_pipeline\",\n      packages=['snowflake-snowpark-python'],\n      is_permanent=True,\n      stage_location=\"@dash_sprocs\",\n      replace=True)\n    \n    campaign_spend_data_pipeline_task = \"\"\"\n    CREATE OR REPLACE TASK campaign_spend_data_pipeline_task\n        WAREHOUSE = 'DASH_L'\n        SCHEDULE  = '3 MINUTE'\n    AS\n        CALL campaign_spend_data_pipeline()\n    \"\"\"\n    session.sql(campaign_spend_data_pipeline_task).collect()", "start_char_idx": 2, "end_char_idx": 2159, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a00af8b-eee8-4443-90ad-2d51f4a83d57": {"__data__": {"id_": "2a00af8b-eee8-4443-90ad-2d51f4a83d57", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3239be1b-1c0e-4a4a-8361-663be0ed3c73", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "2e23a4f152fac7613968a58318162cf729ac04c3f72269bb7364ba62e8b4b754", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e94555c-c2f3-4b25-80e5-1270ea419acd", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "7c47fd32dbb7a9f15e27605f827677a84aab36684aba91c6177c0f8f38785644", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2804681c-94ad-47c3-96b5-1b687eb390ae", "node_type": "1", "metadata": {}, "hash": "9dd812b7929770a019723f6000d3eba2d27f61c630fd69683e39228e364116a0", "class_name": "RelatedNodeInfo"}}, "text": "**Child/dependant Task**\n\nThis task automates loading monthly revenue data, performing various\ntransformations, and joining it with transformed campaign spend data.\n\n    \n    \n    def monthly_revenue_data_pipeline(session: Session) -> str:\n      # Load revenue table and transform the data into revenue per year/month using group_by and agg() functions\n      snow_df_spend_per_month_t = session.table('spend_per_month')\n      snow_df_revenue_t = session.table('monthly_revenue')\n      snow_df_revenue_per_month_t = snow_df_revenue_t.group_by('YEAR','MONTH').agg(sum('REVENUE')).sort('YEAR','MONTH').with_column_renamed('SUM(REVENUE)','REVENUE')\n    \n      # Join revenue data with the transformed campaign spend data so that our input features (i.e. cost per channel) and target variable (i.e. revenue) can be loaded into a single table for model training\n      snow_df_spend_and_revenue_per_month_t = snow_df_spend_per_month_t.join(snow_df_revenue_per_month_t, [\"YEAR\",\"MONTH\"])\n    \n      # SAVE in a new table for the next task\n      snow_df_spend_and_revenue_per_month_t.write.mode('overwrite').save_as_table('SPEND_AND_REVENUE_PER_MONTH')\n    \n    # Register data pipelining function as a Stored Procedure so it can be run as a task\n    session.sproc.register(\n      func=monthly_revenue_data_pipeline,\n      name=\"monthly_revenue_data_pipeline\",\n      packages=['snowflake-snowpark-python'],\n      is_permanent=True,\n      stage_location=\"@dash_sprocs\",\n      replace=True)\n    \n    monthly_revenue_data_pipeline_task = \"\"\"\n      CREATE OR REPLACE TASK monthly_revenue_data_pipeline_task\n          WAREHOUSE = 'DASH_L'\n          AFTER campaign_spend_data_pipeline_task\n      AS\n          CALL monthly_revenue_data_pipeline()\n      \"\"\"\n    session.sql(monthly_revenue_data_pipeline_task).collect()\n    \n\nNote: In the **_monthly_revenue_data_pipeline_task_** above, notice the\n**AFTER campaign_spend_data_pipeline_task** clause which makes it a dependant\ntask.", "start_char_idx": 2, "end_char_idx": 1966, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2804681c-94ad-47c3-96b5-1b687eb390ae": {"__data__": {"id_": "2804681c-94ad-47c3-96b5-1b687eb390ae", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "546426ca-21a0-47b4-9661-e8f2495d2769", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "725fc7f0dabe6f9c78a85207de4c04eb1ff6321143f09e220876573d5b756d07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a00af8b-eee8-4443-90ad-2d51f4a83d57", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "d31eb14faa5745c540c706c44a7af24da3570095cf9105cd1e85f5f10bb6c4f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fd8d337-8f8a-45d5-ab78-6c640c9c9a2d", "node_type": "1", "metadata": {}, "hash": "72aa1a1ee916719b85145e3bc64d62fada9bd6bd27144eceaf10250fc9c81b30", "class_name": "RelatedNodeInfo"}}, "text": "Start Tasks\n\nSnowflake Tasks are not started by default so you need to execute the\nfollowing statements to start/resume them.\n\n    \n    \n    session.sql(\"alter task monthly_revenue_data_pipeline_task resume\").collect()\n    session.sql(\"alter task campaign_spend_data_pipeline_task resume\").collect()", "start_char_idx": 2, "end_char_idx": 301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3fd8d337-8f8a-45d5-ab78-6c640c9c9a2d": {"__data__": {"id_": "3fd8d337-8f8a-45d5-ab78-6c640c9c9a2d", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7741213-9ceb-4385-bf59-62411fa9d8d2", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "fcd22d1702790d82845812a004434a6eeae541dd5cd216202df76f0baf553404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2804681c-94ad-47c3-96b5-1b687eb390ae", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "cc15a12dcbc01a2353f47324e4b779b19ae7ec1f0498cd7cb101b1425d524e43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be2f5ad6-4c2a-4b91-8c2d-bab368226a1f", "node_type": "1", "metadata": {}, "hash": "15006ddb60254caa89887f55b83b6d657e30e10a891f99837001d95fa89a828b", "class_name": "RelatedNodeInfo"}}, "text": "Suspend Tasks\n\nIf you resume the above tasks, suspend them to avoid unecessary resource\nutilization by executing the following commands.\n\n    \n    \n    session.sql(\"alter task campaign_spend_data_pipeline_task suspend\").collect()\n    session.sql(\"alter task monthly_revenue_data_pipeline_task suspend\").collect()", "start_char_idx": 2, "end_char_idx": 314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be2f5ad6-4c2a-4b91-8c2d-bab368226a1f": {"__data__": {"id_": "be2f5ad6-4c2a-4b91-8c2d-bab368226a1f", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54bd5851-7151-434b-8f49-d024ecfe1222", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "0946cc39d982b56ca4fad750935e7fd16c0a07ea86b0d9c700dc7da7a93d7124", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3fd8d337-8f8a-45d5-ab78-6c640c9c9a2d", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "d384f5732dd85ac1188d2a0c4a2c72e13e32ba9d4bad9a5d3182d6baf191cf8e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4095955f-215a-4210-9fb6-46e1342a3657", "node_type": "1", "metadata": {}, "hash": "244aa9f96b5324254c7650d36ee4f46cfcc15202d1e2b347d0b29864797b37c6", "class_name": "RelatedNodeInfo"}}, "text": "Tasks Observability\n\nThese tasks and their [DAGs](https://docs.snowflake.com/en/user-guide/tasks-\nintro#label-task-dag) can be viewed in\n[Snowsight](https://docs.snowflake.com/en/user-guide/ui-snowsight-\ntasks#viewing-individual-task-graphs) as shown below.\n\n!Tasks-Observability", "start_char_idx": 2, "end_char_idx": 281, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4095955f-215a-4210-9fb6-46e1342a3657": {"__data__": {"id_": "4095955f-215a-4210-9fb6-46e1342a3657", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c5c336bd-8240-44c0-b522-4a6a923c49a4", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "2f137698180702fbf55fc7291ddb1b3a004d04d01a32a6fe99b93a0b2edc8c00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be2f5ad6-4c2a-4b91-8c2d-bab368226a1f", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "e2a9e659b23acb08f94693b2fabb04dfa0217c44e295a32a0893693bd9865775", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9bb76320-5208-4ddd-8b1d-f7ddeb589402", "node_type": "1", "metadata": {}, "hash": "afef9e3c92e6dfb12fc2162b4a21118d1e46428be274b2b16e57b15f48387f45", "class_name": "RelatedNodeInfo"}}, "text": "Error Notificatons For Tasks\n\nYou can also enable push notifications to a cloud messaging service when\nerrors occur while tasks are being executed. For more information, please\nrefer to the [documentation](https://docs.snowflake.com/en/user-guide/tasks-\nerrors).\n\nPREREQUISITE: Successful completion of Data Engineering steps outlined in\n[Snowpark_For_Python_DE.ipynb](https://github.com/Snowflake-Labs/sfguide-ad-\nspend-roi-snowpark-python-streamlit-scikit-\nlearn/blob/main/Snowpark_For_Python_DE.ipynb).\n\nThe Notebook linked below covers the following machine learning tasks.\n\n  1. Establish secure connection from Snowpark Python to Snowflake\n  2. Load features and target from Snowflake table into Snowpark DataFrame\n  3. Prepare features for model training\n  4. Train ML model using Snowpark ML on Snowflake\n  5. Create Scalar and Vectorized (aka Batch) Python User-Defined Functions (UDFs) for inference on new data points for online and offline inference respectively.\n\n!End-To-End-ML", "start_char_idx": 2, "end_char_idx": 993, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9bb76320-5208-4ddd-8b1d-f7ddeb589402": {"__data__": {"id_": "9bb76320-5208-4ddd-8b1d-f7ddeb589402", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "830e2c34-86b8-4a53-b3ac-e69854db7132", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "f21b03e64ee0d2eb5a4ac2983e2cefb3c958ed789dcf52c87a4e6f6795a9f705", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4095955f-215a-4210-9fb6-46e1342a3657", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "549667d58d3ba6d9772028a2a6c1fcf1ad855d1f632853d47e207b6f38fe0ebe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ede56a6c-8e2a-4ca0-aa91-ae59024343f0", "node_type": "1", "metadata": {}, "hash": "1a31cf538eaf371f07976dcd425d71fd5e93b7af12b678c895d0222f543119df", "class_name": "RelatedNodeInfo"}}, "text": "Machine Learning Notebook in Jupyter or Visual Studio Code\n\nTo get started, follow these steps:\n\n  1. In a terminal window, browse to this folder and run `jupyter notebook` at the command line. (You may also use other tools and IDEs such Visual Studio Code.)\n  2. Open and run through the Snowpark_For_Python_ML.ipynb\n\nIMPORTANT: Make sure in the Jupyter notebook the (Python) kernel is set to\n**_snowpark-de-ml_** \u2013 which is the name of the environment created in **Clone\nGitHub Repository** step.", "start_char_idx": 2, "end_char_idx": 500, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ede56a6c-8e2a-4ca0-aa91-ae59024343f0": {"__data__": {"id_": "ede56a6c-8e2a-4ca0-aa91-ae59024343f0", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "953faea1-1bb9-494c-bcd9-642c51d78fc0", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "00098aa08224bc5cbf75d25ad047862f1e9ed7a6b728e3e421282cafe0fe21bf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9bb76320-5208-4ddd-8b1d-f7ddeb589402", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "9e079ed791accbaa6cc78e31541f65dd2e64380171cd36f5643d19bf15192c94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43f8215f-456c-4ff8-a8a5-4f71ca3fd577", "node_type": "1", "metadata": {}, "hash": "3661355cfc56f57f9055916077bb5adf55444b1bf755048cb52f24cba16efafc", "class_name": "RelatedNodeInfo"}}, "text": "Machine Learning Notebook in Hex\n\nIf you choose to use your existing Hex account\nor [create a free 30-day trial\naccount](https://app.hex.tech/signup/quickstart-30), follow these steps to\nload the notebook and create a data connection to connect to Snowflake from\nHex.\n\n  1. Import Snowpark_For_Python_ML.ipynb as a Project in your account. For more information on importing, refer to the docs.\n  2. Then, instead of using the connection.json to connect to Snowflake, create a Data Connection and use that in the Machine Learning Notebook as shown below.\n\n!HEX Data Connection\n\nNote: You can also create shared data connections for your projects and users\nin your workspace. For more details, refer to the\n[docs](https://learn.hex.tech/docs/administration/workspace_settings/workspace-\nassets#shared-data-connections).\n\n  3. Replace the following code snippet in the notebook\n\n    \n    \n    connection_parameters = json.load(open('connection.json'))\n    session = Session.builder.configs(connection_parameters).create()\n    \n\n**with...**\n\n    \n    \n    import hextoolkit\n    hex_snowflake_conn = hextoolkit.get_data_connection('YOUR_DATA_CONNECTION_NAME')\n    session = hex_snowflake_conn.get_snowpark_session()\n    session.sql('USE SCHEMA DASH_SCHEMA').collect()\n    \n\nFollow these steps to build Streamlit application in Snowsight.\n\n**Step 1.** Click on **Streamlit** on the left navigation menu\n\n**Step 2.** Click on **\\+ Streamlit App** on the top right\n\n**Step 3.** Enter **App name**\n\n**Step 4.** Select **Warehouse** (X-Small) and **App location** (Database and\nSchema) where you'd like to create the Streamlit applicaton\n\n**Step 5.** Click on **Create**\n\n  * At this point, you will be provided code for an example Streamlit application\n\n**Step 6.** Replace sample application code displayed in the code editor on\nthe left with the code provided in\n[Snowpark_Streamlit_Revenue_Prediction_SiS.py](https://github.com/Snowflake-\nLabs/sfguide-ad-spend-roi-snowpark-python-streamlit-scikit-\nlearn/blob/main/Snowpark_Streamlit_Revenue_Prediction_SiS.py)\n\n**Step 7.** Click on **Run** on the top right\n\nIf all goes well, you should see the application in Snowsight as shown below.\n\n!Streamlit-in-Snowflake\n\n**Step 8.** Save data to Snowflake\n\nIn the application, adjust the advertising budget sliders to see the predicted\nROI for those allocations. You can also click on **Save to Snowflake** button\nto save the current allocations and predicted ROI into\nBUDGET_ALLOCATIONS_AND_ROI Snowflake table.\n\nIf you started/resumed the two tasks `monthly_revenue_data_pipeline_task` and\n`campaign_spend_data_pipeline_task` as part of the **Data Engineering** or\n**Data Pipelines** sections, then it is important that you run the following\ncommands to suspend those tasks in order to avoid unecessary resource\nutilization.\n\nIn Notebook using Snowpark Python API\n\n    \n    \n    session.sql(\"alter task campaign_spend_data_pipeline_task suspend\").collect()\n    session.sql(\"alter task monthly_revenue_data_pipeline_task suspend\").collect()\n    \n\nIn Snowsight\n\n    \n    \n    alter task campaign_spend_data_pipeline_task suspend;\n    alter task monthly_revenue_data_pipeline_task suspend;\n    \n\nCongratulations! You've successfully performed data engineering tasks and\ntrained a Linear Regression model to predict future ROI (Return On Investment)\nof variable advertising spend budgets across multiple channels including\nSearch, Video, Social Media, and Email using Snowpark for Python and scikit-\nlearn. And then you created a Streamlit application that uses that model to\ngenerate predictions on new budget allocations based on user input.\n\nWe would love your feedback on this QuickStart Guide! Please submit your\nfeedback using this Feedback Form.", "start_char_idx": 2, "end_char_idx": 3738, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43f8215f-456c-4ff8-a8a5-4f71ca3fd577": {"__data__": {"id_": "43f8215f-456c-4ff8-a8a5-4f71ca3fd577", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "719e8fec-2706-4e2a-9bf0-3d0e3d0c224a", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "92192c7e21b5897b19ab3525ff19d0224343b1e36cbddd6ad3aa76406632befa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ede56a6c-8e2a-4ca0-aa91-ae59024343f0", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "dacfcf1daaa3c60dfd43f93dda66dc612a47e1cf9133448124745a7ca3b1d981", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62f4c0bf-3779-4b68-b049-69016c1a821d", "node_type": "1", "metadata": {}, "hash": "9eeff2db15a0af0d8d101e93de217f572593f1d34f26baa8db2c78f1b0780850", "class_name": "RelatedNodeInfo"}}, "text": "What You Learned\n\n  * How to analyze data and perform data engineering tasks using Snowpark DataFrames and APIs\n  * How to use open-source Python libraries from curated Snowflake Anaconda channel\n  * How to train ML model using Snowpark ML in Snowflake\n  * How to create Scalar and Vectorized Snowpark Python User-Defined Functions (UDFs) for online and offline inference respectively\n  * How to create Snowflake Tasks to automate data pipelining and (re)training of the model\n  * How to create Streamlit application that uses the Scalar UDF for inference", "start_char_idx": 2, "end_char_idx": 557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62f4c0bf-3779-4b68-b049-69016c1a821d": {"__data__": {"id_": "62f4c0bf-3779-4b68-b049-69016c1a821d", "embedding": null, "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7c0692e1-a293-42ab-a093-dcd19dc6e376", "node_type": "4", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "24b05f3ca9dd4da69395d1070f44e205a88f3acc6ec9d8fbf61e31039f296834", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43f8215f-456c-4ff8-a8a5-4f71ca3fd577", "node_type": "1", "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}, "hash": "90173e8f6a5daa2abd9c366482b1d0769bb4e8d981d6350a7fa71308a5f90618", "class_name": "RelatedNodeInfo"}}, "text": "Related Resources\n\n  * Source Code on GitHub\n  * Intro to Machine Learning with Snowpark ML\n  * Advanced: Snowpark for Python Data Engineering Guide\n  * Advanced: Snowpark for Python Machine Learning Guide\n  * Snowpark for Python Developer Guide\n  * Snowpark for Python API Reference", "start_char_idx": 2, "end_char_idx": 285, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9873c114-bd9e-4f9d-b8bb-23695668a938": {"__data__": {"id_": "9873c114-bd9e-4f9d-b8bb-23695668a938", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "In this project, you will use a trial Snowflake account with `ACCOUNTADMIN` access and a Matillion account provisioned through Snowflake's partner connect. You will learn how to source 3rd party data from Snowflake data marketplace, use Matillion's GUI to build an end-to-end transformation pipeline, extract real-time data from public APIs using Matillion, and leverage Matillion to scale up/down Snowflake's virtual warehouses. \n\nYou will build an end-to-end data transformation pipeline for Financial Services data using Matillion and Snowflake. The pipeline will leverage different data sources, join and transform the data, and orchestrate the process through a user-friendly GUI service.\n\nThe goal of the project is to aggregate the Profit & Loss of each trader in your team and get a real-time aggregated view of their total realized and unrealized gains/losses. To accomplish this, you will acquire historical data of trades from a S3 bucket, create a transformation pipeline to calculate each trader's PnL, and use the Yahoo Finance API to get real-time stock data.\n\nThe project involves launching a Matillion ETL instance through Snowflake partner connect, ingesting traders' historical data from a S3 bucket into a Snowflake table, and developing a transformation pipeline to calculate each trader's PnL by joining with stock data from Zepl. You will also use the Alter Warehouse component to scale up Snowflake's Virtual Warehouse for resource-heavy transformation jobs.\n\nThe transformation job will aggregate the trading history data to find the number of shares bought/sold and their value. It will calculate the net number of shares and value for each trader and create a table enriched with each trader's average price for each stock.\n\nTo start the project, you will need a trial Snowflake account with `ACCOUNTADMIN` access and a Matillion account provisioned through Snowflake's partner connect. You will use the S3 Load Generator component to ingest traders' historical data from a S3 bucket into a Snowflake table. You will then use the Alter Warehouse component to scale up Snowflake's Virtual Warehouse to accommodate resource-heavy transformation jobs.\n\nIn the transformation job, you will use the Table Input component to read the TRADES_HISTORY table, the Filter component to filter the data based on the type of action (BUY), and the Calculator component to calculate the amount of investment in each buy transaction.\n\nOverall, this project will help you build an end-to-end data transformation pipeline for Financial Services data using Matillion and Snowflake, leveraging different data sources and providing a user-friendly GUI for managing the process.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "962563b4-e4d1-4481-9129-d8ca7ecea48f": {"__data__": {"id_": "962563b4-e4d1-4481-9129-d8ca7ecea48f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The provided information is a set of instructions for creating a data transformation job using Matillion and Snowflake. The job involves aggregating investments made in each stock, filtering and calculating values for buy and sell transactions, joining the aggregations, calculating the amount of investment in each buy transaction, calculating the average price of stocks traded, and rewriting the result to a table called CURRENT_POSITION. The job can be executed by right-clicking on it and selecting \"Run Job\". The end goal of the transformation pipeline is to provide a snapshot of every trader's profit or loss based on their BUY and SELL transactions. Another transformation job called VHOL_PNL_xform is also mentioned, but the instructions for building it are not provided in the given information.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c8b2e19-1275-4764-b65d-81fe58c59e72": {"__data__": {"id_": "5c8b2e19-1275-4764-b65d-81fe58c59e72", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The given information provides instructions for a data engineering process using Matillion and Snowflake. The process involves joining datasets, calculating gains/losses, rewriting tables, aggregating data, and creating a view. \n\n1. The first step is to join the \"CURRENT_POSITION\" and \"STOCK_HISTORY\" datasets. The join is performed on the \"SYMBOL\" column and the join type is left join.\n\n2. Next, the output columns are specified, including columns from both datasets.\n\n3. A calculator component is added to calculate realized and unrealized gains. The calculations are based on the \"NET_SHARES\", \"CLOSE\", and \"AVG_PRICE\" columns.\n\n4. The results are then written to a new table called \"TRADER_PNL_TODAY\" using the Rewrite Table component.\n\n5. An aggregate component is added to sum up the gains by trader. The groupings are done by the \"TRADER\" column and the aggregations are performed on the \"UNREAL_GAINS\" and \"REAL_GAINS\" columns.\n\n6. Finally, a view called \"TRADER_PNL_TOTAL_VIEW\" is created to store the aggregated results.\n\nThe process also includes steps for setting up an orchestration job, using a Python script to format query parameters, pulling current stock price data from an API, and transforming the data.\n\nOverall, the process involves joining, calculating, rewriting, aggregating, and creating a view for stock data in Snowflake using Matillion.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77986709-cd77-49b6-93cd-ba1fdb5283c1": {"__data__": {"id_": "77986709-cd77-49b6-93cd-ba1fdb5283c1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The provided information includes instructions for a data engineering pipeline using Matillion ETL for Snowflake. The pipeline involves reading data from a previous transformation job, filtering for Cersei's trades, joining Cersei's trades with Yahoo API data, calculating win/loss logic, and writing Cersei's profits back to Snowflake. The pipeline is developed using a low-code/no-code platform and allows for scaling up and down within Snowflake. The summary also mentions prerequisites for the pipeline, what will be learned during the process, and the resources needed.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffeab90b-eea4-4615-8495-63a508faca0e": {"__data__": {"id_": "ffeab90b-eea4-4615-8495-63a508faca0e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The summary is about setting up and running data engineering pipelines with Snowpark Python. \n\n1. Fork the Quickstart Repository and Enable GitHub Actions: Create a fork of the repository and enable the workflow in the forked repository.\n\n2. Create GitHub Codespace: Use GitHub Codespaces to create a hosted development environment with a web-based VS Code environment.\n\n3. Configure Snowflake Credentials: Edit the SnowSQL config file to replace the accountname, username, and password with your own values.\n\n4. Verify Your Anaconda Environment is Activated: Make sure the Anaconda environment named \"snowflake-demo\" is activated in your terminal.\n\n5. Snowflake Extensions for VS Code: Use the Snowflake extension for VS Code to run SQL queries against Snowflake.\n\n6. Run the Script: Execute the \"steps/01_setup_snowflake.sql\" script to set up the necessary objects in Snowflake.\n\n7. Run the Script: Execute the \"steps/02_load_raw.py\" script to load raw data from Parquet files into Snowflake.\n\n8. Running Snowpark Python Locally: Run the Snowpark Python code locally from your laptop for local debugging.\n\n9. Viewing What Happened in Snowflake: Check the Query History in Snowflake to see the SQL generated by the Snowpark API.\n\n10. Schema Inference: Use the \"INFER_SCHEMA()\" function in Snowflake to infer the schema of files in a stage.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdb3373c-2c0b-4521-85fe-3c34889fd46e": {"__data__": {"id_": "bdb3373c-2c0b-4521-85fe-3c34889fd46e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The provided information is a collection of snippets from a document titled \"Data Engineering Pipelines with Snowpark Python\". \n\n1. The document discusses the process of loading data into a Snowflake table using the `copy_into_table()` method on a DataFrame. This method creates the target table in Snowflake and uses the highly optimized Snowflake `COPY INTO` command.\n\n2. Snowflake tables can support structured and semi-structured data and are stored in Snowflake's mature cloud table format. This eliminates the need to manage a file-based data lake and allows for secure storage and management of data without worrying about raw files. Data can be transformed and queried using SQL or other languages without the need for separate compute services like Spark.\n\n3. Snowflake's Virtual Warehouse allows for dynamic scaling of compute clusters. Users can resize the compute environment to increase or decrease capacity, and the per-second billing ensures that there are no extra charges for running code in a shorter time.\n\n4. Snowflake Data Marketplace provides access to third-party datasets. Users can connect to specific feeds and access up-to-date data without the need for additional steps to keep the data updated.\n\n5. The document provides instructions on running scripts and queries to load and process data in Snowflake. It also mentions the use of Snowpark DataFrame API to define views and streams for incremental processing.\n\n6. Snowflake streams simplify incremental processing by automatically tracking changes in a table or view. Users can create streams on views and query them to retrieve only the changed records since the last DML operation.\n\n7. The document mentions the ability to create and deploy user-defined functions (UDFs) in Snowflake using Snowpark Python. It provides instructions on running a UDF locally for testing purposes.\n\nOverall, the document highlights the features and advantages of Snowflake for data engineering pipelines, including easy data ingestion, structured and semi-structured data support, dynamic scaling, access to third-party datasets, and incremental processing capabilities.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ad541c3-084f-44dd-8f5a-55480a8140ad": {"__data__": {"id_": "3ad541c3-084f-44dd-8f5a-55480a8140ad", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The provided information is about deploying UDFs (User-Defined Functions) and stored procedures to Snowflake using the SnowCLI tool. \n\nTo deploy a UDF, the SnowCLI tool is used to package the application, copy it to a Snowflake stage, and create the object in Snowflake. The commands \"cd steps/05_fahrenheit_to_celsius_udf\" and \"snow function create\" are used to deploy the UDF. The UDF can be invoked through SQL or using the SnowCLI utility.\n\nThe SnowCLI tool is a command line tool for developers that simplifies the development and deployment of Snowflake objects such as Snowpark Python UDFs and stored procedures. It handles tasks such as dealing with third-party packages, creating a zip file of the project, copying the zip file to the Snowflake stage, and creating the Snowflake function or stored procedure object.\n\nTo deploy a stored procedure, the SnowCLI tool is used similarly to deploying a UDF. The commands \"cd steps/06_orders_update_sp\" and \"snow procedure create\" are used to deploy the stored procedure. The stored procedure can be invoked through SQL or using the SnowCLI utility.\n\nThe Snowpark API is used for data transformations and provides functionality similar to the Spark SQL API. A Snowpark session object needs to be created using the `Session.builder.configs().create()` methods. SQL statements can be issued to Snowflake using the `session.sql()` function.\n\nTo test the stored procedure locally, the script `steps/06_orders_update_sp/app.py` can be executed using the command \"python app.py\". The Python code runs locally, but the Snowpark DataFrame code issues SQL queries to the Snowflake account.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5370f65-bb14-4729-bb9a-54b7e8462aaf": {"__data__": {"id_": "f5370f65-bb14-4729-bb9a-54b7e8462aaf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The summary is about deploying and running a stored procedure (sproc) in Snowflake using the SnowCLI tool. The process involves creating a table with a defined schema, executing a complex aggregation query, and orchestrating tasks to run the sproc. The tasks can be monitored using Snowflake's task observability features. Additionally, new data can be added to the pipeline and processed incrementally using Snowflake's stream/CDC capabilities.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed9f5596-1e66-40db-86ef-b3e9fe17ce29": {"__data__": {"id_": "ed9f5596-1e66-40db-86ef-b3e9fe17ce29", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The provided information includes instructions and steps for various tasks related to data engineering pipelines with Snowpark Python. \n\n1. Viewing the Task History: A query is provided to view the task history, specifically the `ORDERS_UPDATE_TASK` and `DAILY_CITY_METRICS_UPDATE_TASK` tasks.\n\n2. Query History for Tasks: Instructions are given to view the queries executed by tasks, including removing filters and adding a filter option for user tasks.\n\n3. Update the Fahrenheit to Celsius UDF: Changes are made to the `FAHRENHEIT_TO_CELSIUS_UDF()` UDF, including importing a package from `scipy` and modifying the `main()` function.\n\n4. Test your Changes Locally: Instructions are provided to test the UDF locally by executing the `app.py` script and installing required packages.\n\n5. Configuring Your Forked GitHub Project: Steps are given to store Snowflake credentials in GitHub as action secrets for CI/CD pipelines.\n\n6. Push Changes to Forked Repository: Instructions are provided to commit and push changes to the forked GitHub repository.\n\n7. Viewing GitHub Actions Workflow: The workflow code is mentioned, and steps are given to view the results of the workflow run.\n\n8. What we've covered: A summary of the topics covered in the Quickstart, including Snowflake features, Python UDFs, Snowpark DataFrame API, Task Observability, and GitHub Actions integration.\n\n9. Related Resources: A list of related resources, including the Snowflake Demo Hub, source code on GitHub, Snowpark Developer Guide, and related tools like the Snowflake Visual Studio Code Extension and SnowCLI Tool.\n\nOverall, the information provides a step-by-step guide for working with data engineering pipelines using Snowpark Python and integrating with GitHub Actions for CI/CD.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "427bf846-34e9-4338-98a5-513a6d279c05": {"__data__": {"id_": "427bf846-34e9-4338-98a5-513a6d279c05", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "This document provides a guide on data engineering with Apache Airflow. It covers various topics such as using Airflow to create a data scheduler, writing a DAG and uploading it onto Airflow, building scalable pipelines using dbt, Airflow, and Snowflake, and using Snowpark to interact with Snowflake data using Python. \n\nBefore beginning, the following prerequisites are needed: a Snowflake account and user with appropriate permissions, a GitHub account, an IDE with Git integration (such as Visual Studio Code), Docker Desktop installed, an OpenAI API key (optional for chatbot functionality), and the Astro CLI installed.\n\nThe guide then proceeds to explain how to create a working Airflow pipeline with dbt and Snowflake. It provides step-by-step instructions on creating the necessary folders and files, configuring and setting up the dbt project, creating dbt models for the transform and analysis folders, and executing the dbt project using the provided DAG.\n\nFinally, the guide introduces the integration of Snowpark for data analysis with Python. It explains how to modify existing files and add new requirements to the local Airflow environment to incorporate Snowpark. It also provides code for creating a new DAG file called \"cosmosandsnowflake.py\" that combines Cosmos and Snowpark functionalities.\n\nOverall, the guide offers a comprehensive walkthrough of data engineering with Apache Airflow, covering various aspects such as scheduling, pipeline building, data analysis, and integration with Snowflake and Snowpark.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6fdf46bc-37d4-49a5-82c0-8496b204a11e": {"__data__": {"id_": "6fdf46bc-37d4-49a5-82c0-8496b204a11e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "This document provides a guide on data engineering with Apache Airflow, dbt, Snowflake, and Snowpark. It explains how to install the necessary packages and connect to a Snowflake database to fetch data. The guide also covers creating a dbt DAG using Cosmos and running dbt models using Airflow. It provides instructions on setting up a Streamlit dashboard to view analyzed data. The document concludes by mentioning additional resources and providing a summary of the topics covered in the guide.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "685defd6-96f1-4897-9546-f81063fff1f2": {"__data__": {"id_": "685defd6-96f1-4897-9546-f81063fff1f2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The provided information is a guide on data engineering with Snowpark, Python, and dbt. It outlines the requirements for starting the project, including having a Snowflake account, a Snowflake database named DEMO_DB, and a Snowflake user with appropriate permissions. It also mentions the need for Anaconda and dbt to be installed on the computer, as well as a preferred IDE like Visual Studio Code.\n\nThe guide explains how to create a dbt project and run it successfully. It provides an overview of creating a Python model in dbt, including the necessary function and parameters. It then provides an example of creating a Python model file named \"my_first_python_model.py\" and explains the steps to configure it. The guide also mentions that dbt will handle the import of the Snowflake Snowpark Python library and that only table or incremental materializations are supported.\n\nAdditionally, the guide provides an overview of the executed queries when running the \"my_first_python_model\" model. It includes the creation of a stored procedure and the materialization of the model.\n\nThe guide further explains the stored procedure code generated by dbt and emphasizes that all Python code in dbt Python models is executed remotely on the platform. It also mentions the option to define and register user-defined functions (UDFs) within the model.\n\nFinally, the guide mentions the execution of a second Python model with a UDF and provides an example of how to define and register the function within the model.\n\nOverall, the guide provides step-by-step instructions and explanations for data engineering with Snowpark, Python, and dbt, covering the setup, model creation, execution, and UDF implementation.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dcbb48d4-d09d-4dfd-a8f5-8923929cb554": {"__data__": {"id_": "dcbb48d4-d09d-4dfd-a8f5-8923929cb554", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The summary is about the combination of dbt Python models and the Snowflake platform in data engineering. It discusses how data engineers can use both SQL and Python-based transformations in dbt to build robust data pipelines. It mentions that while SQL is often preferred, there are cases where data transformations cannot be expressed in SQL, and Python can be used in those cases. The Snowflake platform supports native Python integration and provides a rich Snowpark API for Python. The summary also provides tips for debugging dbt Python models and lists the topics covered in the article. It mentions related resources such as the Snowpark Developer Guide for Python, Snowpark API Reference, and dbt Python models. Additionally, it introduces Snowpark and Snowpark ML, explaining their features and benefits. It briefly mentions Streamlit and outlines what readers will learn in the article. The summary also includes prerequisites for the tutorial and instructions for creating tables, loading data, and setting up stages in Snowflake.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "409baa8c-2dea-4762-a444-814214cacee9": {"__data__": {"id_": "409baa8c-2dea-4762-a444-814214cacee9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The summary is about the steps required to get started with data engineering and machine learning using Snowpark Python. The first step is to clone the GitHub repository that contains all the necessary code. There are two options for installation: local installation or using Hex. For local installation, the steps include downloading and installing the miniconda installer, creating a Python 3.9 conda environment called \"snowpark-de-ml\", activating the conda environment, and installing Snowpark Python, Snowpark ML, and other libraries. For using Hex, users can create a free trial account and directly use Snowpark for Python without installing it locally. The summary also mentions the steps to follow for running the data engineering notebook in Jupyter or Visual Studio Code, as well as in Hex. It provides instructions for setting up the environment and loading the necessary notebooks. Additionally, the summary includes information about two tasks: the root/parent task that automates loading campaign spend data and performing transformations, and the child/dependent task that automates loading monthly revenue data, performing transformations, and joining it with the campaign spend data. The summary concludes with instructions for starting and suspending tasks in Snowflake.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9836401-2894-468e-8124-9283ba78fc69": {"__data__": {"id_": "f9836401-2894-468e-8124-9283ba78fc69", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "This document provides information on tasks observability in Snowflake, including how to view tasks and their DAGs in Snowsight. It also mentions the ability to enable push notifications for errors during task execution. The document states that successful completion of data engineering steps outlined in the Snowpark_For_Python_DE.ipynb notebook is a prerequisite for the tasks covered in the linked machine learning notebook. The machine learning notebook covers various tasks such as establishing a secure connection from Snowpark Python to Snowflake, loading features and target from a Snowflake table into a Snowpark DataFrame, preparing features for model training, training a machine learning model using Snowpark ML on Snowflake, and creating scalar and vectorized Python user-defined functions for inference on new data points. The document provides instructions for running the machine learning notebook in Jupyter or Visual Studio Code, including setting the Python kernel to \"snowpark-de-ml\". It also provides instructions for running the notebook in Hex, including creating a data connection to Snowflake. The document further provides steps for building a Streamlit application in Snowsight, including replacing the sample application code with code provided in a specific file. The document concludes by listing the key learnings from the tutorial, such as analyzing data and performing data engineering tasks using Snowpark DataFrames and APIs, using open-source Python libraries from the Snowflake Anaconda channel, training ML models using Snowpark ML, creating scalar and vectorized Snowpark Python UDFs, creating Snowflake Tasks for data pipelining and model training, and creating Streamlit applications that use scalar UDFs for inference. Finally, the document provides a list of related resources, including source code on GitHub and various guides and references for Snowpark for Python.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8badfca-c366-4a3c-9068-eea224fb0510": {"__data__": {"id_": "f8badfca-c366-4a3c-9068-eea224fb0510", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The provided information includes instructions and steps for various tasks related to data engineering pipelines with Snowpark Python. The tasks include deploying UDFs and stored procedures using the SnowCLI tool, creating and executing data transformation jobs, and integrating with GitHub Actions for CI/CD. The information also covers topics such as Snowflake features, Snowpark DataFrame API, Task Observability, and using dbt for building scalable pipelines. Overall, the information provides a comprehensive guide for working with data engineering pipelines using Snowpark Python and integrating with other tools and platforms.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f18a2285-c3bf-452d-93fe-8f56cc7b2c9e": {"__data__": {"id_": "f18a2285-c3bf-452d-93fe-8f56cc7b2c9e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The document provides a comprehensive guide on data engineering with Snowpark, Python, and dbt. It covers the setup requirements, model creation, execution, and user-defined function implementation. The guide emphasizes the use of SQL and Python-based transformations in dbt and highlights the native Python integration and Snowpark API provided by the Snowflake platform. It also mentions tips for debugging dbt Python models and lists related resources for further exploration. Additionally, the document introduces Snowpark ML and Streamlit and provides instructions for setting up the environment and running the tutorials. It concludes with a summary of the key learnings and a list of related resources.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"95ad9605-f0d4-4bc3-8aaf-9212c12b29e5": {"node_ids": ["e0cc3c41-cab9-44a8-85fa-1f25a674b638"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "f2ca1935-cb5a-4945-82f5-f27e8b1a6f96": {"node_ids": ["672501e6-4f5e-48ad-9678-43e71b643117"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "4f5c7665-f156-4b41-ad35-8df8f9ad013e": {"node_ids": ["886c95dd-fc49-4176-b86f-a255f6d3d812", "7361ef57-4b45-4d2b-83bf-827b953a57a7"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "0d7d8577-801f-4242-b4fe-8efb3010aebd": {"node_ids": ["7dae2f14-5878-4730-96ff-e152c953c045"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "c5fa9ef1-ac5c-41b9-8ab3-a33983c68cbe": {"node_ids": ["cdc3816a-8895-46ed-978f-6ab3eab87cbe"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "7aba2b2b-a34a-4d73-b81e-7a815a067512": {"node_ids": ["6c52d986-5eae-4ee4-ac40-61dda63ec8c0"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "a56aa6b5-5dfa-43bc-84cf-4dbaadc5138a": {"node_ids": ["cc040959-623c-4040-956b-782c6dd734a4"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "810734c5-8988-406b-84b8-58d212b43716": {"node_ids": ["95d837a6-2e21-467d-ad6a-b19dd90e5eaf"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "b7868653-afc3-467c-b796-4a6382eb5407": {"node_ids": ["e82cf00f-fd12-4db3-a391-bc9f361676f7"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "803d1f18-0bb5-4e29-8f82-7dfb005f7b4b": {"node_ids": ["a8367fbe-ea13-4c0a-a26f-ed5d9093c1aa"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "3fac0a30-ffb1-4e20-aab2-1181fc1ac6e2": {"node_ids": ["14ff226c-ce77-47bf-adf8-2d4bc8714d91"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "7baec97b-de2b-443a-a08e-c9a78636ff2c": {"node_ids": ["483dce61-d77f-4626-b5d0-65cad7dd3eac"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "20054bdd-e5cd-4958-a755-662c0a52a0ae": {"node_ids": ["6c781c20-22e9-4f6c-a8bb-54c760b70d00"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "7557ca6f-4040-4bab-b0ff-59655ed13169": {"node_ids": ["d610c7ff-9365-497d-ad24-d8fa79697341"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "62b3ca07-dcd7-4326-9f1a-f09a1b21b0af": {"node_ids": ["72df2313-fdd4-44c8-b7d7-0631a662bd5d"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "bdd73f33-aba0-48df-a07a-903239b03a0c": {"node_ids": ["ccad89f5-e782-4a9a-b64c-4425f024c759"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "cf3bce4b-776f-46a0-bc31-9e9eb747a0c0": {"node_ids": ["3d622c97-5302-4bbb-9bfd-182b51c66ef8"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "052f529a-4826-4498-af43-c34c1f7f51df": {"node_ids": ["14450234-6df5-461c-a1d4-aabd69f184c2"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "106d0f0a-9104-4283-a354-95c62e57c866": {"node_ids": ["72062af9-3fa2-4e19-9042-58b69d05c586"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "376f4579-e99f-4f91-ab15-163e9fb527f8": {"node_ids": ["04efc28d-9f1f-4d85-8319-2c0a63072f69"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "1e5b25b5-d5cd-4df8-8d39-641ed9f4be5e": {"node_ids": ["806bb1e9-799a-4f9f-9c52-d63b275c6d94"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "186a0c4f-4cb9-4256-a5f4-53c96ddb73f5": {"node_ids": ["d45bbc1f-1d6e-4c89-842c-7f813e91a44d"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "dcf65fc6-f88e-48b4-87f2-1900d41de02b": {"node_ids": ["38eef608-e212-49f5-b019-c3827e95fcc8"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "eba9096b-64db-4b8b-94ab-5f02e1ba42b9": {"node_ids": ["f716fa92-79fa-4e58-9e5d-554f43d5c217"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "434464da-3718-4906-b4a9-4f0dacf6e2af": {"node_ids": ["26bda402-a0b0-4b52-9c33-cb4c37723d1f"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "e9bbc953-907a-46ec-a9c3-31c23d7ea908": {"node_ids": ["6c78f99d-86c0-43f5-bd0b-4679c6062b4b"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "4910572b-9e9c-46f7-b230-12f17e471e52": {"node_ids": ["965b3ea3-b7ef-45e5-a898-2a1919a5f259"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "fc326342-a839-4cc3-90f1-38c1a0e27f01": {"node_ids": ["95ef1cc5-6086-4ed4-a48a-6dd39e3a1c26"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "029099c6-84d0-406d-bef8-cfbbfd73c0b0": {"node_ids": ["05b23e8e-78df-4abf-a5d8-82c2537ba0a0"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "cee08ed0-4513-4e2d-9d2d-a97de599c4ff": {"node_ids": ["b97bdc72-dc8f-498d-af3e-5297e4ce1aa9"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "d44e99ff-027a-440a-a126-6d8b141a29a3": {"node_ids": ["639e14a0-3ccd-4a7b-bd12-7a53bc437331"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "85cebce5-42fb-4a8c-b87e-7e952b0f483b": {"node_ids": ["d50f388c-4891-4cbd-80f9-77f515e1bc6d"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "b5efa891-4a65-4d19-ad79-3adfb61d5a50": {"node_ids": ["eadff117-e0df-49dd-8027-2517eaea58bd"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "fb3db9aa-6583-4276-a46a-10659bd4fc10": {"node_ids": ["ccba5cf1-3d9b-4d8b-b203-a5ec70676e44"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "3e5ff74d-d112-4fb7-8eec-fae6954cd457": {"node_ids": ["3be92cbc-3c55-40c6-8f74-d5449eae7262"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_type": null, "file_size": 39223, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "8e99f547-c15e-4eda-9493-c0c5f39beebb": {"node_ids": ["3e5a72b7-6352-44bb-be25-3272f9ba2341"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "da9b7e1c-d5de-4cd4-9dcb-e3e8a66220be": {"node_ids": ["5b76e307-f671-4a5d-9643-516c3f7c93b0"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "33218217-3d78-4c4a-824f-f82492907e73": {"node_ids": ["76c0efbe-1b9c-4960-9174-35d2ec36abeb"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "20b56c6a-7e30-434e-afef-c9506ecab9ec": {"node_ids": ["dafc84dd-d6a1-40ad-8d96-43d38d1c27cf"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "545eeb11-1a1b-4638-bbae-0ea57477cb54": {"node_ids": ["04e0cef6-36a4-4b76-84b0-f9a296bd310c"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "e7b2c337-0599-40d9-b210-cec1d9a7a25b": {"node_ids": ["da2f741d-5e0a-45ea-bfe5-09c851bd80fb"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "6e4acd37-ba41-48ad-921f-9b65dc9f47b2": {"node_ids": ["07574e85-5069-4f67-be09-6cc940e3623a"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "604cb90f-a657-40c8-8353-c58df0a35b61": {"node_ids": ["3feed60a-bf50-449c-8f93-514c0066c30e"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "62b59f10-0a7b-4149-8fdc-05b0d5fd1ef3": {"node_ids": ["ef3c8757-f0b2-4a18-b7ea-a5c24daeaeb4"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "6cd45afe-f968-436a-8e81-2dd2b58194f5": {"node_ids": ["41907683-eced-4a7b-a812-64f10df7f0d3"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "f20c0e72-b4f0-4ada-bb96-6cdd95963ee5": {"node_ids": ["ac422644-bd04-4d7d-9f64-3b8e071a4fc7"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "7deb220d-4a1f-4237-80b5-33f5c49d0e2c": {"node_ids": ["d91831d0-712f-48d8-be84-032035486903"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "78440e52-aaff-470c-8781-fd2b5b3fc92d": {"node_ids": ["d1da3857-695e-48a7-a157-2525c080587d"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "dbb76a11-9160-489e-a013-51bee5761cc8": {"node_ids": ["c915dd27-2f1d-4a68-af5b-a311cb8d411d"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "a0c28fc5-05d8-41d3-9bd2-fcc5cfa65b4a": {"node_ids": ["f1da23db-1cdf-4c07-9463-d456e05cc406"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "d9dd41ac-8aa2-4467-97a9-88b9db0a84d8": {"node_ids": ["e2c740c4-5275-49c1-9ca6-3961c9bb1245"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "c74bef42-e58f-4600-b5a8-d2fbccf4fb0f": {"node_ids": ["cf83cc77-2620-4ce1-8f92-d71f9b153818"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "093f2e9d-6aab-4dd1-b135-86b5d19a2cc4": {"node_ids": ["c9169c54-efc4-490a-b76d-8c183d465ca3"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "57de6d4e-95d5-4f91-a123-e1a36afea9c1": {"node_ids": ["bebc0cc4-07ad-43c9-81f9-38e4cc77c141"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "9e5df34b-2a6c-41d3-8c54-564d339ae9a4": {"node_ids": ["b2f6183c-aade-475d-84ca-6e4ed9062030"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "f115bb9b-2b87-4288-953c-f20e45ff9fc1": {"node_ids": ["9c196c65-be91-486c-8da0-8823d1084831"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "347315ec-253d-4b49-b83c-b32e27a95544": {"node_ids": ["b0238ce9-0b10-4082-a853-8f63b1883232"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "bf9bfd70-d6f0-4f68-900b-50276268e19a": {"node_ids": ["418bc452-5f15-43c3-bf53-a23d59901b16"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "f563e7ed-2918-4695-81fd-08cf02679a66": {"node_ids": ["da535acd-26c7-42ed-8e68-3e0e24c645f9"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "e95ef82a-dd67-42ab-b785-0b8eb04d22fb": {"node_ids": ["84686a4b-f84e-4fcd-84b8-af52047746a7"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "f1dcfb1e-6fd0-435c-b103-87c877d79d18": {"node_ids": ["664c805b-40fc-41d5-92e9-a94a3755e319"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "30fcf9a8-04b9-4774-a290-840e5169a5e0": {"node_ids": ["2bfc3a6c-ddcf-4b53-b1ea-7c500b1fa03e"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "fd6d4c43-2e78-4a77-a30c-e1101d7f267c": {"node_ids": ["f68991ce-455e-4d5d-9f9f-7863d96ca8a7"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "5658da6d-6827-4cfd-8f88-86b2dc87a1e2": {"node_ids": ["932b775e-8193-4648-b194-7a77e3f90baa"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "183844fd-a2cb-42a9-836b-13a5cd5bb9b9": {"node_ids": ["e210a659-6bd1-4edf-ac06-7c9045bf2575"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "9e71769f-c97b-465c-ace2-833cc64f6b4f": {"node_ids": ["59e0119e-dba2-42e5-86b2-9abef45b4ff8"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "f73f02a0-ba7d-4d26-9361-4c039781b924": {"node_ids": ["7f0b889f-8a6b-4302-bea8-29b61576b350"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "a2ee8b75-fd02-4176-8760-848b15e9d6a9": {"node_ids": ["e10b80ab-d049-4e06-8391-33eb010089d9"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "e85fb5d8-87c2-42c7-8559-0daf40607a34": {"node_ids": ["cd6703cd-c79b-4e2c-b870-1da40ebd0fb5"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "10e6c215-3511-4944-8ff1-dfbc9ee37dab": {"node_ids": ["63192a43-5a8e-460e-8039-adcddca6b484"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "dd2a6354-857b-4f29-b213-010226709082": {"node_ids": ["847c1f20-1db1-4e93-968a-c158ef1ed826"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "54dc5a26-f401-492a-9571-2af41969fe79": {"node_ids": ["6c1e93f8-ccb6-42bf-b74d-f6fcccc62497"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "917be18f-8d2a-412b-8f05-c5a599d3343e": {"node_ids": ["0c210266-1fc7-486f-96c6-9a2f2f7766d6"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "7fe0a61b-3435-4f36-a819-4c6bdb894523": {"node_ids": ["0ad3e607-5b1a-48de-b12f-40160ca7db63"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "9f063f5c-be2a-49fc-a7fa-8ece9cf447b1": {"node_ids": ["7114d7d5-1d90-4a0f-8fed-75e6fba75344"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "85c83823-db05-44a9-80b4-f7d5c55e1d73": {"node_ids": ["12724248-74e7-454c-b0c3-42a4a83269c8"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "0c407ece-0af7-420a-8daf-b29a6c3b07a2": {"node_ids": ["571ecbf1-142b-4195-bd3e-e8761924cf5a"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "6e18eba2-7b31-4f5e-8194-f3c49b04577a": {"node_ids": ["3732f2dd-fad0-4aa8-920b-7187e54ae973"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "c89a58e2-49ab-44bb-b5ba-de444fea94ac": {"node_ids": ["68fd2dfe-e540-4626-8e34-4627fd6883aa"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "0f6ef32e-8282-4782-b688-a5584d519c41": {"node_ids": ["4af9bd85-b29e-4eb3-823e-eb099d22d9ef"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "019fed00-da53-4202-8a61-228b8cc8f1d5": {"node_ids": ["7abbc120-615b-4839-baa3-cfa9d9d1abae"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "c11c2700-a60a-4ea1-b81b-78ff5298bcc5": {"node_ids": ["f7f22a94-b0ac-44d0-b0ad-c851eba9adb6"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "7275ffbc-4ceb-4b8d-a295-5a80cc46b027": {"node_ids": ["ac00025c-986e-4388-aa4d-bbe113b04aad"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "3e4e3074-122c-4aa6-887f-bb3306d80bb3": {"node_ids": ["a203f1dc-d00a-4151-b09d-a070bbfc246c"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "9fa01623-0348-40cd-8b5c-87ca69e5678b": {"node_ids": ["c8413286-8464-445e-a43e-451f8e2b4df5"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "8a05e471-4072-42f4-8360-96a8211333ba": {"node_ids": ["c935c321-644a-48da-8435-af62f5f3b83c"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "db102b6f-ab71-4ee9-a851-6aa5d8355447": {"node_ids": ["82386efe-a813-4e7c-bb5a-975fa8f69c66"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "16f67917-3ecf-4982-8412-fe0c652d1c2a": {"node_ids": ["bbf1d756-a7b4-4c33-a41a-de7df05619b4"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_type": null, "file_size": 56896, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "8fc458a1-63ee-4526-bc09-8227c21bd13d": {"node_ids": ["d846d164-8f4e-4759-bcbb-2a42a0e202a6"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "71ae0e58-1d2b-40a2-bddb-1b5cabcd82cb": {"node_ids": ["84d45653-9d38-430a-a57d-99c96dbe8a2a"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "64092970-53da-4b4f-9ec7-a65a0ca58551": {"node_ids": ["69cb76e5-83bc-47ec-87ce-e937aef3d408"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "43a1c15d-f815-4260-bc04-3bf73e295bf5": {"node_ids": ["862fa2fc-fbf8-4a67-b2c2-7cc63c6cd22b", "ca1d1c0e-7faa-431a-94a0-f92d55064823", "7f07a55b-a751-4ba8-8097-f9a9e9e65fec"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "6abead3b-1d43-4181-89c5-688e14e555e2": {"node_ids": ["b927f684-ca7f-4fff-8b5d-211041461a41"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "aaa9bb82-bba0-40ad-8bc6-3571e4698bc0": {"node_ids": ["8ef84d32-5a52-4b18-bafe-573e6078caba", "554f3eb7-194f-416f-8f1c-fb43c0cc8b0b"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "a71869a8-8ef7-49e2-b64f-d365d3df7ca1": {"node_ids": ["b9330e42-bd95-4513-a9f0-2b1283dc0a79", "d2559101-6a2f-4664-8845-098727c5cbdd", "f30b7af8-c113-42de-ab8b-f1447c523663"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "51b5bdb9-9108-4a0f-a89a-467bc99288b7": {"node_ids": ["4dac82da-e676-4141-97ed-9b576e358bf9"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "5557f947-a70c-409c-8946-926445b10890": {"node_ids": ["66ea6cb5-aff7-4fab-a6bc-cc462947f5c6"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "980b3526-c63a-4596-bc4d-66c986b49b32": {"node_ids": ["93600d8f-e327-4d83-9039-135861ba9035"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "a64f3653-738f-4768-902d-fccf3e1be5a3": {"node_ids": ["a051959a-dd48-4b5f-92dc-7d02e091099a"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_type": null, "file_size": 31265, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "b9fbf0d3-6f2c-4fc5-bfe4-45c1cc377beb": {"node_ids": ["9562397c-d024-4942-ad37-cb919365016e"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "76763497-e58f-4819-8640-da876e9da6b4": {"node_ids": ["4f80f3ca-b081-4102-be03-92d5d6e8af32"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "c35d9e44-90b5-4766-9d50-a2c38a050015": {"node_ids": ["8c62a42e-f52c-4cb3-a01e-eeb47d1f5020"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "09eb4de1-b223-451d-83fe-6555c0de6742": {"node_ids": ["3cc6d81e-15f0-43fd-9812-1d43d6a44c37"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "163a2cc0-c33d-4e34-8c74-2d69e3247bea": {"node_ids": ["0da4f774-4ba2-4797-9d77-330d5f98eb7a"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "b7f19304-dd58-4e75-a745-96f9105e9fe2": {"node_ids": ["7fb51725-faa5-4e3b-9721-d41b25a6c315"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "7b5c9401-281a-40b6-b72a-77d5206685ba": {"node_ids": ["f29aea69-bf90-4b99-af81-1b8606123fd2"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "7cb08143-5e41-44d5-a8da-1e626eb271f1": {"node_ids": ["97603c75-aa6f-451b-a7e8-5e3305477932"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "fed2e603-ed06-4762-89b6-3deb2c5935b5": {"node_ids": ["f47e422a-e24b-422e-9de2-53c16dbfba2b"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "6d7912ef-c68f-4bce-86c2-f04793fe1dc0": {"node_ids": ["fee303f5-616e-4bb2-85b4-6920906f46ad"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "78f8c795-ea9e-4eae-8e07-5025fefbcb90": {"node_ids": ["1d478305-3651-4df2-be1d-121b38db2ab9", "af7c6f2c-4fa2-43c7-a73e-525026cdb4df", "a69ffa28-4676-4bc1-a148-f00adc53c1e0"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "75d5e96c-a8cf-4bfa-b131-4b850ed8ae43": {"node_ids": ["2422a6aa-a9b7-4066-a51a-ecdf1829d9c3"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "c817712f-d768-4d9f-a7fa-5deb72ddac90": {"node_ids": ["2a22afc1-351e-40a5-b820-d0d702c5cbb8"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "3eff2c07-e375-4bda-8031-c81ef3314f89": {"node_ids": ["c85882ad-9aa3-4a3c-a743-b95c65aa1f3d"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "5a8eaabf-e0df-4ee8-85af-aead151d7ebc": {"node_ids": ["9b922dca-78a7-49cc-81e5-6702aebc9b02"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "64a779fd-74a5-4fe1-8536-04937d022f0f": {"node_ids": ["125feea5-8124-4058-8b1b-e4f43296f1d6"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "7732490d-fd9a-4f47-91ca-8c28687a3ff6": {"node_ids": ["c33a5a18-dc83-402e-94a1-b106c02e6570"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_type": null, "file_size": 27992, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "3110282f-d8ee-4b21-8212-1e96891acd23": {"node_ids": ["f78f606e-2762-4d36-879a-d732473a858d"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "cfab389d-3054-4336-8e55-26d73f4cea03": {"node_ids": ["86975a3e-7d1b-4457-bc94-9596cbfe77fc"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "b92cffaa-499b-418b-8857-39ccd5a2202c": {"node_ids": ["c5252813-0706-4e98-b55f-88c21063a4fd"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "c8a10ff8-5321-47d6-90dc-8e262e259895": {"node_ids": ["3c999d6c-441a-4664-9efa-2c6fa51890df"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "a6bf2401-22f1-4ba2-b289-c183ea1a9203": {"node_ids": ["33f79220-28b9-42a4-93a1-66fe1ea9c8e7"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "88a94c02-45ac-4f28-9aca-67851e7c1bc4": {"node_ids": ["e834e703-b659-4e4b-8b86-a725252722d1"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "90056080-e722-4354-afc0-ccce76552a66": {"node_ids": ["ce35d7d2-ce7f-4e50-8ac6-a981c68715c5"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "10cddbf2-38d8-4e46-b8cf-94fa7561d4e1": {"node_ids": ["271baa91-1e12-4763-bc0a-c94bf11c8df7"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "82d51892-61ce-437b-a2dd-067ec10b6155": {"node_ids": ["8c79111e-5991-4052-b7ef-924f5419bbf0"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "3a6a55f1-6d9c-4dee-b23e-ddbf9db019ca": {"node_ids": ["a6b4d19e-b174-447f-85dc-8a8662fbd397"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "00af1a4e-94cf-40de-830d-d93570989fa4": {"node_ids": ["bb7c2f4d-2930-4f36-9791-7bd99cf21736"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "273d027a-3b3e-41c9-b6d6-ea8efc3a679c": {"node_ids": ["8075e816-806e-44aa-84a6-53e17241c595"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "bc258d10-f3f1-4693-b227-973ff974f16d": {"node_ids": ["3e94555c-c2f3-4b25-80e5-1270ea419acd"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "3239be1b-1c0e-4a4a-8361-663be0ed3c73": {"node_ids": ["2a00af8b-eee8-4443-90ad-2d51f4a83d57"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "546426ca-21a0-47b4-9661-e8f2495d2769": {"node_ids": ["2804681c-94ad-47c3-96b5-1b687eb390ae"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "b7741213-9ceb-4385-bf59-62411fa9d8d2": {"node_ids": ["3fd8d337-8f8a-45d5-ab78-6c640c9c9a2d"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "54bd5851-7151-434b-8f49-d024ecfe1222": {"node_ids": ["be2f5ad6-4c2a-4b91-8c2d-bab368226a1f"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "c5c336bd-8240-44c0-b522-4a6a923c49a4": {"node_ids": ["4095955f-215a-4210-9fb6-46e1342a3657"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "830e2c34-86b8-4a53-b3ac-e69854db7132": {"node_ids": ["9bb76320-5208-4ddd-8b1d-f7ddeb589402"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "953faea1-1bb9-494c-bcd9-642c51d78fc0": {"node_ids": ["ede56a6c-8e2a-4ca0-aa91-ae59024343f0"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "719e8fec-2706-4e2a-9bf0-3d0e3d0c224a": {"node_ids": ["43f8215f-456c-4ff8-a8a5-4f71ca3fd577"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}, "7c0692e1-a293-42ab-a093-dcd19dc6e376": {"node_ids": ["62f4c0bf-3779-4b68-b049-69016c1a821d"], "metadata": {"file_path": "C:\\AI-ML\\ai-ml-projects\\sfguide-blog-ai-assistant\\content\\blogs\\getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_type": null, "file_size": 29669, "creation_date": "2024-02-07", "last_modified_date": "2024-02-07", "last_accessed_date": "2024-02-07"}}}}